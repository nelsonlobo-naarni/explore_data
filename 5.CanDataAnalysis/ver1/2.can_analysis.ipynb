{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836fb9f7",
   "metadata": {},
   "source": [
    "- ASC data for vehicle 4268 bearing device ID: 6\n",
    "- Oct 27th 2025. 7:11pm - 7:28pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568e8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import logging\n",
    "import argparse\n",
    "# import trino\n",
    "import io\n",
    "import boto3\n",
    "from itertools import islice\n",
    "from datetime import datetime, date, timedelta\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a49f71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python version: 3.14.0\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "repo_path = '/Users/kenobi/Documents/naarni/repo/dview-naarni-data-platform'\n",
    "sys.path.append(os.path.join(repo_path, 'tasks'))\n",
    "\n",
    "# Import necessary files and its respective functions\n",
    "from common.db_operations import connect_to_trino, fetch_data_for_day, write_df_to_iceberg,drop_table,execute_query\n",
    "from common.optimizer_logic import optimize_dataframe_memory\n",
    "\n",
    "# Import business logic functions\n",
    "from biz_logic.energy_mileage.energy_mileage_daily_v0 import energy_mileage_stats ,impute_odometer_readings\n",
    "\n",
    "from biz_logic.energy_consumption.energy_consumption_report import energy_consumption_stats\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Print the Python version being used\n",
    "print(f\"Using Python version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- reporting config (edit ONLY this) ----\n",
    "TABLE_NAME = \"can_parsed_output_100\"   # <— change only this\n",
    "\n",
    "# derived (don’t edit)\n",
    "REPORT_TABLE = f\"adhoc.facts_prod.{TABLE_NAME}\"\n",
    "REPORT_S3_LOCATION = f\"s3a://naarni-data-lake/aqua/warehouse/facts_prod.db/{TABLE_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db41484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(start_date, end_date, vehicle_ids):\n",
    "    \"\"\"\n",
    "    Fetch raw battery data from the database for the specified date range and vehicle IDs.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date in 'YYYY-MM-DD' format\n",
    "        end_date: End date in 'YYYY-MM-DD' format\n",
    "        vehicle_ids: List of vehicle IDs\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (df_cpo100, df_can_ac) containing raw data from both tables\n",
    "    \"\"\"\n",
    "    logging.info(f\"Fetching raw battery data from {start_date} to {end_date} for vehicles {vehicle_ids}\")\n",
    "    \n",
    "    # Format vehicle IDs for the query\n",
    "    vehicle_ids_str = ', '.join([f\"'{vid}'\" for vid in vehicle_ids])\n",
    "    \n",
    "    # Connect to Trino\n",
    "    # conn = connect_to_trino(host=\"analytics.internal.naarni.com\", port=443, user=\"admin\", catalog=\"adhoc\", schema=\"default\")\n",
    "    conn = connect_to_trino(host=\"trino.naarni.internal\",port=80,user=\"admin\",catalog=\"adhoc\",schema=\"default\")\n",
    "\n",
    "\n",
    "    # Query for cpo100 data\n",
    "    cpo100_query = f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM\n",
    "        facts_prod.can_parsed_output_100\n",
    "    WHERE \n",
    "        id in ({vehicle_ids_str})\n",
    "        and date(timestamp AT TIME ZONE 'Asia/Kolkata') between DATE('{start_date}') AND DATE('{end_date}')\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute queries and fetch data\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Fetch cpo100 data\n",
    "    cur.execute(cpo100_query)\n",
    "    cpo100_columns = [desc[0] for desc in cur.description]\n",
    "    cpo100_rows = cur.fetchall()\n",
    "    df_cpo100 = pd.DataFrame(cpo100_rows, columns=cpo100_columns)\n",
    "\n",
    "    logging.info(f\"Done Fetching data.\")\n",
    "    logging.info(f\"Retrieved {len(df_cpo100)} cpo100 records from the database.\")\n",
    "    \n",
    "    # Close connections\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return df_cpo100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927c7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = connect_to_trino(host=\"analytics.internal.naarni.com\",port=443,user=\"admin\",catalog=\"adhoc\",schema=\"default\")\n",
    "\n",
    "# vehicle_ids=[\"6\"]\n",
    "# start_date = \"2025-10-01\"\n",
    "# end_date = \"2025-10-02\"\n",
    "# df_lakehouse = fetch_data(start_date, end_date, vehicle_ids)\n",
    "# display(df_lakehouse.head(20))\n",
    "# # df.to_csv(\"can_parsed_output_100_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8118a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_lakehouse.sort_values(by=[\"sequence\"], ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345794be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse = pd.read_csv(\"can_parsed_output_100_clickhouse.csv\")\n",
    "df_clickhouse = pd.read_csv(\"can_parsed_output_100_clickhouse.csv\")\n",
    "df_raw = pd.read_csv(\"c2c_can_01Oct2025.csv\")\n",
    "df_lakehouse.columns = df_lakehouse.columns.str.strip().str.lower()\n",
    "df_clickhouse.columns = df_clickhouse.columns.str.strip().str.lower()\n",
    "df_raw.columns = df_raw.columns.str.strip().str.lower()\n",
    "\n",
    "def clean_cols(*dfs):\n",
    "    \"\"\"\n",
    "    Cleans column names by:\n",
    "      - stripping leading/trailing whitespace\n",
    "      - converting to lowercase\n",
    "    Returns cleaned DataFrames in the same order.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for df in dfs:\n",
    "        df.columns = df.columns.str.strip().str.lower()\n",
    "        cleaned.append(df)\n",
    "    return cleaned\n",
    "\n",
    "# Apply the cleaning function to all three\n",
    "df_lakehouse, df_clickhouse, df_raw = clean_cols(df_lakehouse, df_clickhouse, df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bec8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_epoch_ms_from_str(ts_str):\n",
    "    \"\"\"Convert '2025-10-01 00:00:00.776 115875' → epoch ms.\"\"\"\n",
    "    if pd.isna(ts_str):\n",
    "        return np.nan\n",
    "    match = re.match(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+)\", str(ts_str))\n",
    "    if not match:\n",
    "        return np.nan\n",
    "    dt_part = match.group(1)\n",
    "    dt = pd.to_datetime(dt_part, errors='coerce')\n",
    "    return int(dt.timestamp() * 1000) if pd.notna(dt) else np.nan\n",
    "\n",
    "# df_lakehouse[\"timestamp_epoch\"] = df_lakehouse[\"timestamp\"].apply(to_epoch_ms_from_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1beebaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_column(df, col_to_move, col_after):\n",
    "    \"\"\"\n",
    "    Moves column `col_to_move` to appear right after `col_after`.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    if col_to_move not in cols or col_after not in cols:\n",
    "        return df  # nothing to do if columns missing\n",
    "\n",
    "    cols.insert(cols.index(col_after) + 1, cols.pop(cols.index(col_to_move)))\n",
    "    return df[cols]\n",
    "\n",
    "# Apply it\n",
    "df_lakehouse = move_column(df_lakehouse, \"timestamp_epoch\", \"id\")\n",
    "\n",
    "df_lakehouse = df_lakehouse.drop(['timestamp', 'date', 'dt', 'rank'], axis=1, errors='ignore')\n",
    "df_lakehouse.rename(columns={\"timestamp_epoch\": \"timestamp\"}, inplace=True)\n",
    "# df_lakehouse.drop('insert_timestamp', axis=1, inplace=True)\n",
    "# df_clickhouse.drop('timestamp.1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449896be",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m/var/folders/0r/296485b94nsczl7dp338m7980000gn/T/ipykernel_5853/4049460969.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m print(\u001b[33m\"Min timestamp and number of records in SAMPLE:\"\u001b[39m, df_lakehouse.timestamp.min(), len(df_lakehouse))\n\u001b[32m      2\u001b[39m print(\u001b[33m\"Max timestamp and number of records in SAMPLE:\"\u001b[39m, df_lakehouse.timestamp.max(), len(df_lakehouse))\n\u001b[32m      3\u001b[39m print(\u001b[33m\"Min timestamp and number of records in CLICKHOUSE:\"\u001b[39m, df_clickhouse.timestamp.min(), len(df_clickhouse))\n\u001b[32m      4\u001b[39m print(\u001b[33m\"Max timestamp and number of records in CLICKHOUSE:\"\u001b[39m, df_clickhouse.timestamp.max(), len(df_clickhouse))\n",
      "\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6318\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m         ):\n\u001b[32m   6320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'timestamp'"
     ]
    }
   ],
   "source": [
    "print(\"Min timestamp and number of records in SAMPLE:\", df_lakehouse.timestamp.min(), len(df_lakehouse))\n",
    "print(\"Max timestamp and number of records in SAMPLE:\", df_lakehouse.timestamp.max(), len(df_lakehouse))\n",
    "print(\"Min timestamp and number of records in CLICKHOUSE:\", df_clickhouse.timestamp.min(), len(df_clickhouse))\n",
    "print(\"Max timestamp and number of records in CLICKHOUSE:\", df_clickhouse.timestamp.max(), len(df_clickhouse))\n",
    "print(\"Min timestamp and number of records in RAW:\", df_raw.timestamp.min(), len(df_raw))      #Correct IST timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a5629e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/0r/296485b94nsczl7dp338m7980000gn/T/ipykernel_5853/3918971045.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_lakehouse = df_lakehouse.sort_values(by=\u001b[33m\"timestamp\"\u001b[39m).copy()\n\u001b[32m      2\u001b[39m display(df_lakehouse.head())\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m df_clickhouse = df_clickhouse[(df_clickhouse.timestamp>=df_lakehouse.timestamp.min()) & (df_clickhouse.timestamp<=df_lakehouse.timestamp.max())].copy()\n",
      "\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7207\u001b[39m             )\n\u001b[32m   7208\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7209\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7210\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7211\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7212\u001b[39m \n\u001b[32m   7213\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7214\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "df_lakehouse = df_lakehouse.sort_values(by=\"timestamp\").copy()\n",
    "display(df_lakehouse.head())\n",
    "\n",
    "df_clickhouse = df_clickhouse[(df_clickhouse.timestamp>=df_lakehouse.timestamp.min()) & (df_clickhouse.timestamp<=df_lakehouse.timestamp.max())].copy()\n",
    "df_clickhouse = df_clickhouse.reindex(columns=df_lakehouse.columns)\n",
    "df_clickhouse = df_clickhouse.sort_values(by=[\"timestamp\",\"sequence\"]).copy()\n",
    "display(df_clickhouse.head())\n",
    "\n",
    "df_raw = df_raw[(df_raw.timestamp>=df_lakehouse.timestamp.min()) & (df_raw.timestamp<=df_lakehouse.timestamp.max())].copy()\n",
    "df_raw = df_raw.sort_values(by=[\"timestamp\",\"sequence\"]).copy()\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min timestamp and number of records in SAMPLE:\", df_lakehouse.timestamp.min(), len(df_lakehouse))\n",
    "print(\"Max timestamp and number of records in SAMPLE:\", df_lakehouse.timestamp.max(), len(df_lakehouse))\n",
    "print(\"Min timestamp and number of records in CLICKHOUSE:\", df_clickhouse.timestamp.min(), len(df_clickhouse))\n",
    "print(\"Max timestamp and number of records in CLICKHOUSE:\", df_clickhouse.timestamp.max(), len(df_clickhouse))\n",
    "print(\"Min timestamp and number of records in RAW:\", df_raw.timestamp.min(), len(df_raw))      #Correct IST timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee437158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder ClickHouse columns to match Lakehouse\n",
    "df_clickhouse = df_clickhouse.reindex(columns=df_lakehouse.columns)\n",
    "\n",
    "# Rename ClickHouse timestamp for join clarity\n",
    "df_lakehouse = df_lakehouse.rename(columns={\"timestamp\": \"timestamp_lakehouse\",\"sequence\":\"sequence_lakehouse\"})\n",
    "df_clickhouse = df_clickhouse.rename(columns={\"timestamp\": \"timestamp_clickhouse\",\"sequence\":\"sequence_clickhouse\"})\n",
    "\n",
    "# Join on epoch timestamp\n",
    "df_merged = df_lakehouse.merge(df_clickhouse,left_on=[\"timestamp_lakehouse\",\"sequence_lakehouse\"],right_on=[\"timestamp_clickhouse\",\"sequence_clickhouse\"],how=\"outer\",suffixes=(\"_lakehouse\", \"_clickhouse\"))\n",
    "\n",
    "df_merged[\"timestamp_lakehouse\"] = (\n",
    "    df_merged[\"timestamp_lakehouse\"]\n",
    "    .round(0)            # ensure clean integers if floats snuck in\n",
    "    .astype(\"Int64\")     # converts safely while allowing NaN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clickhouse[(df_clickhouse.timestamp_clickhouse >= 1759276800776) and (df_clickhouse.timestamp_clickhouse <= 1759276800776)][\"timestamp_clickhouse\"].value_counts()\n",
    "# 1759369880889: Thursday, 2 October 2025 01:51:20.889\n",
    "# 1759369881129: Thursday, 2 October 2025 01:51:21.129\n",
    "df_clickhouse_cp = df_clickhouse[(df_clickhouse[\"timestamp_clickhouse\"]>=1759276800776) & (df_clickhouse[\"timestamp_clickhouse\"]<=1759277399996)]#[\"timestamp_clickhouse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min timestamp and Max timestamp in CLICKHOUSE:\", df_clickhouse_cp.timestamp_clickhouse.min(), df_clickhouse_cp.timestamp_clickhouse.max())\n",
    "print(\"Min timestamp and Min timestamp in C2C_CAN RAW data:\", df_raw.timestamp.min(), df_raw.timestamp.max())      #Correct IST timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"timestamp\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickhouse_cp[\"timestamp_clickhouse\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_clickhouse_cp[\"timestamp_clickhouse\"].unique()),len(df_raw.timestamp.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse[(df_lakehouse[\"timestamp_lakehouse\"]>1759276800776) & (df_lakehouse[\"timestamp_lakehouse\"]<1759277399996)][\"timestamp_lakehouse\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[(df_raw[\"timestamp\"]>1759369880000) & (df_raw[\"timestamp\"]<1759369881150)].sort_values(by=[\"timestamp\",\"sequence\"])[\"timestamp\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1597b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_counts = df_lakehouse.timestamp_lakehouse.value_counts()\n",
    "ts_counts[ts_counts>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1759369880889: Thursday, 2 October 2025 01:51:20.889\n",
    "# 1759369881129: Thursday, 2 October 2025 01:51:21.129\n",
    "df_lakehouse[(df_lakehouse[\"timestamp_lakehouse\"]>1759369880000) & (df_lakehouse[\"timestamp_lakehouse\"]<1759369881150)][\"timestamp_lakehouse\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b86ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickhouse[df_clickhouse.timestamp_clickhouse==1759369881150].sort_values(by=\"sequence_clickhouse\")#.to_csv(\"can_data_ts_1759369881150.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse[df_lakehouse.timestamp_lakehouse==1759369881078].sort_values(by=\"sequence_lakehouse\")#.to_csv(\"can_data_ts_1759369881078.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba11d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = pd.read_csv(\"c2c_candata_021025.csv\")\n",
    "df_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_review.timestamp.min(),df_review.timestamp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ts_counts),len(df_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse[df_lakehouse.timestamp_lakehouse==1759369802571].sort_values(by=\"sequence_lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[(df_review[\"timestamp\"]>1759369880000) & (df_review[\"timestamp\"]<1759369881150)].sort_values(by=[\"timestamp\",\"sequence\"])[\"timestamp\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01baba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review[df_review.timestamp == 1759369802571].sort_values(by=\"sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.timestamp.min(), df_review.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_lakehouse[(df_lakehouse.timestamp_lakehouse>=df_review.timestamp.min()) & (df_lakehouse.timestamp_lakehouse<=df_review.timestamp.max())].sort_values(by=\"sequence_lakehouse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52045f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_counts = df_review.timestamp.value_counts().sort_values(ascending=False)\n",
    "ts_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ce8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_counts[ts_counts>20].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad117fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define suffixes\n",
    "suffix_l, suffix_r = \"_lakehouse\", \"_clickhouse\"\n",
    "\n",
    "# --- Identify shared base columns ---\n",
    "base_cols = sorted(\n",
    "    list(\n",
    "        set(c.replace(suffix_l, \"\")\n",
    "            for c in df_merged.columns if c.endswith(suffix_l))\n",
    "        & set(c.replace(suffix_r, \"\")\n",
    "            for c in df_merged.columns if c.endswith(suffix_r))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Comparing {len(base_cols)} common columns...\")\n",
    "\n",
    "# --- Define the row comparison function ---\n",
    "def row_diff(row):\n",
    "    for col in base_cols:\n",
    "        l, r = f\"{col}{suffix_l}\", f\"{col}{suffix_r}\"\n",
    "        val_l = row.get(l, None)\n",
    "        val_r = row.get(r, None)\n",
    "\n",
    "        # Handle missing values cleanly\n",
    "        if pd.isna(val_l) and pd.isna(val_r):\n",
    "            continue\n",
    "\n",
    "        # Robust comparison (handles pd.NA and mixed types)\n",
    "        try:\n",
    "            if val_l != val_r:\n",
    "                return True\n",
    "        except TypeError:\n",
    "            if str(val_l) != str(val_r):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# --- Apply comparison row-wise ---\n",
    "df_merged[\"is_diff\"] = df_merged.apply(row_diff, axis=1)\n",
    "df_diff = df_merged[df_merged[\"is_diff\"]]\n",
    "\n",
    "print(\"Number of differing rows:\", len(df_diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea275a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for differing rows\n",
    "# df_diff = df_merged[df_merged[\"is_diff\"]].copy()\n",
    "\n",
    "print(f\"Number of differing rows: {len(df_diff)}\")\n",
    "\n",
    "# Display first few differences for a quick inspection\n",
    "pd.set_option(\"display.max_columns\", None)   # so all columns are visible\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "display(df_diff.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d759df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickhouse.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assuming df_lakehouse and df_clickhouse are already aligned and cleaned\n",
    "# cols = df_lakehouse.columns.tolist()\n",
    "# chunk_size = 57\n",
    "# num_chunks = math.ceil(len(cols) / chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4431ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chunked_heatmaps(df_merged, suffix_left=\"_lakehouse\", suffix_right=\"_clickhouse\",\n",
    "                          chunk_size=57, label_left=\"Lakehouse\", label_right=\"ClickHouse\"):\n",
    "    \"\"\"\n",
    "    Plot grouped heatmaps comparing missing values between two suffixed sets of columns\n",
    "    within a merged DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df_merged: merged DataFrame with suffixed columns\n",
    "    - suffix_left: suffix for the first dataset (default '_lakehouse')\n",
    "    - suffix_right: suffix for the second dataset (default '_clickhouse')\n",
    "    - chunk_size: number of columns to display per comparison pair\n",
    "    - label_left / label_right: titles for plots\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify matching column roots (without suffix)\n",
    "    base_cols = sorted(\n",
    "        list(\n",
    "            set(\n",
    "                c.replace(suffix_left, \"\")\n",
    "                for c in df_merged.columns\n",
    "                if c.endswith(suffix_left)\n",
    "            )\n",
    "            & set(\n",
    "                c.replace(suffix_right, \"\")\n",
    "                for c in df_merged.columns\n",
    "                if c.endswith(suffix_right)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    n_chunks = math.ceil(len(base_cols) / chunk_size)\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(base_cols))\n",
    "        group = base_cols[start:end]\n",
    "\n",
    "        # Build lists of suffixed column names for each side\n",
    "        left_cols = [f\"{c}{suffix_left}\" for c in group]\n",
    "        right_cols = [f\"{c}{suffix_right}\" for c in group]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(25, 10), sharey=True)\n",
    "\n",
    "        sns.heatmap(df_merged[left_cols].isnull(), cmap=['#007f5f', '#f94144'],\n",
    "                    cbar=False, yticklabels=False, ax=axes[0])\n",
    "        axes[0].set_title(f\"{label_left} — Columns {start+1} to {end}\", fontsize=13)\n",
    "\n",
    "        sns.heatmap(df_merged[right_cols].isnull(), cmap=['#007f5f', '#f94144'],\n",
    "                    cbar=False, yticklabels=False, ax=axes[1])\n",
    "        axes[1].set_title(f\"{label_right} — Columns {start+1} to {end}\", fontsize=13)\n",
    "\n",
    "        plt.suptitle(f\"Null Comparison Heatmap (Columns {group[0]} → {group[-1]})\", fontsize=15)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_chunked_heatmaps(df_merged,suffix_left=\"_lakehouse\",suffix_right=\"_clickhouse\",chunk_size=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only matching timestamp rows\n",
    "df_matched = df_merged[\n",
    "    (df_merged[\"timestamp_lakehouse\"].notna()) &\n",
    "    (df_merged[\"timestamp_clickhouse\"].notna()) &\n",
    "    (df_merged[\"timestamp_lakehouse\"] == df_merged[\"timestamp_clickhouse\"])\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27038311",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_l, suffix_r = \"_lakehouse\", \"_clickhouse\"\n",
    "\n",
    "# Identify shared base columns\n",
    "base_cols = sorted(\n",
    "    list(\n",
    "        set(c.replace(suffix_l, \"\")\n",
    "            for c in df_matched.columns if c.endswith(suffix_l))\n",
    "        & set(c.replace(suffix_r, \"\")\n",
    "            for c in df_matched.columns if c.endswith(suffix_r))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def98ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_counts = {}\n",
    "for col in base_cols:\n",
    "    left = f\"{col}{suffix_l}\"\n",
    "    right = f\"{col}{suffix_r}\"\n",
    "    mismatches = (df_matched[left] != df_matched[right]) & (\n",
    "        ~(df_matched[left].isna() & df_matched[right].isna())\n",
    "    )\n",
    "    mismatch_counts[col] = mismatches.sum()\n",
    "\n",
    "# Convert to sorted Series for easy viewing\n",
    "mismatch_summary = pd.Series(mismatch_counts).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73edc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_summary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be722ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows with at least one differing value\n",
    "def has_diff(row):\n",
    "    for col in base_cols:\n",
    "        l, r = f\"{col}{suffix_l}\", f\"{col}{suffix_r}\"\n",
    "        val_l, val_r = row[l], row[r]\n",
    "        if pd.isna(val_l) and pd.isna(val_r):\n",
    "            continue\n",
    "        if val_l != val_r:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df_matched[\"is_diff\"] = df_matched.apply(has_diff, axis=1)\n",
    "df_diff_rows = df_matched[df_matched[\"is_diff\"]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05632ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_diffs(row):\n",
    "    diffs = {}\n",
    "    for col in base_cols:\n",
    "        l, r = f\"{col}{suffix_l}\", f\"{col}{suffix_r}\"\n",
    "        if l in row and r in row:\n",
    "            val_l, val_r = row[l], row[r]\n",
    "            if pd.isna(val_l) and pd.isna(val_r):\n",
    "                continue\n",
    "            if val_l != val_r:\n",
    "                diffs[col] = (val_l, val_r)\n",
    "    return diffs\n",
    "\n",
    "df_diff_rows[\"diff_columns\"] = df_diff_rows.apply(extract_diffs, axis=1)\n",
    "df_diff_view = df_diff_rows[[\"timestamp_lakehouse\", \"diff_columns\"]]\n",
    "display(df_diff_view.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_lakehouse),len(df_lakehouse.timestamp_lakehouse.unique()))\n",
    "print(len(df_clickhouse),len(df_clickhouse.timestamp_clickhouse.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4899be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_clickhouse.timestamp_clickhouse.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse.loc[(df_lakehouse[\"timestamp_lakehouse\"]>1759369871078) & (df_lakehouse[\"timestamp_lakehouse\"]<1759369881078)].sort_values(by=[\"timestamp_lakehouse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse[df_lakehouse.timestamp_lakehouse==1759369881078].sort_values(by=\"sequence_lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea704ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickhouse[df_clickhouse.timestamp_clickhouse==1759369881078].sort_values(by=\"sequence_clickhouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged),len(df_lakehouse),len(df_clickhouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voltage columns: pack_cellvoltage_1 ... pack_cellvoltage_380\n",
    "voltage_cols = sorted(\n",
    "    [c for c in df_lakehouse.columns if re.match(r\"pack_cellvoltage_\\d+\", c)],\n",
    "    key=lambda x: int(re.findall(r\"\\d+\", x)[0])\n",
    ")\n",
    "\n",
    "# Temperature columns: pack_temperature_1 ... pack_temperature_80\n",
    "temp_cols = sorted(\n",
    "    [c for c in df_lakehouse.columns if re.match(r\"pack_temperature\\d+\", c)],\n",
    "    key=lambda x: int(re.findall(r\"\\d+\", x)[0])\n",
    ")\n",
    "\n",
    "print(f\"Voltage columns: {len(voltage_cols)}\")\n",
    "print(f\"Temperature columns: {len(temp_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_voltage_coverage_intervals(df, voltage_cols, timestamp_col):\n",
    "    \"\"\"\n",
    "    Computes how long it takes to receive a full set of 380 voltages \n",
    "    by accumulating non-null voltages across timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "    intervals = []\n",
    "    seen = set()\n",
    "    start_ts = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Start new interval if needed\n",
    "        if start_ts is None:\n",
    "            start_ts = row[timestamp_col]\n",
    "\n",
    "        # Add non-null voltage columns seen in this row\n",
    "        for col in voltage_cols:\n",
    "            if pd.notna(row[col]):\n",
    "                seen.add(col)\n",
    "\n",
    "        # If full coverage achieved\n",
    "        if len(seen) == len(voltage_cols):   # 380\n",
    "            end_ts = row[timestamp_col]\n",
    "\n",
    "            # Compute time difference\n",
    "            duration_ms = end_ts - start_ts\n",
    "            duration_sec = duration_ms / 1000\n",
    "\n",
    "            intervals.append({\n",
    "                \"start_ts\": start_ts,\n",
    "                \"end_ts\": end_ts,\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"duration_sec\": duration_sec,\n",
    "                \"duration_rows\": idx   # number of rows needed\n",
    "            })\n",
    "\n",
    "            # Reset for the next cycle\n",
    "            seen = set()\n",
    "            start_ts = None\n",
    "\n",
    "    return pd.DataFrame(intervals)\n",
    "\n",
    "\n",
    "voltage_intervals_lake = compute_voltage_coverage_intervals(df_lakehouse, voltage_cols, \"timestamp_lakehouse\")\n",
    "voltage_intervals_click = compute_voltage_coverage_intervals(df_clickhouse, voltage_cols, \"timestamp_clickhouse\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(voltage_intervals_lake[\"duration_sec\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Full-Voltage Coverage Duration (Lakehouse)\")\n",
    "plt.xlabel(\"Seconds to receive all 380 cell voltages\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_intervals_lake.duration_sec.describe(percentiles=[0.25, 0.5, 0.75, 0.8,0.85,0.9,0.95, 0.99,0.995, 0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temperature_coverage_intervals(df, temperature_cols, timestamp_col):\n",
    "    \"\"\"\n",
    "    Computes how long it takes to receive a full set of 80 temperatures\n",
    "    by accumulating non-null temperatures across timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "    intervals = []\n",
    "    seen = set()\n",
    "    start_ts = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Start new interval if needed\n",
    "        if start_ts is None:\n",
    "            start_ts = row[timestamp_col]\n",
    "\n",
    "        # Add non-null temperature columns seen in this row\n",
    "        for col in temperature_cols:\n",
    "            if pd.notna(row[col]):\n",
    "                seen.add(col)\n",
    "\n",
    "        # If full coverage achieved\n",
    "        if len(seen) == len(temperature_cols):   # 80\n",
    "            end_ts = row[timestamp_col]\n",
    "\n",
    "            # Compute time difference\n",
    "            duration_ms = end_ts - start_ts\n",
    "            duration_sec = duration_ms / 1000\n",
    "\n",
    "            intervals.append({\n",
    "                \"start_ts\": start_ts,\n",
    "                \"end_ts\": end_ts,\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"duration_sec\": duration_sec,\n",
    "                \"duration_rows\": idx   # number of rows needed\n",
    "            })\n",
    "\n",
    "            # Reset for the next cycle\n",
    "            seen = set()\n",
    "            start_ts = None\n",
    "\n",
    "    return pd.DataFrame(intervals)\n",
    "\n",
    "\n",
    "temperature_intervals_lake = compute_temperature_coverage_intervals(df_lakehouse, temp_cols, \"timestamp_lakehouse\")\n",
    "temperature_intervals_click = compute_temperature_coverage_intervals(df_clickhouse, temp_cols, \"timestamp_clickhouse\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(temperature_intervals_lake[\"duration_sec\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Full-Temperature Coverage Duration (Lakehouse)\")\n",
    "plt.xlabel(\"Seconds to receive all 80 cell temperatures\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc017b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_intervals_lake.duration_sec.describe(percentiles=[0.25, 0.5, 0.75, 0.8,0.85,0.9,0.95, 0.99,0.995, 0.999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe79b9",
   "metadata": {},
   "source": [
    "### IDs & Timestamp\n",
    "\n",
    "#### Discrete Variables\n",
    "- id: Related to vehicle id\n",
    "- sequence_lakehouse: \n",
    "- number_of_can_ids\n",
    "- number_of_can_records\n",
    "- vcuversioninformation\n",
    "\n",
    "#### Timestamp\n",
    "- timestamp: YYYY-MM-DD HH:MM:SS format\n",
    "\n",
    "#### Continuous Variables\n",
    "- percentage_of_can_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"id\",\"timestamp_lakehouse\",\"sequence_lakehouse\",\"number_of_can_ids\",\"number_of_records\",\"percentage_of_can_ids\",\"vcuversioninformation\"]\n",
    "for col in l:\n",
    "    print(f\"Describing {col}:\")\n",
    "    print(df_lakehouse[col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lakehouse[\"lowpressureoilpumpfaultcode\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91979e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"lowpressureoilpumpfaultcode\",\"bms_fault_code\",\"vcu_fault_code\",\"fiveinone_faultcode\"]\n",
    "for col in l:\n",
    "    print(f\"Describing {col}:\")\n",
    "    print(df_lakehouse[col].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naarni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
