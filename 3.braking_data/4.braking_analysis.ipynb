{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e2ecff-1990-448c-8fe7-e70697ce04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python version: 3.11.13\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import logging\n",
    "import trino\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.table import Table\n",
    "\n",
    "\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f\"Using Python version: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f80526-aa2a-4d46-b4dc-0960ea9351cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vehicle_data(start_time=None, end_time=None, vehicle_ids=None):\n",
    "    \"\"\"\n",
    "    Fetch vehicle and GPS data from Trino with flexible time filtering.\n",
    "    Data is fetched from individual tables and then joined using pandas.\n",
    "\n",
    "    Args:\n",
    "        start_time: Start time in 'YYYY-MM-DD HH:MM:SS' format (optional)\n",
    "        end_time: End time in 'YYYY-MM-DD HH:MM:SS' format (optional)\n",
    "        vehicle_ids: List of vehicle IDs to filter (optional)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with vehicle data from all three tables\n",
    "    \"\"\"\n",
    "    # Set default vehicle IDs if not provided\n",
    "    # if vehicle_ids is None:\n",
    "    #     vehicle_ids = ['18','19']\n",
    "\n",
    "    # Format vehicle IDs for the query\n",
    "    vehicle_ids_str = \"', '\".join(vehicle_ids)\n",
    "\n",
    "    # Build time filter conditions\n",
    "    time_filter = \"\"\n",
    "\n",
    "    if start_time and end_time:\n",
    "        # Convert string times to datetime objects\n",
    "        start_dt = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "        end_dt = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Format for the query\n",
    "        time_filter = f\"\"\"\n",
    "        AND timestamp >= CAST('{start_time}' AS TIMESTAMP) AT TIME ZONE 'Asia/Kolkata' - INTERVAL '5' HOUR - INTERVAL '30' MINUTE\n",
    "        AND timestamp < CAST('{end_time}' AS TIMESTAMP) AT TIME ZONE 'Asia/Kolkata' - INTERVAL '5' HOUR - INTERVAL '30' MINUTE\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Default time range if not provided\n",
    "        time_filter = \"\"\"\n",
    "        AND timestamp >= CAST('2025-10-07' AS TIMESTAMP) AT TIME ZONE 'Asia/Kolkata' - INTERVAL '5' HOUR - INTERVAL '30' MINUTE\n",
    "        AND timestamp < CAST('2025-10-09' AS TIMESTAMP) AT TIME ZONE 'Asia/Kolkata' - INTERVAL '5' HOUR - INTERVAL '30' MINUTE\n",
    "        \"\"\"\n",
    "\n",
    "    # Connect to Trino\n",
    "    conn = trino.dbapi.connect(\n",
    "        host=\"trino\",\n",
    "        port=8080,\n",
    "        user=\"admin\",\n",
    "        catalog=\"adhoc\",\n",
    "        schema=\"default\"\n",
    "    )\n",
    "\n",
    "    # Query: Fetch data from can_parsed_output_100\n",
    "    query_cpo100 = f\"\"\"\n",
    "    SELECT \n",
    "        id, timestamp, dt, \n",
    "        CAST(timestamp AT TIME ZONE 'Asia/Kolkata' AS timestamp) AS IST,\n",
    "        date_trunc('minute', timestamp) as ts_mins,  -- Truncate to minutes\n",
    "        total_battery_current, bat_voltage, gear_position, odometerreading, \n",
    "        round(vehicle_speed_vcu,2) as vehicle_speed_vcu,\n",
    "        vehiclereadycondition, ignitionstatus, gun_connection_status, brakepedalpos\n",
    "    FROM \n",
    "        facts_prod.can_parsed_output_100\n",
    "    WHERE \n",
    "        id IN ('{vehicle_ids_str}')\n",
    "        {time_filter}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute query and fetch data into dataframe\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(query_cpo100)\n",
    "    columns_cpo100 = [desc[0] for desc in cur.description]\n",
    "    rows_cpo100 = cur.fetchall()\n",
    "    df_cpo100 = pd.DataFrame(rows_cpo100, columns=columns_cpo100)\n",
    "\n",
    "    # Close connections\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Sort the final dataframe\n",
    "    final_df = df_cpo100.sort_values(['id', 'IST'])\n",
    "\n",
    "    # Reset index\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    imputed_df = impute_odometer_readings(final_df)\n",
    "\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b551f19-318e-4d07-aed7-c7adb53c4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_odometer_readings(df):\n",
    "    \"\"\"\n",
    "    Impute missing OdometerReading values using vehicle speed and timestamp data.\n",
    "\n",
    "    This function fills missing odometer readings by:\n",
    "    1. Identifying segments between known odometer readings\n",
    "    2. Using vehicle speed and time differences to calculate distance\n",
    "    3. Interpolating odometer values based on proportional time elapsed\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing vehicle data with columns:\n",
    "            - id: Vehicle identifier\n",
    "            - IST: Timestamp\n",
    "            - OdoMeterReading: Odometer reading (may contain NaN values)\n",
    "            - Vehicle_speed_VCU: Vehicle speed in km/h\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed OdometerReading values\n",
    "    \"\"\"\n",
    "    logging.info(\"⚙️ Starting odometer reading imputation...\")\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    # Check if DataFrame is empty\n",
    "    if df.empty:\n",
    "        logging.warning(\"Input DataFrame is empty. Returning empty DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    # Group by vehicle ID and process each group separately\n",
    "    imputed_dfs = []\n",
    "\n",
    "    for vehicle_id, vehicle_df in df.groupby('id'):\n",
    "        # Sort by timestamp to ensure correct order\n",
    "        vehicle_df = vehicle_df.sort_values('IST')\n",
    "\n",
    "        # Check if there are any missing odometer readings\n",
    "        if vehicle_df['odometerreading'].isna().any():\n",
    "            # Get indices where odometer is not null (boundaries)\n",
    "            non_null_indices = vehicle_df[vehicle_df['odometerreading'].notna()].index\n",
    "\n",
    "            if len(non_null_indices) > 0:\n",
    "                # For each segment between two known odometer readings\n",
    "                for i in range(len(non_null_indices) - 1):\n",
    "                    start_idx = non_null_indices[i]\n",
    "                    end_idx = non_null_indices[i + 1]\n",
    "\n",
    "                    # Get the segment between two known odometer readings\n",
    "                    segment = vehicle_df.loc[start_idx:end_idx]\n",
    "\n",
    "                    # Skip if there are no missing values in this segment\n",
    "                    if segment['odometerreading'].isna().sum() == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Get boundary values\n",
    "                    start_odometer = vehicle_df.loc[start_idx, 'odometerreading']\n",
    "                    end_odometer = vehicle_df.loc[end_idx, 'odometerreading']\n",
    "\n",
    "                    # Get timestamps for boundaries\n",
    "                    start_time = vehicle_df.loc[start_idx, 'IST']\n",
    "                    end_time = vehicle_df.loc[end_idx, 'IST']\n",
    "\n",
    "                    # Total time difference in seconds\n",
    "                    total_time_diff = (end_time - start_time).total_seconds()\n",
    "\n",
    "                    # Skip if time difference is zero to avoid division by zero\n",
    "                    if total_time_diff == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Calculate cumulative distance for each row in the segment\n",
    "                    for idx in segment.index:\n",
    "                        if idx == start_idx:\n",
    "                            # First row already has the correct odometer reading\n",
    "                            continue\n",
    "\n",
    "                        # Get time difference from start\n",
    "                        time_diff = (vehicle_df.loc[idx, 'IST'] - start_time).total_seconds()\n",
    "\n",
    "                        # Calculate odometer reading using linear interpolation\n",
    "                        if total_time_diff > 0:\n",
    "                            # Proportional distance based on time elapsed\n",
    "                            time_ratio = time_diff / total_time_diff\n",
    "                            interpolated_odometer = start_odometer + (end_odometer - start_odometer) * time_ratio\n",
    "\n",
    "                            # Update the odometer reading\n",
    "                            vehicle_df.loc[idx, 'odometerreading'] = interpolated_odometer\n",
    "\n",
    "                # Handle the case where there are missing values at the beginning\n",
    "                if pd.isna(vehicle_df['odometerreading'].iloc[0]):\n",
    "                    # Use the first available odometer reading\n",
    "                    first_valid_idx = vehicle_df['odometerreading'].first_valid_index()\n",
    "                    if first_valid_idx is not None:\n",
    "                        first_valid_odometer = vehicle_df.loc[first_valid_idx, 'odometerreading']\n",
    "                        vehicle_df.loc[:first_valid_idx, 'odometerreading'] = first_valid_odometer\n",
    "\n",
    "                # Handle the case where there are missing values at the end\n",
    "                if pd.isna(vehicle_df['odometerreading'].iloc[-1]):\n",
    "                    # Use the last available odometer reading\n",
    "                    last_valid_idx = vehicle_df['odometerreading'].last_valid_index()\n",
    "                    if last_valid_idx is not None:\n",
    "                        last_valid_odometer = vehicle_df.loc[last_valid_idx, 'odometerreading']\n",
    "                        vehicle_df.loc[last_valid_idx:, 'odometerreading'] = last_valid_odometer\n",
    "\n",
    "        imputed_dfs.append(vehicle_df)\n",
    "\n",
    "    # Check if we have any DataFrames to concatenate\n",
    "    if not imputed_dfs:\n",
    "        logging.warning(\"No vehicle data to process. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # Combine all the imputed DataFrames\n",
    "    result_df = pd.concat(imputed_dfs)\n",
    "\n",
    "    logging.info(f\"✅ Odometer imputation completed. {result_df['odometerreading'].isna().sum()} missing values remaining.\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da751ae3-d229-463c-80b1-0072f90123d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_max_bpp_buckets(max_bpp_values, top_speed):\n",
    "    \"\"\"\n",
    "    Create buckets for max_bpp values and calculate mode.\n",
    "\n",
    "    Args:\n",
    "        max_bpp_values (list): List of max_bpp values from braking events.\n",
    "        top_speed (float): The top speed threshold used for filtering.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing bucket information and mode.\n",
    "    \"\"\"\n",
    "    if not max_bpp_values:\n",
    "        return {}\n",
    "\n",
    "    # Create buckets starting from top_speed in increments of 5\n",
    "    min_bpp = min(max_bpp_values)\n",
    "    max_bpp = max(max_bpp_values)\n",
    "\n",
    "    # Adjust bucket start to be a multiple of 5\n",
    "    bucket_start = int(min_bpp // 5) * 5\n",
    "    bucket_end = int(max_bpp // 5) * 5 + 5\n",
    "\n",
    "    # Create bucket ranges\n",
    "    buckets = {}\n",
    "    for i in range(bucket_start, bucket_end + 1, 5):\n",
    "        bucket_key = f\"{i}-{i+5}\"\n",
    "        buckets[bucket_key] = 0\n",
    "\n",
    "    # Assign values to buckets\n",
    "    for value in max_bpp_values:\n",
    "        bucket_index = int(value // 5) * 5\n",
    "        bucket_key = f\"{bucket_index}-{bucket_index+5}\"\n",
    "        if bucket_key in buckets:\n",
    "            buckets[bucket_key] += 1\n",
    "\n",
    "    # Calculate mode\n",
    "    try:\n",
    "        mode_result = stats.mode(max_bpp_values)\n",
    "        mode_value = mode_result.mode[0] if len(mode_result.mode) > 0 else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error calculating mode: {e}\")\n",
    "        mode_value = None\n",
    "\n",
    "    # Find the bucket with the most events (mode bucket)\n",
    "    mode_bucket = max(buckets, key=buckets.get) if buckets else None\n",
    "\n",
    "    return {\n",
    "        'buckets': buckets,\n",
    "        'mode_bucket': mode_bucket,\n",
    "        'mode_value': mode_value,\n",
    "        'top_speed': top_speed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af285314-3840-4ffc-a40c-32b0de2e2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_filtered_braking_events(df, top_speed=30.0, search_window_seconds=10.0):\n",
    "    \"\"\"\n",
    "    Analyzes a DataFrame to find and extract braking events.\n",
    "\n",
    "    This function identifies hard stops (speed dropping to 0) and then works\n",
    "    backwards within a specified time window to find the exact moment the\n",
    "    brake pedal was pressed. It filters events based on a minimum top speed\n",
    "    reached during the braking maneuver.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with vehicle data. Must contain 'timestamp',\n",
    "                           'brakepedalpos', and 'vehicle_speed_vcu' columns.\n",
    "                           The 'timestamp' should be in a format that can be\n",
    "                           converted to a datetime object (e.g., milliseconds since epoch).\n",
    "        top_speed (float): The minimum peak speed (km/h) for an event to be considered.\n",
    "        search_window_seconds (float): The time window before a hard stop to search\n",
    "                                      for the initial brake press.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - list of pd.DataFrame: A list where each element is a DataFrame\n",
    "                                    representing a single braking event.\n",
    "            - list of float: A list of the durations (in seconds) for each event.\n",
    "            - dict: A dictionary containing bucketed max_bpp data and mode information.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        logging.warning(\"Input DataFrame is empty. No events to analyze.\")\n",
    "        return [], [], {}\n",
    "\n",
    "    # Convert timestamp to human-readable IST and clean the data\n",
    "    # Assuming timestamp is in milliseconds, which is common\n",
    "    df['IST'] = pd.to_datetime(df['timestamp'], unit='ms').dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\n",
    "    df = df[[\"IST\", \"brakepedalpos\", \"vehicle_speed_vcu\"]].copy()\n",
    "    df.dropna(subset=[\"vehicle_speed_vcu\", \"brakepedalpos\"], inplace=True)\n",
    "    df.sort_values(by='IST', inplace=True)\n",
    "    df.loc[:, 'IST_formatted_string'] = df['IST'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Identify hard stop events (speed becomes 0 from a non-zero value)\n",
    "    hard_stop_mask = (df['vehicle_speed_vcu'] == 0.0) & (df['vehicle_speed_vcu'].shift(1) > 0.0)\n",
    "    hard_stop_events = df[hard_stop_mask].copy()\n",
    "\n",
    "    if hard_stop_events.empty:\n",
    "        logging.info(\"No hard stop events found in the DataFrame.\")\n",
    "        return [], [], {}\n",
    "\n",
    "    all_event_data = []\n",
    "    event_durations = []\n",
    "    max_bpp_values = []  # Collect max_bpp values for bucketing\n",
    "\n",
    "    # Extract and aggregate the data for each filtered event\n",
    "    for _, event_row in hard_stop_events.iterrows():\n",
    "        end_time = event_row['IST']\n",
    "        # Define a search window before the stop\n",
    "        search_start_time = end_time - pd.Timedelta(seconds=search_window_seconds)\n",
    "\n",
    "        # Look for the first instance of brake pedal application within this window\n",
    "        search_segment = df[(df['IST'] >= search_start_time) & (df['IST'] <= end_time)].copy()\n",
    "\n",
    "        # Find the first row where brakepedalpos is greater than 0\n",
    "        first_brake_press = search_segment[search_segment['brakepedalpos'] > 0.0].head(1)\n",
    "\n",
    "        # Check if a brake press was found in the search window\n",
    "        if not first_brake_press.empty:\n",
    "            # Get the exact start time of the braking event\n",
    "            start_time = first_brake_press.iloc[0]['IST']\n",
    "\n",
    "            # Filter the event segment from the exact start of braking to the stop\n",
    "            event_segment = df[(df['IST'] >= start_time) & (df['IST'] <= end_time)].copy()\n",
    "\n",
    "            # Check if the top speed in this dynamic segment meets the filter criteria\n",
    "            if not event_segment.empty and event_segment['vehicle_speed_vcu'].max() >= top_speed:\n",
    "                # Calculate the dynamic time taken to come to a full stop\n",
    "                time_to_stop_seconds = (end_time - start_time).total_seconds()\n",
    "\n",
    "                # Append the filtered event data and its duration\n",
    "                all_event_data.append(event_segment)\n",
    "                event_durations.append(time_to_stop_seconds)\n",
    "\n",
    "                # Collect max_bpp value for this event\n",
    "                max_bpp = event_segment['brakepedalpos'].max()\n",
    "                max_bpp_values.append(max_bpp)\n",
    "\n",
    "    if not all_event_data:\n",
    "        logging.info(f\"No events found that reached a top speed of {top_speed} km/h or greater and had a brake press within {search_window_seconds} seconds of the stop.\")\n",
    "        return [], [], {}\n",
    "\n",
    "    # Create max_bpp buckets\n",
    "    bucket_info = create_max_bpp_buckets(max_bpp_values, top_speed)\n",
    "\n",
    "    return all_event_data, event_durations, bucket_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ecd7a01-9ed0-4059-8fdb-a61b45d5ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_max_bpp_buckets(max_bpp_values, top_speed):\n",
    "    \"\"\"\n",
    "    Create buckets for max_bpp values and calculate mode.\n",
    "\n",
    "    Args:\n",
    "        max_bpp_values (list): List of max_bpp values from braking events.\n",
    "        top_speed (float): The top speed threshold used for filtering.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing bucket information and mode.\n",
    "    \"\"\"\n",
    "    if not max_bpp_values:\n",
    "        return {}\n",
    "\n",
    "    # Create buckets starting from top_speed in increments of 5\n",
    "    min_bpp = min(max_bpp_values)\n",
    "    max_bpp = max(max_bpp_values)\n",
    "\n",
    "    # Adjust bucket start to be a multiple of 5\n",
    "    bucket_start = int(min_bpp // 5) * 5\n",
    "    bucket_end = int(max_bpp // 5) * 5 + 5\n",
    "\n",
    "    # Create bucket ranges\n",
    "    buckets = {}\n",
    "    for i in range(bucket_start, bucket_end + 1, 5):\n",
    "        bucket_key = f\"{i}-{i+5}\"\n",
    "        buckets[bucket_key] = 0\n",
    "\n",
    "    # Assign values to buckets\n",
    "    for value in max_bpp_values:\n",
    "        bucket_index = int(value // 5) * 5\n",
    "        bucket_key = f\"{bucket_index}-{bucket_index+5}\"\n",
    "        if bucket_key in buckets:\n",
    "            buckets[bucket_key] += 1\n",
    "\n",
    "    # Calculate mode\n",
    "    try:\n",
    "        mode_result = stats.mode(max_bpp_values)\n",
    "        mode_value = mode_result.mode[0] if len(mode_result.mode) > 0 else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error calculating mode: {e}\")\n",
    "        mode_value = None\n",
    "\n",
    "    # Find the bucket with the most events (mode bucket)\n",
    "    mode_bucket = max(buckets, key=buckets.get) if buckets else None\n",
    "\n",
    "    return {\n",
    "        'buckets': buckets,\n",
    "        'mode_bucket': mode_bucket,\n",
    "        'mode_value': mode_value,\n",
    "        'top_speed': top_speed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3435d7e8-4324-458a-92a0-5903da93facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_event_metrics(event_df, duration):\n",
    "    \"\"\"\n",
    "    Compute metrics for a single braking event.\n",
    "\n",
    "    Args:\n",
    "        event_df (pd.DataFrame): DataFrame for a single braking event.\n",
    "        duration (float): Duration of the event in seconds.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing computed metrics.\n",
    "    \"\"\"\n",
    "    # Start velocity (km/h) - first row\n",
    "    start_velocity = event_df['vehicle_speed_vcu'].iloc[0]\n",
    "\n",
    "    # Total distance: integral of speed (convert km/h to m/s) over time\n",
    "    event_df = event_df.copy()\n",
    "    event_df['speed_mps'] = event_df['vehicle_speed_vcu'] * (5/18)  # Convert km/h to m/s\n",
    "    # Time differences in seconds\n",
    "    time_diffs = event_df['IST'].diff().dt.total_seconds().fillna(0)\n",
    "    # Distance for each segment: speed_mps * time_diffs\n",
    "    distances = event_df['speed_mps'] * time_diffs\n",
    "    total_distance = distances.sum()\n",
    "\n",
    "    # Deceleration: (start_velocity in m/s) / total_time\n",
    "    start_velocity_mps = start_velocity * (5/18)\n",
    "    deceleration = start_velocity_mps / duration if duration > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'start_velocity': start_velocity,\n",
    "        'total_time': duration,\n",
    "        'total_distance': total_distance,\n",
    "        'deceleration': deceleration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21e5577-4df9-4939-bd49-c4f8cc5535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bucket_statistics(braking_events, event_durations, bucket_info):\n",
    "    \"\"\"\n",
    "    Generate statistics for each bucket including medians, 95th and 99th percentiles.\n",
    "\n",
    "    Args:\n",
    "        braking_events (list): List of DataFrames, each representing a braking event.\n",
    "        event_durations (list): List of durations for each event.\n",
    "        bucket_info (dict): Dictionary containing bucket information.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Three DataFrames containing median, 95th percentile, and 99th percentile statistics.\n",
    "    \"\"\"\n",
    "    event_data = []\n",
    "\n",
    "    for i, event_df in enumerate(braking_events):\n",
    "        # Get the max_bpp for this event\n",
    "        max_bpp = event_df['brakepedalpos'].max()\n",
    "        # Compute bucket\n",
    "        bucket_index = int(max_bpp // 5) * 5\n",
    "        bucket_key = f\"{bucket_index}-{bucket_index+5}\"\n",
    "\n",
    "        # Compute metrics for the event\n",
    "        metrics = compute_event_metrics(event_df, event_durations[i])\n",
    "\n",
    "        # Append to event_data\n",
    "        event_data.append({\n",
    "            'bucket': bucket_key,\n",
    "            'start_velocity': metrics['start_velocity'],\n",
    "            'total_time': metrics['total_time'],\n",
    "            'total_distance': metrics['total_distance'],\n",
    "            'deceleration': metrics['deceleration']\n",
    "        })\n",
    "\n",
    "    # Create a dataframe\n",
    "    df_events = pd.DataFrame(event_data)\n",
    "\n",
    "    # For the first dataframe: medians\n",
    "    df_median = df_events.groupby('bucket',observed=False).agg(\n",
    "        total_events=('start_velocity', 'count'),\n",
    "        median_start_velocity=('start_velocity', 'median'),\n",
    "        median_total_time=('total_time', 'median'),\n",
    "        median_total_distance=('total_distance', 'median'),\n",
    "        median_deceleration=('deceleration', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # For the second dataframe: 95th percentile\n",
    "    df_95 = df_events.groupby('bucket',observed=False).agg(\n",
    "        total_events=('start_velocity', 'count'),\n",
    "        p95_start_velocity=('start_velocity', lambda x: np.percentile(x, 95)),\n",
    "        p95_total_time=('total_time', lambda x: np.percentile(x, 95)),\n",
    "        p95_total_distance=('total_distance', lambda x: np.percentile(x, 95)),\n",
    "        p95_deceleration=('deceleration', lambda x: np.percentile(x, 95))\n",
    "    ).reset_index()\n",
    "\n",
    "    # For the third dataframe: 99th percentile\n",
    "    df_99 = df_events.groupby('bucket',observed=False).agg(\n",
    "        total_events=('start_velocity', 'count'),\n",
    "        p99_start_velocity=('start_velocity', lambda x: np.percentile(x, 99)),\n",
    "        p99_total_time=('total_time', lambda x: np.percentile(x, 99)),\n",
    "        p99_total_distance=('total_distance', lambda x: np.percentile(x, 99)),\n",
    "        p99_deceleration=('deceleration', lambda x: np.percentile(x, 99))\n",
    "    ).reset_index()\n",
    "\n",
    "    return df_median.round(2), df_95.round(2), df_99.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f37469-be4a-417c-982b-5254066eb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_start_velocity_bucket_statistics(braking_events, event_durations):\n",
    "    \"\"\"\n",
    "    Generate statistics for each start velocity bucket including medians, 95th and 99th percentiles.\n",
    "\n",
    "    Args:\n",
    "        braking_events (list): List of DataFrames, each representing a braking event.\n",
    "        event_durations (list): List of durations for each event.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Three DataFrames containing median, 95th percentile, and 99th percentile statistics.\n",
    "    \"\"\"\n",
    "    event_data = []\n",
    "\n",
    "    for i, event_df in enumerate(braking_events):\n",
    "        # Get the max_bpp for this event\n",
    "        max_bpp = event_df['brakepedalpos'].max()\n",
    "        \n",
    "        # Compute metrics for the event\n",
    "        metrics = compute_event_metrics(event_df, event_durations[i])\n",
    "        \n",
    "        # Append to event_data\n",
    "        event_data.append({\n",
    "            'start_velocity': metrics['start_velocity'],\n",
    "            'max_bpp': max_bpp,\n",
    "            'total_time': metrics['total_time'],\n",
    "            'total_distance': metrics['total_distance'],\n",
    "            'deceleration': metrics['deceleration']\n",
    "        })\n",
    "\n",
    "    # Create a dataframe\n",
    "    df_events = pd.DataFrame(event_data)\n",
    "    \n",
    "    # Create start velocity buckets (5 km/h intervals)\n",
    "    min_velocity = df_events['start_velocity'].min()\n",
    "    max_velocity = df_events['start_velocity'].max()\n",
    "    \n",
    "    # Adjust bucket start to be a multiple of 5\n",
    "    bucket_start = int(min_velocity // 5) * 5\n",
    "    bucket_end = int(max_velocity // 5) * 5 + 5\n",
    "    \n",
    "    # Create bucket labels\n",
    "    bucket_labels = []\n",
    "    for i in range(bucket_start, bucket_end, 5):\n",
    "        bucket_labels.append(f\"{i}-{i+5}\")\n",
    "    \n",
    "    # Assign each event to a bucket\n",
    "    df_events['bucket'] = pd.cut(\n",
    "        df_events['start_velocity'],\n",
    "        bins=np.arange(bucket_start, bucket_end + 5, 5),\n",
    "        labels=bucket_labels,\n",
    "        right=False\n",
    "    )\n",
    "    \n",
    "    # Drop rows with NaN in bucket (events that don't fit in any bucket)\n",
    "    df_events = df_events.dropna(subset=['bucket'])\n",
    "    \n",
    "    # Define a safe percentile function that handles empty arrays\n",
    "    def safe_percentile(x, p):\n",
    "        if len(x) == 0:\n",
    "            return np.nan\n",
    "        return np.percentile(x, p)\n",
    "    \n",
    "    # For the first dataframe: medians\n",
    "    df_median = df_events.groupby('bucket',observed=False).agg(\n",
    "        total_events=('start_velocity', 'count'),\n",
    "        median_max_bpp=('max_bpp', 'median'),\n",
    "        median_total_time=('total_time', 'median'),\n",
    "        median_total_distance=('total_distance', 'median'),\n",
    "        median_deceleration=('deceleration', 'median')\n",
    "    ).reset_index()\n",
    "\n",
    "    # For the second dataframe: 95th percentile\n",
    "    df_95 = df_events.groupby('bucket',observed=False).agg(\n",
    "        total_events=('start_velocity', 'count'),\n",
    "        p95_max_bpp=('max_bpp', lambda x: safe_percentile(x, 95)),\n",
    "        p95_total_time=('total_time', lambda x: safe_percentile(x, 95)),\n",
    "        p95_total_distance=('total_distance', lambda x: safe_percentile(x, 95)),\n",
    "        p95_deceleration=('deceleration', lambda x: safe_percentile(x, 95))\n",
    "    ).reset_index()\n",
    "\n",
    "    # For the third dataframe: 99th percentile\n",
    "    df_99 = df_events.groupby('bucket',observed=False).agg(\n",
    "        total_events=('start_velocity', 'count'),\n",
    "        p99_max_bpp=('max_bpp', lambda x: safe_percentile(x, 99)),\n",
    "        p99_total_time=('total_time', lambda x: safe_percentile(x, 99)),\n",
    "        p99_total_distance=('total_distance', lambda x: safe_percentile(x, 99)),\n",
    "        p99_deceleration=('deceleration', lambda x: safe_percentile(x, 99))\n",
    "    ).reset_index()\n",
    "\n",
    "    return df_median.round(2), df_95.round(2), df_99.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b5687d-e793-46e9-8826-3eceecae098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bucket_pdf_report(bucket_type, df_median, df_95, df_99, output_filename):\n",
    "    \"\"\"\n",
    "    Generate a PDF report with bucket statistics tables.\n",
    "    \n",
    "    Args:\n",
    "        bucket_type (str): Type of bucket (e.g., \"Max Brake Pedal Position\" or \"Start Velocity\")\n",
    "        df_median (pd.DataFrame): DataFrame with median statistics\n",
    "        df_95 (pd.DataFrame): DataFrame with 95th percentile statistics\n",
    "        df_99 (pd.DataFrame): DataFrame with 99th percentile statistics\n",
    "        output_filename (str): Path to save the PDF file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    \n",
    "    with PdfPages(output_filename) as pdf:\n",
    "        # Create a summary page\n",
    "        fig_summary = plt.figure(figsize=(12, 8))\n",
    "        ax_summary = fig_summary.add_subplot(111)\n",
    "        ax_summary.axis('off')\n",
    "        \n",
    "        # Add title\n",
    "        plt.suptitle(f\"{bucket_type} Bucket Analysis Report\", fontsize=18, y=0.95)\n",
    "        \n",
    "        # Create summary text\n",
    "        summary_text = f\"\"\"\n",
    "        Report Summary:\n",
    "        - Bucket Type: {bucket_type}\n",
    "        - Total Buckets: {len(df_median)}\n",
    "        - Date Range: {start_time.split()[0]} to {end_time.split()[0]}\n",
    "        \n",
    "        This report contains three tables showing:\n",
    "        1. Median statistics for each bucket\n",
    "        2. 95th percentile statistics for each bucket\n",
    "        3. 99th percentile statistics for each bucket\n",
    "        \n",
    "        Each table shows:\n",
    "        - Bucket range\n",
    "        - Total events in the bucket\n",
    "        - Median/95th/99th percentile values for:\n",
    "          * Max Brake Pedal Position (BPP)\n",
    "          * Total Time (seconds)\n",
    "          * Total Distance (meters)\n",
    "          * Deceleration (m/s²)\n",
    "        \"\"\"\n",
    "        \n",
    "        ax_summary.text(0.5, 0.5, summary_text, \n",
    "                        ha='center', va='center', \n",
    "                        fontsize=12, \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"black\", lw=1))\n",
    "        \n",
    "        pdf.savefig(fig_summary, bbox_inches='tight')\n",
    "        plt.close(fig_summary)\n",
    "        \n",
    "        # Create pages for each statistics table\n",
    "        dataframes = [\n",
    "            (df_median, \"Median Statistics\"),\n",
    "            (df_95, \"95th Percentile Statistics\"),\n",
    "            (df_99, \"99th Percentile Statistics\")\n",
    "        ]\n",
    "        \n",
    "        for df, title in dataframes:\n",
    "            fig_table = plt.figure(figsize=(16, 10))\n",
    "            ax_table = fig_table.add_subplot(111)\n",
    "            ax_table.axis('off')\n",
    "            \n",
    "            # Create table\n",
    "            table = ax_table.table(\n",
    "                cellText=df.values,\n",
    "                colLabels=df.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colColours=['#f3f3f3']*len(df.columns)\n",
    "            )\n",
    "            \n",
    "            # Style the table\n",
    "            table.auto_set_font_size(False)\n",
    "            table.set_fontsize(10)\n",
    "            table.scale(1, 1.5)\n",
    "            \n",
    "            # Highlight header row\n",
    "            for i in range(len(df.columns)):\n",
    "                table[(0, i)].set_facecolor('#40466e')\n",
    "                table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "            \n",
    "            # Add title\n",
    "            plt.title(f\"{bucket_type} - {title}\", fontsize=16, pad=20)\n",
    "            \n",
    "            # Adjust layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save to PDF\n",
    "            pdf.savefig(fig_table, bbox_inches='tight')\n",
    "            plt.close(fig_table)\n",
    "    \n",
    "    print(f\"PDF report saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ae9d96-a17f-40c0-90e1-294843d015ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define time range and vehicle IDs\n",
    "start_time = '2025-08-01 00:00:00'\n",
    "# end_time = '2025-08-04 00:00:00'\n",
    "end_time = '2025-10-15 23:59:59'\n",
    "vehicle_ids = ['9', '7', '11', '13', '14','15']  # Add all vehicle IDs you want to analyze\n",
    "# vehicle_ids = ['11','13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c665b954-bd3e-4318-80dd-d43b49aec534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing vehicle 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 17:53:02 - INFO - ⚙️ Starting odometer reading imputation...\n"
     ]
    }
   ],
   "source": [
    "# Process each vehicle separately\n",
    "for vehicle_id in vehicle_ids:\n",
    "    print(f\"\\nProcessing vehicle {vehicle_id}...\")\n",
    "    \n",
    "    # 1. Fetch the data for this vehicle\n",
    "    vehicle_data_df = get_vehicle_data(start_time, end_time, [vehicle_id])\n",
    "    \n",
    "    if vehicle_data_df.empty:\n",
    "        print(f\"No data found for vehicle {vehicle_id}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # 2. Analyze the DataFrame to find braking events\n",
    "    braking_events, event_durations, bucket_info = analyze_filtered_braking_events(\n",
    "        df=vehicle_data_df,\n",
    "        top_speed=25.0,\n",
    "        search_window_seconds=15.0\n",
    "    )\n",
    "    \n",
    "    if not braking_events:\n",
    "        print(f\"No braking events found for vehicle {vehicle_id} matching the criteria.\")\n",
    "        continue\n",
    "    # 3. Generate bucket statistics\n",
    "    df_median, df_95, df_99 = generate_bucket_statistics(braking_events, event_durations, bucket_info)\n",
    "    \n",
    "    # 4. Display results\n",
    "    print(f\"\\nResults for Vehicle {vehicle_id}:\")\n",
    "    print(f\"Total braking events found: {len(braking_events)}\")\n",
    "    print(f\"Date Range: {start_time.split()[0]} to {end_time.split()[0]}\")\n",
    "    \n",
    "    # Display bucket information\n",
    "    if bucket_info:\n",
    "        print(\"\\nMax Brake Pedal Position (BPP) Buckets:\")\n",
    "        for bucket, count in bucket_info['buckets'].items():\n",
    "            print(f\"  {bucket}%: {count} events\")\n",
    "        \n",
    "        if bucket_info['mode_bucket']:\n",
    "            print(f\"\\nMode Bucket: {bucket_info['mode_bucket']}%\")\n",
    "        if bucket_info['mode_value']:\n",
    "            print(f\"Mode Value: {bucket_info['mode_value']:.2f}%\")\n",
    "    \n",
    "    # Display the three statistics dataframes\n",
    "    print(\"\\nMedian Statistics by Bucket:\")\n",
    "    display(df_median)\n",
    "    \n",
    "    print(\"\\n95th Percentile Statistics by Bucket:\")\n",
    "    display(df_95)\n",
    "    \n",
    "    print(\"\\n99th Percentile Statistics by Bucket:\")\n",
    "    display(df_99)   \n",
    "\n",
    "    # Generate max_bpp bucket report\n",
    "    generate_bucket_pdf_report(\n",
    "        bucket_type=\"Max Brake Pedal Position\",\n",
    "        df_median=df_median,\n",
    "        df_95=df_95,\n",
    "        df_99=df_99,\n",
    "        output_filename=f\"reports/max_bpp_bucket/vehicle_{vehicle_id}_max_bpp_bucket_report.pdf\"\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf4f62-7d30-4a80-8af2-0c30274a24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Execute Velocity bucketing review\")\n",
    "\n",
    "for vehicle_id in vehicle_ids:\n",
    "    print(f\"\\nProcessing vehicle {vehicle_id}...\")\n",
    "    \n",
    "    # 1. Fetch the data for this vehicle\n",
    "    vehicle_data_df = get_vehicle_data(start_time, end_time, [vehicle_id])\n",
    "    \n",
    "    if vehicle_data_df.empty:\n",
    "        print(f\"No data found for vehicle {vehicle_id}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # 2. Analyze the DataFrame to find braking events\n",
    "    braking_events, event_durations, bucket_info = analyze_filtered_braking_events(\n",
    "        df=vehicle_data_df,\n",
    "        top_speed=25.0,\n",
    "        search_window_seconds=15.0\n",
    "    )\n",
    "    \n",
    "    if not braking_events:\n",
    "        print(f\"No braking events found for vehicle {vehicle_id} matching the criteria.\")\n",
    "        continue\n",
    "    \n",
    "    # 3. Generate start velocity bucket statistics\n",
    "    df_median, df_95, df_99 = generate_start_velocity_bucket_statistics(braking_events, event_durations)\n",
    "    \n",
    "    # 4. Display results\n",
    "    print(f\"\\nResults for Vehicle {vehicle_id}:\")\n",
    "    print(f\"Total braking events found: {len(braking_events)}\")\n",
    "    print(f\"Date Range: {start_time.split()[0]} to {end_time.split()[0]}\")\n",
    "    \n",
    "    # Display bucket information\n",
    "    if bucket_info:\n",
    "        print(\"\\nMax Brake Pedal Position (BPP) Buckets:\")\n",
    "        for bucket, count in bucket_info['buckets'].items():\n",
    "            print(f\"  {bucket}%: {count} events\")\n",
    "        \n",
    "        if bucket_info['mode_bucket']:\n",
    "            print(f\"\\nMode Bucket: {bucket_info['mode_bucket']}%\")\n",
    "        if bucket_info['mode_value']:\n",
    "            print(f\"Mode Value: {bucket_info['mode_value']:.2f}%\")\n",
    "    \n",
    "    # Display the three statistics dataframes\n",
    "    print(\"\\nMedian Statistics by Start Velocity Bucket:\")\n",
    "    display(df_median)\n",
    "    \n",
    "    print(\"\\n95th Percentile Statistics by Start Velocity Bucket:\")\n",
    "    display(df_95)\n",
    "    \n",
    "    print(\"\\n99th Percentile Statistics by Start Velocity Bucket:\")        \n",
    "    display(df_99)\n",
    "    \n",
    "    generate_bucket_pdf_report(\n",
    "        bucket_type=\"Initial Velocity\",\n",
    "        df_median=df_median,\n",
    "        df_95=df_95,\n",
    "        df_99=df_99,\n",
    "        output_filename=f\"reports/init_velocity_bucket/vehicle_{vehicle_id}_initial_velocity_bucket_report.pdf\"\n",
    "    )    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
