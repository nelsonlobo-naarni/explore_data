{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a8e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import duckdb \n",
    "import warnings\n",
    "import fastparquet\n",
    "from tqdm import tqdm \n",
    "from typing import List, Optional, Union\n",
    "import psutil\n",
    "import time # For timing the execution\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Optional: adjust pandas display for debugging; you can comment these out\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7402badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_mem():\n",
    "    \"\"\"Try to return freed memory back to the OS (no-op on some platforms).\"\"\"\n",
    "    try:\n",
    "        libc = ctypes.CDLL(None)\n",
    "        if hasattr(libc, \"malloc_trim\"):\n",
    "            libc.malloc_trim(0)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22de80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DUCKDB CHUNK GENERATOR (Fixed) ---\n",
    "\n",
    "def duckdb_chunk_generator(conn, sql_query, chunk_size):\n",
    "    \"\"\"Generates Pandas DataFrames in chunks directly from DuckDB cursor.\"\"\"\n",
    "    cursor = conn.cursor() \n",
    "    cursor.execute(sql_query)\n",
    "    \n",
    "    while True:\n",
    "        # Uses the corrected method name: fetch_df_chunk\n",
    "        chunk = cursor.fetch_df_chunk(chunk_size) \n",
    "        if chunk is None or chunk.empty:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "# --- ROBUST FILE EXTRACTION (Fixed from OSErrors) ---\n",
    "\n",
    "def extract_files_to_disk(zip_path, output_dir):\n",
    "    \"\"\"Cleans directory and extracts all Parquet files from ZIP.\"\"\"\n",
    "    if output_dir.exists():\n",
    "        logging.info(f\"ðŸ§¹ Clearing existing directory: {output_dir.resolve()}\")\n",
    "        # Robust cleanup to avoid OS/lock issues\n",
    "        try:\n",
    "            shutil.rmtree(output_dir)\n",
    "        except OSError:\n",
    "             for item in output_dir.iterdir():\n",
    "                if item.is_dir():\n",
    "                    shutil.rmtree(item)\n",
    "                else:\n",
    "                    os.remove(item) \n",
    "             os.rmdir(output_dir)\n",
    "\n",
    "    output_dir.mkdir(parents=True)\n",
    "        \n",
    "    logging.info(\"ðŸ”„ Extracting ALL Parquet files from ZIP to disk...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            all_files_to_extract = [f for f in z.namelist() if f.endswith(\".parquet\")]\n",
    "            logging.info(f\"ðŸ”Ž Found {len(all_files_to_extract)} total Parquet files in archive.\")\n",
    "            for filename in all_files_to_extract:\n",
    "                z.extract(filename, path=output_dir)\n",
    "            return len(all_files_to_extract)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"âŒ ZIP file not found at: {zip_path}\") from None\n",
    "\n",
    "def setup_duckdb_query(output_dir, utc_start, utc_end, core_cols):\n",
    "    \"\"\"Sets up DuckDB connection and SQL query.\"\"\"\n",
    "    parquet_glob_path = str(output_dir.joinpath(\"**/*.parquet\"))\n",
    "    # Only select the columns you need for Stage 1 processing\n",
    "    column_list = \", \".join([f'\"{c}\"' for c in core_cols])\n",
    "    \n",
    "    # CRITICAL: Predicate Pushdown filter on the internal 'timestamp' column\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM read_parquet('{parquet_glob_path}')\n",
    "        WHERE \n",
    "            \"timestamp\" >= '{utc_start.isoformat()}' AND \n",
    "            \"timestamp\" < '{utc_end.isoformat()}'\n",
    "    \"\"\"\n",
    "    return duckdb.connect(), sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f69358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_subset(parquet_path: str, start_dt: datetime, end_dt: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a subset of the processed Feather file using a date filter\n",
    "    applied directly by DuckDB (predicate pushdown).\n",
    "    \n",
    "    Args:\n",
    "        feather_path: Path to the processed Feather file.\n",
    "        start_dt: Start datetime for the filter (inclusive).\n",
    "        end_dt: End datetime for the filter (exclusive).\n",
    "        \n",
    "    Returns:\n",
    "        A new DataFrame containing only the filtered data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data subset from {start_dt} to {end_dt}...\")\n",
    "    \n",
    "    # Use DuckDB to query the Feather file directly on disk\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # The SQL query filters rows on the disk file based on the 'timestamp' column.\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM read_parquet('{parquet_path}')\n",
    "        WHERE \n",
    "            \"timestamp\" >= '{start_dt.isoformat()}' AND \n",
    "            \"timestamp\" < '{end_dt.isoformat()}'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch the filtered, smaller DataFrame\n",
    "    df_subset = con.execute(sql_query).fetchdf()\n",
    "    con.close()\n",
    "    \n",
    "    logging.info(f\"âœ… Loaded {len(df_subset):,} rows for the requested subset.\")\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c19c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIXED CATEGORY DEFINITIONS ---\n",
    "# These fixed labels ensure your output columns are consistent across all data chunks.\n",
    "FIXED_CATEGORIES = {\n",
    "    'maxtemp_bucket': [\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"],\n",
    "    'temp_delta_bucket': [\"<2\", \"2â€“5\", \"5â€“8\", \">8\"],\n",
    "    'volt_delta_bucket': [\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"] \n",
    "}\n",
    "# ----------------------------------\n",
    "\n",
    "def create_session_report(\n",
    "    df: pd.DataFrame, \n",
    "    timezone: str = 'Asia/Kolkata',\n",
    "    min_discharging_duration_min: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs sessionization and calculates the percentage distribution of \n",
    "    key operational buckets per session using a fixed set of categories.\n",
    "    \"\"\"\n",
    "    # 1. CRITICAL STEP: Ensure data is sorted by ID and Timestamp\n",
    "    df = df.sort_values(['id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure 'timestamp' is timezone-aware IST\n",
    "    if not isinstance(df['timestamp'].dtype, pd.DatetimeTZDtype):\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', utc=True).dt.tz_convert(timezone)\n",
    "        except Exception:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "    # 2. Identify the start of a new session\n",
    "    df['id_change'] = df['id'].ne(df['id'].shift())\n",
    "    df['mode_change'] = df['alt_mode'].ne(df['alt_mode'].shift())\n",
    "    df['session_start'] = df['id_change'] | df['mode_change']\n",
    "    df['unique_session_id'] = df['session_start'].cumsum()\n",
    "\n",
    "    # 3. Aggregate Session Metrics (Non-bucket columns)\n",
    "    session_df = df.groupby('unique_session_id').agg(\n",
    "        id=('id', 'first'),\n",
    "        reg_num=('reg_num', 'first'),\n",
    "        customer=('customer', 'first'),\n",
    "        model=('model', 'first'),\n",
    "        # mode=('mode', 'first'),\n",
    "        alt_mode=('alt_mode', 'first'),\n",
    "        start_time=('timestamp', 'min'),\n",
    "        end_time=('timestamp', 'max'),\n",
    "    ) # Unique_session_id is the index\n",
    "    \n",
    "    # 4. --- NEW: Calculate Percentage of Buckets per Session using FIXED CATEGORIES ---\n",
    "    bucket_cols = ['maxtemp_bucket', 'temp_delta_bucket', 'volt_delta_bucket']\n",
    "    agg_dfs = []\n",
    "    \n",
    "    for col in bucket_cols:\n",
    "        if col in df.columns and col in FIXED_CATEGORIES:\n",
    "            \n",
    "            # --- Key Fix: Convert to CategoricalDtype with fixed categories ---\n",
    "            # This ensures all categories are present, even those with zero count.\n",
    "            df[col] = pd.Categorical(df[col], categories=FIXED_CATEGORIES[col], ordered=True)\n",
    "            # -------------------------------------------------------------------\n",
    "            \n",
    "            # Calculate value counts normalized to get percentages\n",
    "            pct_series = round(df.groupby('unique_session_id')[col].value_counts(normalize=True) * 100,2)\n",
    "            \n",
    "            # Unstack the bucket labels to create new columns, fill missing (zero) categories\n",
    "            pivot_df = pct_series.unstack(fill_value=0)\n",
    "            \n",
    "            # Rename columns for clear identification and safe use (e.g., replaces 'â€“' with '_')\n",
    "            pivot_df.columns = [\n",
    "                f\"{col}_{str(c).replace('â€“', '_').replace('<', 'lt').replace('>', 'gt')}_pct\" \n",
    "                for c in pivot_df.columns\n",
    "            ]\n",
    "            agg_dfs.append(pivot_df)\n",
    "\n",
    "    # Merge all percentage tables back into the main session_df\n",
    "    if agg_dfs:\n",
    "        for agg_df in agg_dfs:\n",
    "            session_df = session_df.join(agg_df, how='left')\n",
    "    \n",
    "    session_df = session_df.reset_index(drop=True) # Now reset index\n",
    "\n",
    "    # 5. Final Calculations and Formatting\n",
    "    \n",
    "    # Calculate Duration in MINUTES (seconds / 60) and round to 2 decimal places\n",
    "    session_df['duration'] = (\n",
    "        (session_df['end_time'] - session_df['start_time']).dt.total_seconds() / 60\n",
    "    ).round(2)\n",
    "\n",
    "    # Add sequential session number (e.g., 1, 2, 3...) per vehicle ID\n",
    "    session_df['session'] = session_df.groupby('id').cumcount() + 1\n",
    "    \n",
    "    # 6. Final Output Selection and Formatting\n",
    "    \n",
    "    session_report = session_df.copy()\n",
    "\n",
    "    # Define the core columns to ensure they are first\n",
    "    core_cols = ['id', 'reg_num', 'customer', 'model', 'alt_mode', 'session', 'start_time', 'end_time', 'duration']\n",
    "    \n",
    "    # Order columns: core columns first, then new percentage columns\n",
    "    new_cols = [c for c in session_report.columns if c not in core_cols]\n",
    "    session_report = session_report[core_cols + new_cols]\n",
    "    \n",
    "    # --- FINAL FORMATTING ---\n",
    "    # Round times to seconds precision (floor)\n",
    "    session_report['start_time'] = session_report['start_time'].dt.floor('S')\n",
    "    session_report['end_time'] = session_report['end_time'].dt.floor('S')\n",
    "    session_report['date_val'] = session_report['start_time'].dt.date\n",
    "\n",
    "    # Drop Timezone information (+05:30)\n",
    "    session_report['start_time'] = session_report['start_time'].dt.tz_localize(None)\n",
    "    session_report['end_time'] = session_report['end_time'].dt.tz_localize(None)\n",
    "    # -------------------------\n",
    "\n",
    "    return session_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97968681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 75-Day Session Analysis ---\n",
      "Processing range: 2025-09-01 to 2025-11-14\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Sessions: 100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 75/75 [02:29<00:00,  2.00s/day, Date: 2025-11-14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "âœ… ANALYSIS COMPLETE\n",
      "Total time taken: 151.01 seconds (2.52 minutes)\n",
      "Days processed:   75\n",
      "Total Rows Ingested: 51,788,694\n",
      "Final Report Shape: (5147217, 23) (Rows: 5147217, Columns: 23)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import gc\n",
    "import logging # Required for managing log levels\n",
    "from tqdm.auto import tqdm \n",
    "import time # For timing the execution\n",
    "\n",
    "# NOTE: The create_session_report and read_parquet_subset functions \n",
    "# must be defined/imported before running this code.\n",
    "\n",
    "def run_multi_day_session_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Iteratively loads, processes, and aggregates session reports for a defined number of days,\n",
    "    displaying a progress bar and suppressing INFO logging during the process.\n",
    "    \"\"\"\n",
    "    all_reports = []\n",
    "    total_rows_ingested = 0 # <--- ADD THIS LINE HERE\n",
    "\n",
    "    # --- Logging Suppression: Temporarily set log level to WARNING to hide INFO messages ---\n",
    "    current_log_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING) \n",
    "    \n",
    "    try:\n",
    "        progress_bar = tqdm(range(num_days), desc=\"Processing Daily Sessions\", unit=\"day\", colour=\"cyan\") # Keeps the cyan color you had, distinct from the other function)\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            current_start_dt = start_date + timedelta(days=i)\n",
    "            current_end_dt = current_start_dt + timedelta(days=1)\n",
    "            \n",
    "            progress_bar.set_postfix_str(f\"Date: {current_start_dt.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # 1. Load the small subset (Logging is suppressed here)\n",
    "            df_subset = read_parquet_subset(\n",
    "                parquet_path=parquet_path,\n",
    "                start_dt=current_start_dt,\n",
    "                end_dt=current_end_dt\n",
    "            )\n",
    "            \n",
    "            if df_subset.empty:\n",
    "                del df_subset\n",
    "                gc.collect()\n",
    "                continue\n",
    "            \n",
    "            total_rows_ingested += len(df_subset)   #count rows being processed\n",
    "\n",
    "            # 2. Process the subset\n",
    "            session_report_chunk = create_session_report(df_subset)\n",
    "            \n",
    "            # 3. Aggregate the result\n",
    "            all_reports.append(session_report_chunk)\n",
    "            \n",
    "            # 4. Explicit Memory Management\n",
    "            del df_subset \n",
    "            gc.collect()  \n",
    "            \n",
    "    finally:\n",
    "        # --- Restore Logging Level ---\n",
    "        logging.getLogger().setLevel(current_log_level)\n",
    "        \n",
    "    # 5. Final Concatenation\n",
    "    if all_reports:\n",
    "        final_report_df = pd.concat(all_reports, ignore_index=True)\n",
    "        return final_report_df, total_rows_ingested\n",
    "    else:\n",
    "        return pd.DataFrame(), total_rows_ingested\n",
    "\n",
    "# =========================================================================\n",
    "# --- Execution Example ---\n",
    "# =========================================================================\n",
    "\n",
    "# Define the 75-day process window\n",
    "filter_start_date = datetime(2025, 9, 1) # Start date\n",
    "total_days_to_process = 75\n",
    "\n",
    "print(f\"--- Starting 75-Day Session Analysis ---\")\n",
    "print(f\"Processing range: {filter_start_date.strftime('%Y-%m-%d')} to {(filter_start_date + timedelta(days=total_days_to_process-1)).strftime('%Y-%m-%d')}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the iterative analysis\n",
    "session_report_df, total_rows = run_multi_day_session_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=filter_start_date,\n",
    "    num_days=total_days_to_process\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_sec = end_time - start_time\n",
    "elapsed_time_min = elapsed_time_sec / 60\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"âœ… ANALYSIS COMPLETE\")\n",
    "print(f\"Total time taken: {elapsed_time_sec:.2f} seconds ({elapsed_time_min:.2f} minutes)\")\n",
    "print(f\"Days processed:   {total_days_to_process}\")\n",
    "print(f\"Total Rows Ingested: {total_rows:,}\")\n",
    "print(f\"Final Report Shape: {session_report_df.shape} (Rows: {session_report_df.shape[0]}, Columns: {session_report_df.shape[1]})\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2500b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msession_report_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbcs_analysis_sessions.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m session_report_df.head(\u001b[32m5\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# session_report_df[(session_report_df.id == '11')&(session_report_df.date_val=='2025-09-08')&(session_report_df.alt_mode == 'CHARGING')]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/core/generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/io/formats/csvs.py:270\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/io/formats/csvs.py:275\u001b[39m, in \u001b[36mCSVFormatter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._need_to_save_header:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_header()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/io/formats/csvs.py:313\u001b[39m, in \u001b[36mCSVFormatter._save_body\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_i >= end_i:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/naarni/repo/naarni_env/lib/python3.14/site-packages/pandas/io/formats/csvs.py:324\u001b[39m, in \u001b[36mCSVFormatter._save_chunk\u001b[39m\u001b[34m(self, start_i, end_i)\u001b[39m\n\u001b[32m    321\u001b[39m data = \u001b[38;5;28mlist\u001b[39m(res._iter_column_arrays())\n\u001b[32m    323\u001b[39m ix = \u001b[38;5;28mself\u001b[39m.data_index[slicer]._get_values_for_csv(**\u001b[38;5;28mself\u001b[39m._number_format)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m \u001b[43mlibwriters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/writers.pyx:56\u001b[39m, in \u001b[36mpandas._libs.writers.write_csv_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "session_report_df.to_csv('bcs_analysis_sessions.csv')\n",
    "session_report_df.head(5)\n",
    "# session_report_df[(session_report_df.id == '11')&(session_report_df.date_val=='2025-09-08')&(session_report_df.alt_mode == 'CHARGING')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05597b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def weighted_avg_factory(df, weight_col='duration'):\n",
    "    \"\"\"\n",
    "    Creates a callable function for pandas aggregation that calculates \n",
    "    the weighted average of a series using a specific weight column from the \n",
    "    original DataFrame (df) based on the group's indices.\n",
    "    \"\"\"\n",
    "    def weighted_avg(series):\n",
    "        # Retrieve the original 'duration' (weights) for the rows in the current group (series)\n",
    "        weights = df.loc[series.index, weight_col]\n",
    "        \n",
    "        # Guard against division by zero (e.g., if total duration is 0)\n",
    "        if weights.sum() == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Weighted average calculation\n",
    "        return ((series * weights).sum() / weights.sum()).round(4)\n",
    "\n",
    "    return weighted_avg\n",
    "\n",
    "# --- ASSUMING session_report_df is your input DataFrame (the 75-day report) ---\n",
    "# NOTE: Replace 'session_report_df' with the actual variable name from your script (e.g., the output of run_multi_day_session_analysis)\n",
    "df = session_report_df \n",
    "\n",
    "# Ensure 'duration' is numeric and set up the weighted average function\n",
    "df['duration'] = pd.to_numeric(df['duration'], errors='coerce')\n",
    "w_avg = weighted_avg_factory(df, 'duration')\n",
    "\n",
    "# Define all percentage columns for aggregation\n",
    "pct_cols = [col for col in df.columns if col.endswith('_pct')]\n",
    "\n",
    "# Define the full aggregation dictionary\n",
    "agg_dict = {\n",
    "    'reg_num': 'first',\n",
    "    'customer': 'first',\n",
    "    'model': 'first',\n",
    "    'session': 'count',  # Becomes total_sessions\n",
    "    'duration': 'sum'   # Becomes total_duration\n",
    "}\n",
    "\n",
    "# Add weighted average for all percentage columns\n",
    "weighted_agg_dict = {col: w_avg for col in pct_cols}\n",
    "agg_dict.update(weighted_agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff02f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perform aggregation grouped by 'id' and 'mode'\n",
    "report_agg = df.groupby(['id', 'alt_mode', 'date_val']).agg(agg_dict).reset_index()\n",
    "\n",
    "# 2. Final Formatting and Column Renaming/Reordering\n",
    "report_agg = report_agg.rename(columns={'session': 'total_sessions', 'duration': 'total_duration'})\n",
    "report_agg['total_duration'] = report_agg['total_duration'].round(2)\n",
    "\n",
    "report_agg['maxtemp_bucket_lt28_pct'] = report_agg['maxtemp_bucket_lt28_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_28_32_pct'] = report_agg['maxtemp_bucket_28_32_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_32_35_pct'] = report_agg['maxtemp_bucket_32_35_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_35_40_pct'] = report_agg['maxtemp_bucket_35_40_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_gt40_pct'] = report_agg['maxtemp_bucket_gt40_pct'].round(2)\n",
    "\n",
    "report_agg['temp_delta_bucket_lt2_pct'] = report_agg['temp_delta_bucket_lt2_pct'].round(2)\n",
    "report_agg['temp_delta_bucket_2_5_pct'] = report_agg['temp_delta_bucket_2_5_pct'].round(2)\n",
    "report_agg['temp_delta_bucket_5_8_pct'] = report_agg['temp_delta_bucket_5_8_pct'].round(2)\n",
    "report_agg['temp_delta_bucket_gt8_pct'] = report_agg['temp_delta_bucket_gt8_pct'].round(2)\n",
    "\n",
    "report_agg['volt_delta_bucket_0_10_pct'] = report_agg['volt_delta_bucket_0_10_pct'].round(2)\n",
    "report_agg['volt_delta_bucket_10_20_pct'] = report_agg['volt_delta_bucket_10_20_pct'].round(2)\n",
    "report_agg['volt_delta_bucket_20_30_pct'] = report_agg['volt_delta_bucket_20_30_pct'].round(2)\n",
    "report_agg['volt_delta_bucket_gt30_pct'] = report_agg['volt_delta_bucket_gt30_pct'].round(2)\n",
    "\n",
    "\n",
    "# Reorder columns as requested\n",
    "final_columns = [\n",
    "    'id', 'reg_num', 'customer', 'model', 'alt_mode', 'date_val','total_sessions', 'total_duration'\n",
    "] + pct_cols\n",
    "\n",
    "report_agg = report_agg[final_columns]\n",
    "\n",
    "\n",
    "report_agg.to_csv('bcs_analysis_report_v1.csv')\n",
    "report_agg.head()\n",
    "\n",
    "report_agg[(report_agg.maxtemp_bucket_35_40_pct>40)|(report_agg.maxtemp_bucket_gt40_pct>40)].to_csv('bcs_session_maxT.csv')\n",
    "report_agg[(report_agg.maxtemp_bucket_35_40_pct>40)|(report_agg.maxtemp_bucket_gt40_pct>40)].head()\n",
    "report_agg[(report_agg.temp_delta_bucket_5_8_pct>40)|(report_agg.temp_delta_bucket_gt8_pct>40)].to_csv('bcs_session_deltaT.csv')\n",
    "report_agg[(report_agg.temp_delta_bucket_5_8_pct>40)|(report_agg.temp_delta_bucket_gt8_pct>40)].head()\n",
    "report_agg[(report_agg.volt_delta_bucket_20_30_pct>40)|(report_agg.volt_delta_bucket_gt30_pct>40)].to_csv('bcs_session_deltaV.csv')\n",
    "report_agg[(report_agg.volt_delta_bucket_20_30_pct>40)|(report_agg.volt_delta_bucket_gt30_pct>40)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee07b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_detailed_hotspot(df, item_id_col='tc_id', pack_id_col='pack_id'):\n",
    "    \"\"\"\n",
    "    Computes detailed hotspot analysis for a single vehicle on a single day.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame for a single vehicle and day.\n",
    "        item_id_col (str): The column representing the individual item ID \n",
    "                           (e.g., 'tc_id' or 'pack_id').\n",
    "        pack_id_col (str): The column representing the pack ID (e.g., 'pack_id').\n",
    "                           This is separated for TC-level grouping.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Detailed hotspot metrics grouped by item_id and pack_id.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Basic Info\n",
    "    date_val = df['date_val'].iloc[0]\n",
    "    vid = df['id'].iloc[0]\n",
    "    \n",
    "    # Pre-calculate total time for the day (in seconds)\n",
    "    total_day_sec = df['dt_sec'].sum()\n",
    "    # total_day_mins = min(total_day_sec / 60.0, 1440.0) # Clipping the daily duration\n",
    "    total_day_mins = total_day_sec / 60.0\n",
    "    \n",
    "    # --- Helper to aggregate ---\n",
    "    def get_agg(subset_df, id_col, val_col, pack_col):\n",
    "        if subset_df.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        # Group by the item ID and its associated Pack ID\n",
    "        return subset_df.groupby([id_col, pack_col])[val_col].sum()\n",
    "\n",
    "    # Determine the columns to group by (Max TC/Pack and Min TC/Pack)\n",
    "    # The columns in the raw data are fixed: batt_maxtemp_tc, pack_id_max, etc.\n",
    "    group_cols = [item_id_col, pack_id_col]\n",
    "\n",
    "    # Use the relevant columns from the raw data for aggregation:\n",
    "    c_df = df[df['alt_mode'] == 'CHARGING']\n",
    "    dc_df = df[df['alt_mode'] == 'DISCHARGING']\n",
    "    \n",
    "    # 1. C-Max (Charging, Max Temp)\n",
    "    s_cmax = get_agg(c_df, 'batt_maxtemp_tc', 'dt_sec', 'pack_id_max')\n",
    "    \n",
    "    # 2. DC-Max (Discharging, Max Temp)\n",
    "    s_dcmax = get_agg(dc_df, 'batt_maxtemp_tc', 'dt_sec', 'pack_id_max')\n",
    "    \n",
    "    # 3. C-Min (Charging, Min Temp)\n",
    "    s_cmin = get_agg(c_df, 'batt_mintemp_tc', 'dt_sec', 'pack_id_min')\n",
    "    \n",
    "    # 4. DC-Min (Discharging, Min Temp)\n",
    "    s_dcmin = get_agg(dc_df, 'batt_mintemp_tc', 'dt_sec', 'pack_id_min')\n",
    "    \n",
    "    # --- Merge all series ---\n",
    "    dfs = []\n",
    "    \n",
    "    # Combine the index names for consistent merging (TC ID and Pack ID)\n",
    "    index_names = ['tc_id', 'pack_id'] \n",
    "\n",
    "    if not s_cmax.empty:\n",
    "        s_cmax.index.names = index_names\n",
    "        dfs.append(s_cmax.rename('cmax_dur'))\n",
    "        \n",
    "    if not s_dcmax.empty:\n",
    "        s_dcmax.index.names = index_names\n",
    "        dfs.append(s_dcmax.rename('dcmax_dur'))\n",
    "        \n",
    "    if not s_cmin.empty:\n",
    "        s_cmin.index.names = index_names\n",
    "        dfs.append(s_cmin.rename('cmin_dur'))\n",
    "        \n",
    "    if not s_dcmin.empty:\n",
    "        s_dcmin.index.names = index_names\n",
    "        dfs.append(s_dcmin.rename('dcmin_dur'))\n",
    "        \n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    merged = pd.concat(dfs, axis=1).fillna(0)\n",
    "    \n",
    "    # Ensure all duration columns exist\n",
    "    target_dur_cols = ['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']\n",
    "    merged = merged.reindex(columns=list(set(merged.columns) | set(target_dur_cols)), fill_value=0)\n",
    "    \n",
    "    # --- Calculations ---\n",
    "    \n",
    "    # Assign the clipped total time to the daily_dur column\n",
    "    merged['daily_dur'] = total_day_mins \n",
    "\n",
    "    # Convert seconds to minutes for Duration columns\n",
    "    merged[['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']] = merged[['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']] / 60.0\n",
    "    \n",
    "    # Calculate Percentages against the Total Day Time (mode-agnostic)\n",
    "    if total_day_mins > 0:\n",
    "        for col in target_dur_cols:\n",
    "            merged[f'{col}_pct'] = (merged[col] / total_day_mins) * 100.0\n",
    "    else:\n",
    "        for col in target_dur_cols:\n",
    "            merged[f'{col}_pct'] = 0.0\n",
    "        \n",
    "    # --- Formatting ---\n",
    "    cols_to_round = ['daily_dur','cmax_dur', 'cmax_dur_pct', 'dcmax_dur', 'dcmax_dur_pct', \n",
    "                     'cmin_dur', 'cmin_dur_pct', 'dcmin_dur', 'dcmin_dur_pct']\n",
    "    \n",
    "    # Ensure all percentage columns exist\n",
    "    for col in cols_to_round:\n",
    "        if col not in merged.columns:\n",
    "             merged[col] = 0.0\n",
    "             \n",
    "    merged[cols_to_round] = merged[cols_to_round].round(2)\n",
    "    \n",
    "    # Reset index and insert identifier columns\n",
    "    merged = merged.reset_index()\n",
    "    merged.insert(0, 'id', vid)\n",
    "    merged.insert(0, 'date_val', date_val)\n",
    "    \n",
    "    # Reorder columns\n",
    "    desired_order = ['date_val', 'id', 'tc_id', 'pack_id', 'daily_dur',\n",
    "                     'cmax_dur', 'cmax_dur_pct', 'dcmax_dur', 'dcmax_dur_pct',\n",
    "                     'cmin_dur', 'cmin_dur_pct', 'dcmin_dur', 'dcmin_dur_pct']\n",
    "    \n",
    "    return merged[desired_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub = read_parquet_subset(\"df_with_state.parquet\", datetime(2025, 9, 1), datetime(2025, 9, 2))\n",
    "# df_sub.head(500).to_csv('hotspot_ip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59875820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_day_tc_hotspot_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    all_tables = []\n",
    "    total_rows = 0\n",
    "\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        for d in tqdm(range(num_days), desc=\"Processing Days\", ncols=70):\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            df_sub[\"date_val\"] = day_start.date()\n",
    "\n",
    "            # per-vehicle\n",
    "            for vid, g in df_sub.groupby(\"id\"):\n",
    "                res = compute_detailed_hotspot(g)\n",
    "                if not res.empty:\n",
    "                    res[\"id\"] = vid\n",
    "                    all_tables.append(res)\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    if not all_tables:\n",
    "        return pd.DataFrame(), total_rows\n",
    "\n",
    "    return pd.concat(all_tables, ignore_index=True), total_rows\n",
    "\n",
    "\n",
    "start_date = datetime(2025, 9, 1)\n",
    "\n",
    "tc_hotspot_df, total_rows = run_multi_day_tc_hotspot_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=start_date,\n",
    "    num_days=75\n",
    ")\n",
    "\n",
    "print(tc_hotspot_df.head())\n",
    "print(\"Total rows processed:\", total_rows)\n",
    "\n",
    "sort_cols = ['date_val', 'id', 'pack_id', 'tc_id']\n",
    "tc_hotspot_df = tc_hotspot_df.sort_values(by=sort_cols, ascending=True).reset_index(drop=True)\n",
    "tc_hotspot_df.to_csv('hotspot.csv')\n",
    "display(tc_hotspot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29354578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "tc_hotspot_df[tc_hotspot_df.daily_dur>1440].daily_dur.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the TC-level aggregated data (hotspot.csv)\n",
    "df_hotspot = pd.read_csv('hotspot.csv')\n",
    "\n",
    "# 1. Group by pack_id and sum all duration columns (NUMERATOR)\n",
    "pack_agg = df_hotspot.groupby(['pack_id']).agg(\n",
    "    total_cmax_dur=('cmax_dur', 'sum'),\n",
    "    total_dcmax_dur=('dcmax_dur', 'sum'),\n",
    "    total_cmin_dur=('cmin_dur', 'sum'),\n",
    "    total_dcmin_dur=('dcmin_dur', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "# 2. Calculate the Grand Total for each duration column (DENOMINATOR)\n",
    "grand_totals = pack_agg[['total_cmax_dur', 'total_dcmax_dur', \n",
    "                         'total_cmin_dur', 'total_dcmin_dur']].sum()\n",
    "\n",
    "# 3. Calculate the percentage share for each pack\n",
    "for col in ['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']:\n",
    "    # Get the total duration for this specific metric\n",
    "    total_duration = grand_totals[f'total_{col}']\n",
    "    \n",
    "    if total_duration > 0:\n",
    "        # Calculate the pack's share of the total problem duration\n",
    "        pack_agg[f'{col}_Share_Pct'] = (\n",
    "            pack_agg[f'total_{col}'] / total_duration * 100\n",
    "        ).round(2)\n",
    "    else:\n",
    "        pack_agg[f'{col}_Share_Pct'] = 0.0\n",
    "\n",
    "# 4. Select the final output columns\n",
    "hotspot_pack_level = pack_agg[['pack_id', \n",
    "                             'cmax_dur_Share_Pct', 'dcmax_dur_Share_Pct',\n",
    "                             'cmin_dur_Share_Pct', 'dcmin_dur_Share_Pct']]\n",
    "\n",
    "hotspot_pack_level.to_csv('hotspot_pack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a90fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_high_delta_v_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes charging sessions for high DeltaV (> 20mV) and aggregates metrics \n",
    "    by SOC band across the entire fleet.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure necessary columns are correctly typed\n",
    "    df['volt_delta_mv'] = pd.to_numeric(df['volt_delta_mv'], errors='coerce')\n",
    "    df['total_battery_current'] = pd.to_numeric(df['total_battery_current'], errors='coerce')\n",
    "    if 'dt_sec' in df.columns:\n",
    "        df['dt_sec'] = pd.to_numeric(df['dt_sec'], errors='coerce')\n",
    "\n",
    "    # --- 1. Filter Data ---\n",
    "    df_charging = df[df['alt_mode'] == 'CHARGING'].copy()\n",
    "    df_filtered = df_charging[df_charging['volt_delta_mv'] > 20].copy()\n",
    "    \n",
    "    # Prepare absolute current column\n",
    "    df_filtered['abs_current'] = df_filtered['total_battery_current'].abs()\n",
    "\n",
    "    # --- FIX 1: Update the column name in the final structure list ---\n",
    "    final_report_cols_structure = [\n",
    "        'total_ids', 'SOC band', '% Time', \n",
    "        'mv Imbalance (median)', 'mv Imbalance (P95)', # <--- FIXED: Now uses P95\n",
    "        'Charging Current (Median)', 'Charging Current (Max)'\n",
    "    ]\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        # Return an empty DataFrame with the correct structure\n",
    "        return pd.DataFrame(columns=final_report_cols_structure)\n",
    "\n",
    "    # --- 2. Calculate Total Vehicle Count ---\n",
    "    total_unique_vehicles = df_filtered['id'].nunique()\n",
    "\n",
    "    p95_func = lambda x: x.quantile(0.95)    \n",
    "\n",
    "    # --- 3. Grouping and Aggregation ---\n",
    "    grouped_by_soc = df_filtered.groupby('soc_band_bucket').agg(\n",
    "        **{\n",
    "            'mv Imbalance (median)': ('volt_delta_mv', 'median'),\n",
    "            'mv Imbalance (P95)': ('volt_delta_mv', p95_func), # P95 is calculated here\n",
    "            'Charging Current (Median)': ('abs_current', 'median'),\n",
    "            'Charging Current (Max)': ('abs_current', 'max'),\n",
    "            'Band Duration (sec)': ('dt_sec', 'sum')            \n",
    "        }\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- CALCULATE % TIME ---\n",
    "    total_time_filtered = grouped_by_soc['Band Duration (sec)'].sum()\n",
    "\n",
    "    if total_time_filtered > 0:\n",
    "        grouped_by_soc['% Time'] = (\n",
    "            grouped_by_soc['Band Duration (sec)'] / total_time_filtered * 100\n",
    "        ).round(2)\n",
    "    else:\n",
    "        grouped_by_soc['% Time'] = 0.0\n",
    "        \n",
    "    # Drop the temporary duration column\n",
    "    grouped_by_soc = grouped_by_soc.drop(columns=['Band Duration (sec)'])\n",
    "\n",
    "    # --- 4. Final Formatting ---\n",
    "    final_report = grouped_by_soc.rename(columns={'soc_band_bucket': 'SOC band'})\n",
    "    \n",
    "    # Insert the 'Vehicle ID' column and assign the total count\n",
    "    final_report.insert(0, 'total_ids', total_unique_vehicles)\n",
    "\n",
    "    # --- FIX 2: Update rounding logic to include P95 ---\n",
    "    for col in final_report.columns:\n",
    "        # Check for 'median', 'P95', 'Max', or 'Time' to round all metrics\n",
    "        if 'median' in col or 'P95' in col or 'Max' in col or 'Time' in col:\n",
    "            final_report[col] = final_report[col].round(2)\n",
    "    # ----------------------------------------------------\n",
    "            \n",
    "    # Ensure final column order is correct (now that P95 is in the structure list)\n",
    "    final_report = final_report[[col for col in final_report_cols_structure if col in final_report.columns]]\n",
    "\n",
    "    return final_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb4e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_day_deltav_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads multi-day Parquet data and computes the fleet-wide high DeltaV report, \n",
    "    using a progress bar for the outer loop.\n",
    "    \"\"\"\n",
    "    raw_data_for_deltav = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Logging setup (remains the same to avoid NameError)\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        # --- CHANGE HERE: Wrap range(num_days) with tqdm ---\n",
    "        # NOTE: You need to have 'from tqdm import tqdm' imported in your environment\n",
    "        # Use a descriptive label like \"DeltaV Analysis Days\"\n",
    "        for d in tqdm(range(num_days), desc=\"DeltaV Analysis Days\", ncols=70): \n",
    "        # --------------------------------------------------\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            # --- Replace with your actual Parquet reading function ---\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            \n",
    "            # --- COLLECT RAW DATA ---\n",
    "            cols_needed = ['id', 'dt_sec','alt_mode', 'volt_delta_mv', 'total_battery_current', 'soc_band_bucket']\n",
    "            cols_present = [col for col in cols_needed if col in df_sub.columns]\n",
    "            \n",
    "            raw_data_for_deltav.append(df_sub[cols_present].copy())\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # RUN FLEET-WIDE DELTAV ANALYSIS (ONCE)\n",
    "    # -------------------------------------------------------------\n",
    "    if raw_data_for_deltav:\n",
    "        full_raw_data_df = pd.concat(raw_data_for_deltav, ignore_index=True)\n",
    "        delta_v_report_df = generate_high_delta_v_report(full_raw_data_df)\n",
    "    else:\n",
    "        delta_v_report_df = generate_high_delta_v_report(pd.DataFrame())\n",
    "\n",
    "    return delta_v_report_df, total_rows\n",
    "\n",
    "\n",
    "final_report, total_rows = run_multi_day_deltav_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=datetime(2025, 9, 1),\n",
    "    num_days=75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7730c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report\n",
    "final_report.to_csv('soc_band_mv_current_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_thermal_voltage_hotspot_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a report showing Thermocouple (TC) Hotspot exposure duration \n",
    "    (Max Temp/Min Temp) during high voltage imbalance (volt_delta_mv > 5).\n",
    "    \n",
    "    Required columns: dt_sec, volt_delta_mv, batt_maxtemp_tc, batt_mintemp_tc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Data Type Conversion and Preparation ---\n",
    "    df['volt_delta_mv'] = pd.to_numeric(df['volt_delta_mv'], errors='coerce')\n",
    "    \n",
    "    # Check for duration column and convert to minutes\n",
    "    if 'dt_sec' not in df.columns:\n",
    "        print(\"Error: 'dt_sec' column missing for duration calculation.\")\n",
    "        return pd.DataFrame() \n",
    "\n",
    "    df['dt_min'] = pd.to_numeric(df['dt_sec'], errors='coerce') / 60.0\n",
    "\n",
    "    # --- 2. Filtering ---\n",
    "    # Apply condition: volt_delta_mv > 5 (High Voltage Imbalance)\n",
    "    df_filtered = df[df['volt_delta_mv'] > 5].copy()\n",
    "    \n",
    "    final_report_cols_structure = [\n",
    "        'Thermocouple ID', 'Max Temp Duration (min)', 'Min Temp Duration (min)'\n",
    "    ]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        return pd.DataFrame(columns=final_report_cols_structure)\n",
    "\n",
    "    # --- 3. Aggregation for MAX Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MAX temp and sum the duration.\n",
    "    max_temp_df = df_filtered.groupby('batt_maxtemp_tc')['dt_min'].sum().reset_index()\n",
    "    max_temp_df = max_temp_df.rename(columns={\n",
    "        'batt_maxtemp_tc': 'Thermocouple ID',\n",
    "        'dt_min': 'Max Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 4. Aggregation for MIN Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MIN temp and sum the duration.\n",
    "    min_temp_df = df_filtered.groupby('batt_mintemp_tc')['dt_min'].sum().reset_index()\n",
    "    min_temp_df = min_temp_df.rename(columns={\n",
    "        'batt_mintemp_tc': 'Thermocouple ID',\n",
    "        'dt_min': 'Min Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 5. Merge and Cleanup ---\n",
    "    # Perform an Outer Join to combine both results, keeping all TCs\n",
    "    final_report = pd.merge(\n",
    "        max_temp_df, \n",
    "        min_temp_df, \n",
    "        on='Thermocouple ID', \n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs created by the outer merge with 0\n",
    "    final_report = final_report.fillna(0)\n",
    "\n",
    "    # Round all duration metrics\n",
    "    for col in final_report.columns:\n",
    "        if 'Duration' in col:\n",
    "            final_report[col] = final_report[col].round(2)\n",
    "            \n",
    "    # Filter out TCs with zero total exposure\n",
    "    final_report = final_report[\n",
    "        (final_report['Max Temp Duration (min)'] > 0) | \n",
    "        (final_report['Min Temp Duration (min)'] > 0)\n",
    "    ]\n",
    "    \n",
    "    # Ensure ID column is integer (since TC IDs are usually integers)\n",
    "    final_report['Thermocouple ID'] = final_report['Thermocouple ID'].astype(pd.Int64Dtype())\n",
    "\n",
    "    return final_report\n",
    "\n",
    "\n",
    "def run_multi_day_deltav_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads multi-day Parquet data and computes the fleet-wide high DeltaV report, \n",
    "    using a progress bar for the outer loop.\n",
    "    \"\"\"\n",
    "    raw_data = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Logging setup (remains the same to avoid NameError)\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        # --- CHANGE HERE: Wrap range(num_days) with tqdm ---\n",
    "        # NOTE: You need to have 'from tqdm import tqdm' imported in your environment\n",
    "        # Use a descriptive label like \"DeltaV Analysis Days\"\n",
    "        for d in tqdm(range(num_days), desc=\"ThermalV Analysis Days\", ncols=70): \n",
    "        # --------------------------------------------------\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            # --- Replace with your actual Parquet reading function ---\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            \n",
    "            # --- COLLECT RAW DATA ---\n",
    "            cols_needed = ['batt_maxtemp_tc','batt_mintemp_tc', 'dt_sec', 'volt_delta_mv']\n",
    "            cols_present = [col for col in cols_needed if col in df_sub.columns]\n",
    "            \n",
    "            raw_data.append(df_sub[cols_present].copy())\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # RUN FLEET-WIDE DELTAV ANALYSIS (ONCE)\n",
    "    # -------------------------------------------------------------\n",
    "    if raw_data:\n",
    "        full_raw_data_df = pd.concat(raw_data, ignore_index=True)\n",
    "        delta_v_report_df = generate_thermal_voltage_hotspot_report(full_raw_data_df)\n",
    "    else:\n",
    "        delta_v_report_df = generate_thermal_voltage_hotspot_report(pd.DataFrame())\n",
    "\n",
    "    return delta_v_report_df, total_rows\n",
    "\n",
    "\n",
    "thermal_v_tc_hotspot_report, total_rows = run_multi_day_deltav_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=datetime(2025, 9, 1),\n",
    "    num_days=75\n",
    ")\n",
    "\n",
    "display(thermal_v_tc_hotspot_report.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14113a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_v_tc_hotspot_report.to_csv('hotspot_tc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d264c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_thermal_voltage_pack_hotspot_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a report showing Pack Hotspot exposure duration \n",
    "    (Max Temp/Min Temp) during high voltage imbalance (volt_delta_mv > 5).\n",
    "    \n",
    "    Required columns: dt_sec, volt_delta_mv, batt_maxtemp_tc, batt_mintemp_tc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Data Type Conversion and Preparation ---\n",
    "    df['volt_delta_mv'] = pd.to_numeric(df['volt_delta_mv'], errors='coerce')\n",
    "    \n",
    "    # Check for duration column and convert to minutes\n",
    "    if 'dt_sec' not in df.columns:\n",
    "        print(\"Error: 'dt_sec' column missing for duration calculation.\")\n",
    "        return pd.DataFrame() \n",
    "\n",
    "    df['dt_min'] = pd.to_numeric(df['dt_sec'], errors='coerce') / 60.0\n",
    "\n",
    "    # --- 2. Filtering ---\n",
    "    # Apply condition: volt_delta_mv > 5 (High Voltage Imbalance)\n",
    "    df_filtered = df[df['volt_delta_mv'] > 5].copy()\n",
    "    \n",
    "    final_report_cols_structure = [\n",
    "        'Pack ID', 'Max Temp Duration (min)', 'Min Temp Duration (min)'\n",
    "    ]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        return pd.DataFrame(columns=final_report_cols_structure)\n",
    "\n",
    "    # --- 3. Aggregation for MAX Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MAX temp and sum the duration.\n",
    "    max_temp_df = df_filtered.groupby('pack_id_max')['dt_min'].sum().reset_index()\n",
    "    max_temp_df = max_temp_df.rename(columns={\n",
    "        'pack_id_max': 'Pack ID',\n",
    "        'dt_min': 'Max Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 4. Aggregation for MIN Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MIN temp and sum the duration.\n",
    "    min_temp_df = df_filtered.groupby('pack_id_min')['dt_min'].sum().reset_index()\n",
    "    min_temp_df = min_temp_df.rename(columns={\n",
    "        'pack_id_min': 'Pack ID',\n",
    "        'dt_min': 'Min Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 5. Merge and Cleanup ---\n",
    "    # Perform an Outer Join to combine both results, keeping all TCs\n",
    "    final_report = pd.merge(\n",
    "        max_temp_df, \n",
    "        min_temp_df, \n",
    "        on='Pack ID', \n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs created by the outer merge with 0\n",
    "    final_report = final_report.fillna(0)\n",
    "\n",
    "    # Round all duration metrics\n",
    "    for col in final_report.columns:\n",
    "        if 'Duration' in col:\n",
    "            final_report[col] = final_report[col].round(2)\n",
    "            \n",
    "    # Filter out TCs with zero total exposure\n",
    "    final_report = final_report[\n",
    "        (final_report['Max Temp Duration (min)'] > 0) | \n",
    "        (final_report['Min Temp Duration (min)'] > 0)\n",
    "    ]\n",
    "    \n",
    "    # Ensure ID column is integer (since TC IDs are usually integers)\n",
    "    final_report['Pack ID'] = final_report['Pack ID'].astype(pd.Int64Dtype())\n",
    "\n",
    "    return final_report\n",
    "\n",
    "\n",
    "def run_multi_day_deltav_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads multi-day Parquet data and computes the fleet-wide high DeltaV report, \n",
    "    using a progress bar for the outer loop.\n",
    "    \"\"\"\n",
    "    raw_data = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Logging setup (remains the same to avoid NameError)\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        # --- CHANGE HERE: Wrap range(num_days) with tqdm ---\n",
    "        # NOTE: You need to have 'from tqdm import tqdm' imported in your environment\n",
    "        # Use a descriptive label like \"DeltaV Analysis Days\"\n",
    "        for d in tqdm(range(num_days), desc=\"ThermalV Analysis Days\", ncols=70): \n",
    "        # --------------------------------------------------\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            # --- Replace with your actual Parquet reading function ---\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            \n",
    "            # --- COLLECT RAW DATA ---\n",
    "            cols_needed = ['pack_id_max','pack_id_min', 'dt_sec', 'volt_delta_mv']\n",
    "            cols_present = [col for col in cols_needed if col in df_sub.columns]\n",
    "            \n",
    "            raw_data.append(df_sub[cols_present].copy())\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # RUN FLEET-WIDE DELTAV ANALYSIS (ONCE)\n",
    "    # -------------------------------------------------------------\n",
    "    if raw_data:\n",
    "        full_raw_data_df = pd.concat(raw_data, ignore_index=True)\n",
    "        delta_v_report_df = generate_thermal_voltage_pack_hotspot_report(full_raw_data_df)\n",
    "    else:\n",
    "        delta_v_report_df = generate_thermal_voltage_pack_hotspot_report(pd.DataFrame())\n",
    "\n",
    "    return delta_v_report_df, total_rows\n",
    "\n",
    "\n",
    "thermal_v_pack_hotspot_report, total_rows = run_multi_day_deltav_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=datetime(2025, 9, 1),\n",
    "    num_days=75\n",
    ")\n",
    "\n",
    "display(thermal_v_pack_hotspot_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad53591",
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_v_pack_hotspot_report.to_csv('hotspot_pack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c17437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100)\n",
    "# tc_hotspot_df.groupby(['date_val','id','daily_dur','pack_id'])[['cmax_dur_pct','dcmax_dur_pct','cmin_dur_pct','dcmin_dur_pct']].sum().round(2).head(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naarni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
