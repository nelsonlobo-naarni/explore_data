{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5118c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import duckdb \n",
    "import warnings\n",
    "import fastparquet\n",
    "from tqdm import tqdm \n",
    "from typing import List, Optional, Union\n",
    "import psutil\n",
    "import time # For timing the execution\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Optional: adjust pandas display for debugging; you can comment these out\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3575c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mapping table with 27 entries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>AP39WG0252</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>AP39WF8589</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>AP39WF8584</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>AP39WG0271</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     reg_num   customer  model\n",
       "0  11  AP39WF8593  FB Guntur   12.5\n",
       "1   9  AP39WG0252  FB Guntur   12.5\n",
       "2   7  AP39WF8589  FB Guntur   12.5\n",
       "3  13  AP39WF8584  FB Guntur   12.5\n",
       "4  14  AP39WG0271  FB Guntur   12.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_file = \"../../../data_points/Naarni VehicleID_RegNo_links - Vehicle_mapping.csv\"\n",
    "try:\n",
    "    df_mapping = pd.read_csv(mapping_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mapping file '{mapping_file}' not found. Cannot enrich data.\")\n",
    "    # Create an empty mapping table to allow the rest of the script to run without crashing\n",
    "    df_mapping = pd.DataFrame(columns=[\"id\", \"reg_num\", \"customer\", \"model\"])\n",
    "else:\n",
    "    df_mapping = df_mapping.rename(columns={\n",
    "        \"Device No.\": \"id\",\n",
    "        \"Registration No\": \"reg_num\",\n",
    "        \"Customer\": \"customer\",\n",
    "        \"Model\": \"model\"\n",
    "    })\n",
    "    # Ensure the merge key ('id') is a string to match the chunks\n",
    "    if \"id\" in df_mapping.columns:\n",
    "        df_mapping[\"id\"] = df_mapping[\"id\"].astype(str)\n",
    "        df_mapping = df_mapping[[\"id\", \"reg_num\", \"customer\", \"model\"]]\n",
    "    else:\n",
    "        print(\"Warning: 'Device No.' column not found in mapping file.\")\n",
    "        df_mapping = pd.DataFrame(columns=[\"id\", \"reg_num\", \"customer\", \"model\"])\n",
    "\n",
    "print(f\"Loaded mapping table with {len(df_mapping)} entries.\")\n",
    "# df_mapping is now ready to be passed into the processing function\n",
    "\n",
    "df_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0cf6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_COLS = [\n",
    "    \"id\", \"timestamp\", \"dt\",\n",
    "    \"vehiclereadycondition\", \"gun_connection_status\", \"ignitionstatus\",\n",
    "    \"vehicle_speed_vcu\", \"gear_position\",\n",
    "    \"bat_soc\", \"soh\", \"total_battery_current\",\n",
    "    \"pack1_cellmax_temperature\", \"pack1_cell_min_temperature\",\n",
    "    \"pack1_maxtemperature_cell_number\", \"pack1_celltemperature_cellnumber\",\n",
    "    \"bat_voltage\", \"cellmax_voltagecellnumber\", \"cellminvoltagecellnumber\", \n",
    "    \"cell_min_voltage\",\"cell_max_voltage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7067d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_mem():\n",
    "    \"\"\"Try to return freed memory back to the OS (no-op on some platforms).\"\"\"\n",
    "    try:\n",
    "        libc = ctypes.CDLL(None)\n",
    "        if hasattr(libc, \"malloc_trim\"):\n",
    "            libc.malloc_trim(0)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35fd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_battery_temp_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Uses df.rename(inplace=False), creating one copy, which is fine for chunks\n",
    "    rename_map = {\n",
    "        \"pack1_cellmax_temperature\": \"batt_maxtemp\",\n",
    "        \"pack1_cell_min_temperature\": \"batt_mintemp\",\n",
    "        \"pack1_maxtemperature_cell_number\":\"batt_maxtemp_tc\", \n",
    "        \"pack1_celltemperature_cellnumber\":\"batt_mintemp_tc\",\n",
    "        \"cell_max_voltage\":\"batt_maxvolt\",\n",
    "        \"cellmax_voltagecellnumber\":\"batt_maxvolt_cell\",\n",
    "        \"cell_min_voltage\":\"batt_minvolt\",\n",
    "        \"cellminvoltagecellnumber\":\"batt_minvolt_cell\", \n",
    "    }\n",
    "    existing = {k: v for k, v in rename_map.items() if k in df.columns}\n",
    "    if not existing:\n",
    "        return df\n",
    "    return df.rename(columns=existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c43ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans invalid sensor values and imputes missing values using\n",
    "    forward-fill (with limit) followed by backfill.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 1. SANITISATION: Convert physically impossible values to NaN\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # --- 1a. Temperature sanitisation: -40Â°C and anything < -10Â°C is invalid ---\n",
    "    temp_cols = [\"batt_maxtemp\", \"batt_mintemp\"]\n",
    "    for col in temp_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df.loc[df[col] < -10, col] = pd.NA\n",
    "\n",
    "    # --- 1b. Pack voltages: cannot be 0V or negative ---\n",
    "    volt_cols = [\"batt_maxvolt\", \"batt_minvolt\", \"bat_voltage\"]\n",
    "    for col in volt_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df.loc[df[col] <= 0, col] = pd.NA\n",
    "\n",
    "    # --- 1c. Thermocouple sensor IDs cannot be 0 or invalid ---\n",
    "    tc_cols = [\"batt_maxtemp_tc\", \"batt_mintemp_tc\"]\n",
    "    for col in tc_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df.loc[(df[col] < 1) | (df[col] > 108), col] = pd.NA\n",
    "\n",
    "    # --- 1d. Cell IDs cannot be 0 or invalid ---\n",
    "    cell_cols = [\"batt_maxvolt_cell\", \"batt_minvolt_cell\"]\n",
    "    for col in cell_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            df.loc[(df[col] < 1) | (df[col] > 576), col] = pd.NA\n",
    "\n",
    "    # --- 1e. CURRENT SANITISATION: Clip current values outside [-2500, 2500] ---\n",
    "    current_col = \"total_battery_current\"\n",
    "    if current_col in df.columns:\n",
    "        df[current_col] = pd.to_numeric(df[current_col], errors=\"coerce\")\n",
    "        # Set values > 2500 or < -2500 to NaN\n",
    "        df.loc[abs(df[current_col]) > 2500, current_col] = pd.NA\n",
    "\n",
    "    current_col = \"total_battery_current\"\n",
    "\n",
    "    # 1. Clip positive outliers (Only the current column is affected)\n",
    "    df.loc[df[current_col] > 2500, current_col] = pd.NA\n",
    "\n",
    "    # 2. Clip negative outliers (Only the current column is affected)\n",
    "    df.loc[df[current_col] < -2500, current_col] = pd.NA\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 2. GROUPWISE FORWARD-FILL + BACKFILL IMPUTATION\n",
    "    # ---------------------------------------------------------------\n",
    "    impute_config = [\n",
    "        (\"batt_maxtemp\", 60),\n",
    "        (\"batt_mintemp\", 60),\n",
    "        (\"batt_maxtemp_tc\", 60),\n",
    "        (\"batt_mintemp_tc\", 60),\n",
    "\n",
    "        (\"batt_maxvolt\", 30),\n",
    "        (\"batt_minvolt\", 30),\n",
    "        (\"batt_maxvolt_cell\", 30),\n",
    "        (\"batt_minvolt_cell\", 30),\n",
    "\n",
    "        (\"bat_voltage\", 20),\n",
    "        (\"bat_soc\", 300),\n",
    "        (\"soh\", 300),\n",
    "    ]\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\", sort=False):\n",
    "        idx = grp.index\n",
    "\n",
    "        # Apply ffill(limit) then bfill() for each column\n",
    "        for col, limit in impute_config:\n",
    "            if col in df.columns:\n",
    "                cleaned = grp[col].ffill(limit=limit).bfill()   # key fix here\n",
    "                df.loc[idx, col] = cleaned\n",
    "\n",
    "        # Total current interpolation (smooth & symmetric)\n",
    "        if \"total_battery_current\" in df.columns:\n",
    "            df.loc[idx, \"total_battery_current\"] = grp[\"total_battery_current\"].interpolate(\n",
    "                limit=10, limit_direction=\"both\"\n",
    "            )\n",
    "\n",
    "        # Ready condition + GCS should never remain missing\n",
    "        for col in [\"vehiclereadycondition\", \"gun_connection_status\"]:\n",
    "            if col in df.columns:\n",
    "                df.loc[idx, col] = grp[col].ffill().bfill()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def prepare_df_with_state(df: pd.DataFrame, df_mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    # CRITICAL: This line creates the necessary copy, but now it's only a small chunk!\n",
    "    out = df.copy() \n",
    "    \n",
    "    # 1. Merge Vehicle Information\n",
    "    out[\"id\"] = out[\"id\"].astype(str)  # Ensure merge key type consistency\n",
    "    out = out.merge(\n",
    "        df_mapping, \n",
    "        on=\"id\", \n",
    "        how=\"left\", \n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "\n",
    "    # --- Fill Missing Mapping Information ---\n",
    "    out[\"reg_num\"] = out[\"reg_num\"].mask(\n",
    "        out[\"reg_num\"].isna(),\n",
    "        \"REGNUM_\" + out[\"id\"].astype(str)\n",
    "    )\n",
    "    out[\"customer\"] = out[\"customer\"].mask(\n",
    "        out[\"customer\"].isna(),\n",
    "        \"CUST_\" + out[\"id\"].astype(str)\n",
    "    )\n",
    "    out[\"model\"] = out[\"model\"].mask(\n",
    "        out[\"model\"].isna(),\n",
    "        \"MDL_\" + out[\"id\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # Ensure string dtype for PyArrow\n",
    "    out[\"reg_num\"] = out[\"reg_num\"].astype(str)\n",
    "    out[\"customer\"] = out[\"customer\"].astype(str)\n",
    "    out[\"model\"] = out[\"model\"].astype(str)\n",
    "\n",
    "    # 2. Basic Cleanup & Sorting (to IST)\n",
    "    # The timestamp processing remains the same for consistency and sorting\n",
    "    out[\"timestamp\"] = (\n",
    "        out[\"timestamp\"]\n",
    "        .dt.tz_localize(\"UTC\")\n",
    "        .dt.tz_convert(\"Asia/Kolkata\")\n",
    "    )\n",
    "    out[\"timestamp\"] = pd.to_datetime(out[\"timestamp\"], errors=\"coerce\")\n",
    "    out = (\n",
    "        out.dropna(subset=[\"timestamp\"])\n",
    "           .sort_values([\"id\", \"timestamp\"])\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # 3. Mode calculation (Includes new alt_mode)\n",
    "    \n",
    "    # Helper Series for Gun Connection Status (GCS)\n",
    "    gcs_raw = out[\"gun_connection_status\"]\n",
    "    gcs_num = pd.to_numeric(gcs_raw, errors=\"coerce\")\n",
    "    gcs_str = gcs_raw.astype(str).str.strip().str.lower()\n",
    "    gun_connected = (gcs_num == 1) | gcs_str.isin(\n",
    "        {\"1\", \"true\", \"yes\", \"y\", \"connected\", \"on\"}\n",
    "    )\n",
    "    gun_connected = gun_connected.fillna(False)\n",
    "    \n",
    "    # Helper Series for Vehicle Ready Condition (VRC)\n",
    "    if \"vehiclereadycondition\" in out.columns:\n",
    "        vrc_raw = out[\"vehiclereadycondition\"]\n",
    "        # Assuming VRC is numeric (1 or 0) but handling potential string/bool\n",
    "        vrc_num = pd.to_numeric(vrc_raw, errors=\"coerce\")\n",
    "        vrc_str = vrc_raw.astype(str).str.strip().str.lower()\n",
    "        vehicle_ready = (vrc_num == 1) | vrc_str.isin(\n",
    "            {\"1\", \"true\", \"yes\", \"y\", \"ready\", \"on\"}\n",
    "        )\n",
    "        # Default to not ready if missing, to prevent false DRIVING\n",
    "        vehicle_ready = vehicle_ready.fillna(False) \n",
    "    else:\n",
    "        # If column is missing, assume not ready\n",
    "        vehicle_ready = pd.Series([False] * len(out), index=out.index)\n",
    "    \n",
    "    # 3a. Original 'mode' calculation (for backward compatibility)\n",
    "    out[\"mode\"] = np.where(gun_connected, \"CHARGING\", \"DISCHARGING\")\n",
    "    \n",
    "    # 3b. NEW 'alt_mode' calculation (CHARGING, DRIVING, IDLE)\n",
    "    \n",
    "    # The alt_mode is built using three mutually exclusive conditions:\n",
    "    # 1. CHARGING: GUN_CONNECTION_STATUS == 1 (The VRC/Ignition status doesn't matter here)\n",
    "    # 2. DRIVING: GUN_CONNECTION_STATUS == 0 AND VEHICLE_READY_CONDITION == 1\n",
    "    # 3. IDLE: GUN_CONNECTION_STATUS == 0 AND VEHICLE_READY_CONDITION == 0\n",
    "    \n",
    "    conditions = [\n",
    "        gun_connected,\n",
    "        (~gun_connected) & vehicle_ready\n",
    "    ]\n",
    "    \n",
    "    choices = [\n",
    "        \"CHARGING\",\n",
    "        \"DRIVING\"\n",
    "    ]\n",
    "    \n",
    "    # The default value (else) for np.select will be IDLE, \n",
    "    # which covers the condition: (~gun_connected) & (~vehicle_ready)\n",
    "    out[\"alt_mode\"] = np.select(\n",
    "        conditions, \n",
    "        choices, \n",
    "        default=\"IDLE\"\n",
    "    )\n",
    "\n",
    "    # 4. Delta calculations\n",
    "    for col in [\"batt_maxtemp\", \"batt_mintemp\", \"batt_maxvolt\", \"batt_minvolt\"]:\n",
    "        if col in out.columns:\n",
    "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "            \n",
    "    out[\"batt_temp_delta\"] = out[\"batt_maxtemp\"] - out[\"batt_mintemp\"]\n",
    "    out[\"volt_delta_mv\"] = np.round(\n",
    "        (out[\"batt_maxvolt\"] - out[\"batt_minvolt\"]) * 1000.0, 0\n",
    "    )\n",
    "    out[\"dt_sec\"] = (\n",
    "        out.groupby(\"id\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    )\n",
    "    out.loc[out[\"dt_sec\"] > 3, \"dt_sec\"] = 0\n",
    "\n",
    "    # --- NEW: extract date from timestamp ---\n",
    "    out[\"date_val\"] = out[\"timestamp\"].dt.floor(\"D\")\n",
    "\n",
    "    # --- NEW: pack_id_max & pack_id_min from thermocouple indices (multiples of 9) ---\n",
    "\n",
    "    # Initialize as nullable integer columns\n",
    "    out[\"pack_id_max\"] = pd.Series([pd.NA] * len(out), dtype=\"Int64\")\n",
    "    out[\"pack_id_min\"] = pd.Series([pd.NA] * len(out), dtype=\"Int64\")\n",
    "\n",
    "    # batt_maxtemp_tc -> pack_id_max\n",
    "    if \"batt_maxtemp_tc\" in out.columns:\n",
    "        tc_max = pd.to_numeric(out[\"batt_maxtemp_tc\"], errors=\"coerce\")\n",
    "        mask_max = tc_max.notna() & (tc_max > 0)\n",
    "        out.loc[mask_max, \"pack_id_max\"] = (\n",
    "            ((tc_max[mask_max] - 1) // 9) + 1\n",
    "        ).astype(\"Int64\")\n",
    "\n",
    "    # batt_mintemp_tc -> pack_id_min\n",
    "    if \"batt_mintemp_tc\" in out.columns:\n",
    "        tc_min = pd.to_numeric(out[\"batt_mintemp_tc\"], errors=\"coerce\")\n",
    "        mask_min = tc_min.notna() & (tc_min > 0)\n",
    "        out.loc[mask_min, \"pack_id_min\"] = (\n",
    "            ((tc_min[mask_min] - 1) // 9) + 1\n",
    "        ).astype(\"Int64\")\n",
    "\n",
    "    # 5. Feature Buckets (Binning)\n",
    "    temp_bins = [-np.inf, 28, 32, 35, 40, np.inf]\n",
    "    temp_labels = [\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"]\n",
    "\n",
    "    delta_bins = [-np.inf, 2, 5, 8, np.inf]\n",
    "    delta_labels = [\"<2\", \"2â€“5\", \"5â€“8\", \">8\"]\n",
    "\n",
    "    volt_bins = [0, 10, 20, 30, np.inf]\n",
    "    volt_labels = [\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"]\n",
    "\n",
    "    soc_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, np.inf]\n",
    "    soc_labels = [\"0â€“10\", \"10â€“20\", \"20â€“30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\",\"70-80\", \"80-90\",\"90-100\"]    \n",
    "    \n",
    "    out[\"maxtemp_bucket\"] = pd.cut(\n",
    "        out[\"batt_maxtemp\"], bins=temp_bins, labels=temp_labels\n",
    "    )\n",
    "    out[\"temp_delta_bucket\"] = pd.cut(\n",
    "        out[\"batt_temp_delta\"], bins=delta_bins, labels=delta_labels\n",
    "    )\n",
    "    out[\"volt_delta_bucket\"] = pd.cut(\n",
    "        out[\"volt_delta_mv\"], bins=volt_bins, labels=volt_labels\n",
    "    )\n",
    "    out[\"soc_band_bucket\"] = pd.cut(\n",
    "        out[\"bat_soc\"], bins=soc_bins, labels=soc_labels\n",
    "    )    \n",
    "\n",
    "    # 6. Column selection (alt_mode is added to the list)\n",
    "    cols_keep = [\n",
    "        \"id\", \"reg_num\", \"customer\", \"model\",\n",
    "        \"timestamp\",'date_val', \"dt_sec\", \"mode\", \"alt_mode\",\n",
    "        \"vehiclereadycondition\", \"gun_connection_status\",\n",
    "        \"batt_maxtemp\", \"batt_mintemp\", \"batt_temp_delta\",\n",
    "        \"maxtemp_bucket\", \"temp_delta_bucket\",\n",
    "        \"batt_maxvolt\", \"batt_minvolt\", \"volt_delta_mv\",\n",
    "        \"volt_delta_bucket\",\n",
    "        \"batt_maxtemp_tc\", \"batt_mintemp_tc\",\n",
    "        \"pack_id_max\", \"pack_id_min\",\n",
    "        \"batt_maxvolt_cell\", \"batt_minvolt_cell\",\n",
    "        \"bat_voltage\", \"total_battery_current\",\n",
    "        \"bat_soc\",\"soc_band_bucket\", \"soh\",\n",
    "    ]\n",
    "    cols_keep = [c for c in cols_keep if c in out.columns]\n",
    "    out = out[cols_keep]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2e67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DUCKDB CHUNK GENERATOR (Fixed) ---\n",
    "\n",
    "def duckdb_chunk_generator(conn, sql_query, chunk_size):\n",
    "    \"\"\"Generates Pandas DataFrames in chunks directly from DuckDB cursor.\"\"\"\n",
    "    cursor = conn.cursor() \n",
    "    cursor.execute(sql_query)\n",
    "    \n",
    "    while True:\n",
    "        # Uses the corrected method name: fetch_df_chunk\n",
    "        chunk = cursor.fetch_df_chunk(chunk_size) \n",
    "        if chunk is None or chunk.empty:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "# --- ROBUST FILE EXTRACTION (Fixed from OSErrors) ---\n",
    "\n",
    "def extract_files_to_disk(zip_path, output_dir):\n",
    "    \"\"\"Cleans directory and extracts all Parquet files from ZIP.\"\"\"\n",
    "    if output_dir.exists():\n",
    "        logging.info(f\"ðŸ§¹ Clearing existing directory: {output_dir.resolve()}\")\n",
    "        # Robust cleanup to avoid OS/lock issues\n",
    "        try:\n",
    "            shutil.rmtree(output_dir)\n",
    "        except OSError:\n",
    "             for item in output_dir.iterdir():\n",
    "                if item.is_dir():\n",
    "                    shutil.rmtree(item)\n",
    "                else:\n",
    "                    os.remove(item) \n",
    "             os.rmdir(output_dir)\n",
    "\n",
    "    output_dir.mkdir(parents=True)\n",
    "        \n",
    "    logging.info(\"ðŸ”„ Extracting ALL Parquet files from ZIP to disk...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            all_files_to_extract = [f for f in z.namelist() if f.endswith(\".parquet\")]\n",
    "            logging.info(f\"ðŸ”Ž Found {len(all_files_to_extract)} total Parquet files in archive.\")\n",
    "            for filename in all_files_to_extract:\n",
    "                z.extract(filename, path=output_dir)\n",
    "            return len(all_files_to_extract)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"âŒ ZIP file not found at: {zip_path}\") from None\n",
    "\n",
    "def setup_duckdb_query(output_dir, utc_start, utc_end, core_cols):\n",
    "    \"\"\"Sets up DuckDB connection and SQL query.\"\"\"\n",
    "    parquet_glob_path = str(output_dir.joinpath(\"**/*.parquet\"))\n",
    "    # Only select the columns you need for Stage 1 processing\n",
    "    column_list = \", \".join([f'\"{c}\"' for c in core_cols])\n",
    "    \n",
    "    # CRITICAL: Predicate Pushdown filter on the internal 'timestamp' column\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM read_parquet('{parquet_glob_path}')\n",
    "        WHERE \n",
    "            \"timestamp\" >= '{utc_start.isoformat()}' AND \n",
    "            \"timestamp\" < '{utc_end.isoformat()}'\n",
    "    \"\"\"\n",
    "    return duckdb.connect(), sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc1a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage1_data_setup(analysis_start_date_str: str, \n",
    "                          analysis_end_date_str: str, \n",
    "                          zip_path: Path, \n",
    "                          extraction_dir: Path,\n",
    "                          force_extraction: bool = False) -> tuple[datetime, datetime, int]:\n",
    "    \"\"\"\n",
    "    Handles date range setup, IST-to-UTC conversion, file extraction, \n",
    "    and checks if data is available for processing.\n",
    "    \n",
    "    Args:\n",
    "        analysis_start_date_str: Start date in YYYY-MM-DD format.\n",
    "        analysis_end_date_str: End date in YYYY-MM-DD format.\n",
    "        zip_path: Path to the source ZIP file.\n",
    "        extraction_dir: Target directory for extracted Parquet files.\n",
    "        force_extraction: If True, always clean and re-extract files. \n",
    "                          If False, skips extraction if the directory exists.\n",
    "    \n",
    "    Returns: (utc_start, utc_end, file_count)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Date Parsing and UTC Conversion (Assuming +5:30 IST offset)\n",
    "    target_date = datetime.strptime(analysis_start_date_str, \"%Y-%m-%d\").date()\n",
    "    ist_start = datetime.combine(target_date, datetime.min.time())\n",
    "    \n",
    "    end_date_obj = datetime.strptime(analysis_end_date_str, \"%Y-%m-%d\").date()\n",
    "    ist_end = datetime.combine(end_date_obj, datetime.min.time()) + timedelta(days=1)\n",
    "    \n",
    "    utc_start = ist_start - timedelta(hours=5, minutes=30)\n",
    "    utc_end = ist_end - timedelta(hours=5, minutes=30)\n",
    "    \n",
    "    logging.info(f\"ðŸ” Analysis window (UTC): {utc_start} â†’ {utc_end}\")\n",
    "\n",
    "    # 2. FILE EXTRACTION CONTROL\n",
    "    file_count = 0\n",
    "    \n",
    "    if extraction_dir.exists() and not force_extraction:\n",
    "        logging.info(\"â™»ï¸ Skipping file extraction: Directory exists and force_extraction=False.\")\n",
    "        # Recursively count all .parquet files in the existing directory\n",
    "        file_count = len(list(extraction_dir.rglob('*.parquet')))\n",
    "        if file_count > 0:\n",
    "             logging.info(f\"âœ… Found {file_count} existing files. Proceeding to DuckDB loading.\")\n",
    "        \n",
    "    else:\n",
    "        # If directory doesn't exist, or force_extraction is True, run the full extraction.\n",
    "        logging.info(\"ðŸ”„ Running full extraction (Cleanup + Extract)...\")\n",
    "        # This relies on the robust `extract_files_to_disk` function\n",
    "        file_count = extract_files_to_disk(zip_path, extraction_dir)\n",
    "        \n",
    "    # 3. Validation Check\n",
    "    if file_count == 0:\n",
    "        logging.warning(\"ðŸ›‘ Skipping analysis: No files were found.\")\n",
    "        sys.exit() # Exit the script cleanly if no files were found\n",
    "\n",
    "    return utc_start, utc_end, file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2aee4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:18:22 - INFO - ðŸ” Analysis window (UTC): 2025-08-31 18:30:00 â†’ 2025-11-15 18:30:00\n",
      "2025-11-26 18:18:22 - INFO - â™»ï¸ Skipping file extraction: Directory exists and force_extraction=False.\n",
      "2025-11-26 18:18:22 - INFO - âœ… Found 474 existing files. Proceeding to DuckDB loading.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION (Ensure these are defined at the top of your script) ---\n",
    "ZIP_FILE_PATH = \"../../../data_points/naarni75_cpoall.zip\" \n",
    "EXTRACTION_DIR = Path(\"../../../data_points/extracted_parts/cpo_all\") \n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- Set 75-Day Date Range (Using your target dates) ---\n",
    "analysis_start_date_str = \"2025-09-01\" \n",
    "# NOTE: Using 2025-11-14 since 75 days starts on 2025-09-01 and ends on 2025-11-14.\n",
    "# Using 2025-11-15 will include the start of the 76th day (if data exists).\n",
    "analysis_end_date_str = \"2025-11-15\"   \n",
    "# --------------------------------------------------------\n",
    "\n",
    "# NEW STAGE 1 EXECUTION:\n",
    "utc_start, utc_end, file_count = run_stage1_data_setup(\n",
    "    analysis_start_date_str=analysis_start_date_str,\n",
    "    analysis_end_date_str=analysis_end_date_str,\n",
    "    zip_path=ZIP_FILE_PATH,\n",
    "    extraction_dir=EXTRACTION_DIR,\n",
    "    force_extraction = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4e8cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:18:23 - INFO - âœ… Found 29 unique vehicle IDs in the dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '22', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '41', '42', '46', '6', '7', '9']\n"
     ]
    }
   ],
   "source": [
    "# 2. SETUP DUCKDB QUERY\n",
    "conn, sql_query = setup_duckdb_query(EXTRACTION_DIR, utc_start, utc_end, CORE_COLS)\n",
    "\n",
    "# A quick DuckDB query to get the distinct IDs from the 75-day filtered dataset\n",
    "get_ids_query = f\"\"\"\n",
    "    SELECT DISTINCT id \n",
    "    FROM ({sql_query})\n",
    "\"\"\"\n",
    "# Fetch the list of IDs (this is a very small amount of data)\n",
    "vehicle_ids = conn.execute(get_ids_query).fetchdf()[\"id\"].astype(str).tolist()\n",
    "# vehicle_ids = ['3','16','18','19','32','42','6','7','9','11','12','13','14','15','20','25','27','28','29','30','31','33','35','41','46']\n",
    "\n",
    "logging.info(f\"âœ… Found {len(vehicle_ids)} unique vehicle IDs in the dataset.\")\n",
    "print(sorted(vehicle_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97dfd198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The functions 'duckdb_chunk_generator', 'rename_battery_temp_columns', \n",
    "# 'impute_missing_values', 'prepare_df_with_state', and 'free_mem' \n",
    "# MUST be defined in your Jupyter Notebook environment before calling this.\n",
    "\n",
    "def process_and_save_data(\n",
    "    conn: duckdb.DuckDBPyConnection, \n",
    "    sql_query: str, \n",
    "    chunk_size: int, \n",
    "    parquet_feather_path: str,\n",
    "    vehicle_ids: List[str],\n",
    "    df_mapping: pd.DataFrame,\n",
    "    extract_data: bool = False,\n",
    "    chunk_log_path: Optional[Union[str, Path]] = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Executes the memory-safe chunked processing loop for Stage 1.\n",
    "    \n",
    "    MODIFIED:\n",
    "      - Pre-calculates total rows for a 0-100% progress bar.\n",
    "      - Suppresses INFO logs during the loop (like run_multi_day_session_analysis).\n",
    "      - Updates progress based on rows processed.\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = Path(parquet_feather_path)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # SKIP IF FILE EXISTS\n",
    "    # -------------------------------------------------------\n",
    "    if not extract_data and output_path.exists():\n",
    "        # We use a separate connection to check the existing file\n",
    "        conn_count = duckdb.connect()\n",
    "        try:\n",
    "            total_rows = conn_count.execute(\n",
    "                f\"SELECT count(*) FROM '{parquet_feather_path}'\"\n",
    "            ).fetchone()[0]\n",
    "            logging.info(f\"âœ… Skipping: File already exists with {total_rows:,} rows.\")\n",
    "            return total_rows\n",
    "        finally:\n",
    "            conn_count.close()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # INIT & PRE-CALCULATION\n",
    "    # -------------------------------------------------------\n",
    "    logging.info(f\"ðŸ§  Preparing data stream...\")\n",
    "    \n",
    "    # 1. Get Total Rows for the Progress Bar (Enables 0-100% visualization)\n",
    "    #    We wrap the user query in a count to get the total volume.\n",
    "    try:\n",
    "        total_input_rows = conn.execute(f\"SELECT COUNT(*) FROM ({sql_query})\").fetchone()[0]\n",
    "        logging.info(f\"ðŸ“Š Total rows to process: {total_input_rows:,}\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not determine total row count: {e}. Progress bar will be indefinite.\")\n",
    "        total_input_rows = None\n",
    "\n",
    "    logging.info(f\"ðŸ’¾ Output path: {parquet_feather_path}\")\n",
    "\n",
    "    first_chunk = True\n",
    "    total_processed_rows = 0\n",
    "    chunk_index = 0\n",
    "\n",
    "    # psutil process handle (for RAM)\n",
    "    process = psutil.Process(os.getpid())\n",
    "    psutil.cpu_percent(interval=None) # Prime cpu\n",
    "\n",
    "    # Chunk timing log setup\n",
    "    log_file = None\n",
    "    if chunk_log_path is not None:\n",
    "        log_file = Path(chunk_log_path)\n",
    "        if not log_file.exists():\n",
    "            with log_file.open(\"w\") as f:\n",
    "                f.write(\"timestamp,chunk_idx,chunk_rows,total_rows,duration_sec,rows_per_sec,cpu_pct,ram_mb\\n\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # LOGGING SUPPRESSION\n",
    "    # -------------------------------------------------------\n",
    "    # Save current level and set to WARNING to hide internal INFO logs during the bar update\n",
    "    current_log_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try:\n",
    "        # -------------------------------------------------------\n",
    "        # PROGRESS BAR\n",
    "        # -------------------------------------------------------\n",
    "        # We use 'total=total_input_rows' to get the percentage bar\n",
    "        progress = tqdm(\n",
    "            total=total_input_rows,\n",
    "            desc=\"Processing Data\",\n",
    "            unit=\"row\",\n",
    "            mininterval=0.2,\n",
    "            dynamic_ncols=True,\n",
    "            colour=\"cyan\" # Keeps the cyan color you had, distinct from the other function\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # MAIN LOOP: STREAM CHUNKS\n",
    "        # -------------------------------------------------------\n",
    "        for chunk in duckdb_chunk_generator(conn, sql_query, chunk_size):\n",
    "            t0 = time.perf_counter()\n",
    "            chunk_index += 1\n",
    "            \n",
    "            # We capture the raw size here to update the progress bar accurately \n",
    "            # relative to the input stream\n",
    "            raw_chunk_len = len(chunk) \n",
    "\n",
    "            df_chunk = chunk\n",
    "\n",
    "            # --- PREP 1: Rename ---\n",
    "            df_chunk = rename_battery_temp_columns(df_chunk)\n",
    "\n",
    "            # --- PREP 2: Filter & Cast ---\n",
    "            if \"id\" in df_chunk.columns:\n",
    "                df_chunk[\"id\"] = df_chunk[\"id\"].astype(str)\n",
    "                if vehicle_ids:\n",
    "                    df_chunk = df_chunk[df_chunk[\"id\"].isin(vehicle_ids)]\n",
    "                df_chunk = df_chunk.convert_dtypes()\n",
    "\n",
    "            # --- PREP 3: Time & Impute ---\n",
    "            df_chunk[\"timestamp\"] = pd.to_datetime(df_chunk[\"timestamp\"], errors=\"coerce\")\n",
    "            df_chunk = impute_missing_values(df_chunk)\n",
    "\n",
    "            # --- PREP 4: State Prep ---\n",
    "            df_chunk_state = prepare_df_with_state(df_chunk, df_mapping)\n",
    "\n",
    "            if df_chunk_state.empty:\n",
    "                # Even if empty, we processed 'raw_chunk_len' rows from source\n",
    "                progress.update(raw_chunk_len) \n",
    "                progress.set_postfix_str(\"Skipped empty chunk\")\n",
    "                del df_chunk, df_chunk_state\n",
    "                gc.collect()\n",
    "                free_mem()\n",
    "                continue\n",
    "\n",
    "            rows_saved_this_chunk = len(df_chunk_state)\n",
    "            total_processed_rows += rows_saved_this_chunk\n",
    "\n",
    "            # --- SAVE CHUNK ---\n",
    "            if first_chunk:\n",
    "                df_chunk_state.to_parquet(parquet_feather_path, compression=\"zstd\", index=False)\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                fastparquet.write(parquet_feather_path, df_chunk_state, compression=\"zstd\", write_index=False, append=True)\n",
    "\n",
    "            # --- METRICS ---\n",
    "            t1 = time.perf_counter()\n",
    "            duration = t1 - t0\n",
    "            rows_per_sec = rows_saved_this_chunk / duration if duration > 0 else 0\n",
    "            cpu_pct = psutil.cpu_percent(interval=None)\n",
    "            ram_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "            # --- UPDATE PROGRESS ---\n",
    "            # Update the bar by the amount of data read from SQL (to match total_input_rows)\n",
    "            progress.update(raw_chunk_len)\n",
    "            \n",
    "            # Update the text stats with system metrics\n",
    "            progress.set_postfix(\n",
    "                saved=f\"{total_processed_rows:,}\", # Rows actually saved to disk\n",
    "                cpu=f\"{cpu_pct:4.1f}%\",\n",
    "                ram=f\"{ram_mb:6.1f}MB\",\n",
    "                speed=f\"{rows_per_sec:8.1f} r/s\"\n",
    "            )\n",
    "\n",
    "            # --- FILE LOGGING ---\n",
    "            if log_file is not None:\n",
    "                with log_file.open(\"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"{datetime.now().isoformat()},{chunk_index},\"\n",
    "                        f\"{rows_saved_this_chunk},{total_processed_rows},\"\n",
    "                        f\"{duration:.3f},{rows_per_sec:.1f},\"\n",
    "                        f\"{cpu_pct:.1f},{ram_mb:.1f}\\n\"\n",
    "                    )\n",
    "\n",
    "            # --- CLEANUP ---\n",
    "            del df_chunk, df_chunk_state\n",
    "            gc.collect()\n",
    "            free_mem()\n",
    "\n",
    "    finally:\n",
    "        # -------------------------------------------------------\n",
    "        # RESTORE LOGGING & CLOSE\n",
    "        # -------------------------------------------------------\n",
    "        if 'progress' in locals():\n",
    "            progress.close()\n",
    "        \n",
    "        # Restore the original logging level so subsequent code isn't silenced\n",
    "        logging.getLogger().setLevel(current_log_level)\n",
    "        \n",
    "        conn.close()\n",
    "\n",
    "    logging.info(f\"âœ… Finished processing. Total rows saved: {total_processed_rows:,}\")\n",
    "    return total_processed_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:18:23 - INFO - âœ… Skipping: File already exists with 52,888,998 rows.\n",
      "2025-11-26 18:18:23 - INFO - ðŸŽ‰ Final DataFrame saved. Total rows processed: 52,888,998\n",
      "2025-11-26 18:18:23 - INFO - âœ… Data Processing (Stage 1) complete. Feather file ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "# --- ASSUMING run_stage1_data_setup WAS CALLED AND RETURNED utc_start, utc_end ---\n",
    "# Example configuration that needs to be available:\n",
    "# EXTRACTION_DIR = Path(\"../extracted_parts\") \n",
    "# CORE_COLS = [...]\n",
    "# CRITICAL FIX: Drastically reduced chunk size to prevent memory spike\n",
    "CHUNK_SIZE = 1_000 # Process 50,000 rows max at any time\n",
    "\n",
    "\n",
    "# 1. Setup DuckDB Query (as shown previously)\n",
    "conn, sql_query = setup_duckdb_query(EXTRACTION_DIR, utc_start, utc_end, CORE_COLS)\n",
    "\n",
    "# 2. Define Inputs\n",
    "# output_feather_file = \"df_with_state_30days.feather\"\n",
    "output_parquet_file = \"../df_with_state.parquet\"\n",
    "\n",
    "# 3. Run the memory-safe processing loop\n",
    "total_rows = process_and_save_data(\n",
    "    conn=conn,\n",
    "    sql_query=sql_query,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    parquet_feather_path=output_parquet_file,\n",
    "    vehicle_ids=vehicle_ids,\n",
    "    df_mapping=df_mapping,\n",
    "    extract_data=True\n",
    ")\n",
    "\n",
    "logging.info(f\"ðŸŽ‰ Final DataFrame saved. Total rows processed: {total_rows:,}\")\n",
    "logging.info(\"âœ… Data Processing (Stage 1) complete. Feather file ready for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d803154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_subset(parquet_path: str, start_dt: datetime, end_dt: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a subset of the processed Feather file using a date filter\n",
    "    applied directly by DuckDB (predicate pushdown).\n",
    "    \n",
    "    Args:\n",
    "        feather_path: Path to the processed Feather file.\n",
    "        start_dt: Start datetime for the filter (inclusive).\n",
    "        end_dt: End datetime for the filter (exclusive).\n",
    "        \n",
    "    Returns:\n",
    "        A new DataFrame containing only the filtered data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data subset from {start_dt} to {end_dt}...\")\n",
    "    \n",
    "    # Use DuckDB to query the Feather file directly on disk\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # The SQL query filters rows on the disk file based on the 'timestamp' column.\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM read_parquet('{parquet_path}')\n",
    "        WHERE \n",
    "            \"timestamp\" >= '{start_dt.isoformat()}' AND \n",
    "            \"timestamp\" < '{end_dt.isoformat()}'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch the filtered, smaller DataFrame\n",
    "    df_subset = con.execute(sql_query).fetchdf()\n",
    "    con.close()\n",
    "    \n",
    "    logging.info(f\"âœ… Loaded {len(df_subset):,} rows for the requested subset.\")\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a52164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:18:23 - INFO - Loading data subset from 2025-09-01 00:00:00 to 2025-09-02 00:00:00...\n",
      "2025-11-26 18:18:23 - INFO - âœ… Loaded 155,729 rows for the requested subset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date_val</th>\n",
       "      <th>dt_sec</th>\n",
       "      <th>mode</th>\n",
       "      <th>vehiclereadycondition</th>\n",
       "      <th>gun_connection_status</th>\n",
       "      <th>batt_maxtemp</th>\n",
       "      <th>batt_mintemp</th>\n",
       "      <th>batt_temp_delta</th>\n",
       "      <th>maxtemp_bucket</th>\n",
       "      <th>temp_delta_bucket</th>\n",
       "      <th>batt_maxvolt</th>\n",
       "      <th>batt_minvolt</th>\n",
       "      <th>volt_delta_mv</th>\n",
       "      <th>volt_delta_bucket</th>\n",
       "      <th>batt_maxtemp_tc</th>\n",
       "      <th>batt_mintemp_tc</th>\n",
       "      <th>pack_id_max</th>\n",
       "      <th>pack_id_min</th>\n",
       "      <th>batt_maxvolt_cell</th>\n",
       "      <th>batt_minvolt_cell</th>\n",
       "      <th>bat_voltage</th>\n",
       "      <th>total_battery_current</th>\n",
       "      <th>bat_soc</th>\n",
       "      <th>soc_band_bucket</th>\n",
       "      <th>soh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>DL1PD8509</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 10:47:01.093000+05:30</td>\n",
       "      <td>2025-09-01 00:00:00+05:30</td>\n",
       "      <td>0.000</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>28â€“32</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.287</td>\n",
       "      <td>3.276</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10â€“20</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>503</td>\n",
       "      <td>629.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>DL1PD8509</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 10:47:03.232000+05:30</td>\n",
       "      <td>2025-09-01 00:00:00+05:30</td>\n",
       "      <td>2.139</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>28â€“32</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.287</td>\n",
       "      <td>3.276</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10â€“20</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>503</td>\n",
       "      <td>629.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>DL1PD8509</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 10:47:05.352000+05:30</td>\n",
       "      <td>2025-09-01 00:00:00+05:30</td>\n",
       "      <td>2.120</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>28â€“32</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.288</td>\n",
       "      <td>3.276</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10â€“20</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>518</td>\n",
       "      <td>277</td>\n",
       "      <td>629.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>DL1PD8509</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 10:47:07.592000+05:30</td>\n",
       "      <td>2025-09-01 00:00:00+05:30</td>\n",
       "      <td>2.240</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>28â€“32</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.287</td>\n",
       "      <td>3.276</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10â€“20</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>503</td>\n",
       "      <td>629.900024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>DL1PD8509</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 10:47:09.593000+05:30</td>\n",
       "      <td>2025-09-01 00:00:00+05:30</td>\n",
       "      <td>2.001</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>28â€“32</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.287</td>\n",
       "      <td>3.276</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10â€“20</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>277</td>\n",
       "      <td>629.900024</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30.799999</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    reg_num    customer model                        timestamp  \\\n",
       "0  18  DL1PD8509  ZB Gurgaon  12.5 2025-09-01 10:47:01.093000+05:30   \n",
       "1  18  DL1PD8509  ZB Gurgaon  12.5 2025-09-01 10:47:03.232000+05:30   \n",
       "2  18  DL1PD8509  ZB Gurgaon  12.5 2025-09-01 10:47:05.352000+05:30   \n",
       "3  18  DL1PD8509  ZB Gurgaon  12.5 2025-09-01 10:47:07.592000+05:30   \n",
       "4  18  DL1PD8509  ZB Gurgaon  12.5 2025-09-01 10:47:09.593000+05:30   \n",
       "\n",
       "                   date_val  dt_sec         mode  vehiclereadycondition  \\\n",
       "0 2025-09-01 00:00:00+05:30   0.000  DISCHARGING                      0   \n",
       "1 2025-09-01 00:00:00+05:30   2.139  DISCHARGING                      0   \n",
       "2 2025-09-01 00:00:00+05:30   2.120  DISCHARGING                      0   \n",
       "3 2025-09-01 00:00:00+05:30   2.240  DISCHARGING                      0   \n",
       "4 2025-09-01 00:00:00+05:30   2.001  DISCHARGING                      0   \n",
       "\n",
       "   gun_connection_status  batt_maxtemp  batt_mintemp  batt_temp_delta  \\\n",
       "0                      0            29            27                2   \n",
       "1                      0            29            27                2   \n",
       "2                      0            29            27                2   \n",
       "3                      0            29            27                2   \n",
       "4                      0            29            27                2   \n",
       "\n",
       "  maxtemp_bucket temp_delta_bucket  batt_maxvolt  batt_minvolt  volt_delta_mv  \\\n",
       "0          28â€“32                <2         3.287         3.276           11.0   \n",
       "1          28â€“32                <2         3.287         3.276           11.0   \n",
       "2          28â€“32                <2         3.288         3.276           12.0   \n",
       "3          28â€“32                <2         3.287         3.276           11.0   \n",
       "4          28â€“32                <2         3.287         3.276           11.0   \n",
       "\n",
       "  volt_delta_bucket  batt_maxtemp_tc  batt_mintemp_tc  pack_id_max  \\\n",
       "0             10â€“20                8               18            1   \n",
       "1             10â€“20                8               18            1   \n",
       "2             10â€“20                8               18            1   \n",
       "3             10â€“20                8               18            1   \n",
       "4             10â€“20                8               18            1   \n",
       "\n",
       "   pack_id_min  batt_maxvolt_cell  batt_minvolt_cell  bat_voltage  \\\n",
       "0            2                136                503   629.900024   \n",
       "1            2                136                503   629.900024   \n",
       "2            2                518                277   629.900024   \n",
       "3            2                135                503   629.900024   \n",
       "4            2                135                277   629.900024   \n",
       "\n",
       "   total_battery_current    bat_soc soc_band_bucket        soh  \n",
       "0                    0.0  30.799999           30-40  99.599998  \n",
       "1                    0.0  30.799999           30-40  99.599998  \n",
       "2                    0.0  30.799999           30-40  99.599998  \n",
       "3                    0.0  30.799999           30-40  99.599998  \n",
       "4                    4.5  30.799999           30-40  99.599998  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Timestamp('2025-09-01 05:30:00.937000+0530', tz='Asia/Kolkata'),\n",
       " Timestamp('2025-09-01 23:59:59.826000+0530', tz='Asia/Kolkata'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the 30-day filter window\n",
    "filter_start_date = datetime(2025, 9, 1)\n",
    "filter_end_date = datetime(2025, 9, 2) # Exclusive end date\n",
    "\n",
    "# 1. Load the filtered subset safely\n",
    "df_subset = read_parquet_subset(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_dt=filter_start_date,\n",
    "    end_dt=filter_end_date\n",
    ")\n",
    "\n",
    "display(df_subset.head())\n",
    "\n",
    "# 3. Aggressive memory cleanup after use (CRITICAL)\n",
    "# del df_subset\n",
    "# gc.collect()\n",
    "# free_mem()\n",
    "df_subset[df_subset.batt_mintemp>-40].batt_mintemp.describe()\n",
    "df_subset.volt_delta_mv.describe(percentiles=[0.9,0.95,0.9992])\n",
    "df_subset.timestamp.min(),df_subset.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfd597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a687de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(147.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.volt_delta_mv.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90acdbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-404.5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.total_battery_current[df_subset.total_battery_current>-3200].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f420b32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    135658.00\n",
       "mean          1.81\n",
       "std           0.49\n",
       "min           1.00\n",
       "50%           2.06\n",
       "90%           2.26\n",
       "95%           2.32\n",
       "99%           2.54\n",
       "99.5%         2.72\n",
       "99.7%         2.82\n",
       "max           3.00\n",
       "Name: dt_sec, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "# pd.reset_option('display.float_format')\n",
    "df_subset.head(1000).to_csv('data_ref.csv')\n",
    "df_subset[df_subset.dt_sec>1].dt_sec.describe(percentiles=[0.9,0.95,0.99,0.995,0.997]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26db9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    149022.000000\n",
       "mean        -16.725883\n",
       "std         144.110794\n",
       "min        -404.500000\n",
       "25%           0.000000\n",
       "50%           3.200000\n",
       "75%          19.299999\n",
       "max         326.700012\n",
       "Name: total_battery_current, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.total_battery_current.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0720f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subset[(df_subset.id=='3')&(df_subset.timestamp>'2025-09-01 16:12:48.737000+05:30')&(df_subset.timestamp<='2025-09-01 16:23:59.737000+05:30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32773db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id  date_val                 \n",
       "16  2025-09-01 00:00:00+05:30     959.589667\n",
       "18  2025-09-01 00:00:00+05:30     477.738667\n",
       "19  2025-09-01 00:00:00+05:30    1157.418783\n",
       "3   2025-09-01 00:00:00+05:30    1756.423883\n",
       "Name: dt_sec, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.groupby(['id','date_val'])['dt_sec'].sum()/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec9ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chk = ((df_subset.groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()*100.0/60.0)/(df_subset.groupby(['date_val'])['dt_sec'].sum()/60.0)).sort_values()\n",
    "# chk = (\n",
    "#         (df_subset[df_subset['mode'] == 'CHARGING'].groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()*100.0/60.0) / \n",
    "#         (df_subset[df_subset['mode'] == 'CHARGING'].groupby(['date_val'])['dt_sec'].sum()/60.0)\n",
    "#       ).sort_index(level='batt_maxtemp_tc')\n",
    "# display(chk)\n",
    "\n",
    "\n",
    "# # chk = ((df_subset.groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()*100.0/60.0)/(df_subset.groupby(['date_val'])['dt_sec'].sum()/60.0)).sort_values()\n",
    "# chk2 = (\n",
    "#         (df_subset[df_subset['mode'] == 'CHARGING'].groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()/60.0)).sort_index(level='batt_maxtemp_tc')\n",
    "# display(chk2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27592d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7540202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIXED CATEGORY DEFINITIONS ---\n",
    "# These fixed labels ensure your output columns are consistent across all data chunks.\n",
    "FIXED_CATEGORIES = {\n",
    "    'maxtemp_bucket': [\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"],\n",
    "    'temp_delta_bucket': [\"<2\", \"2â€“5\", \"5â€“8\", \">8\"],\n",
    "    # ðŸ’¥ UPDATED BUCKET: Using the new labels for volt_delta_bucket ðŸ’¥\n",
    "    'volt_delta_bucket': [\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"] \n",
    "}\n",
    "# ----------------------------------\n",
    "\n",
    "def create_session_report(\n",
    "    df: pd.DataFrame, \n",
    "    timezone: str = 'Asia/Kolkata',\n",
    "    min_discharging_duration_min: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs sessionization and calculates the percentage distribution of \n",
    "    key operational buckets per session using a fixed set of categories.\n",
    "    \"\"\"\n",
    "    # 1. CRITICAL STEP: Ensure data is sorted by ID and Timestamp\n",
    "    df = df.sort_values(['id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure 'timestamp' is timezone-aware IST\n",
    "    if not isinstance(df['timestamp'].dtype, pd.DatetimeTZDtype):\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', utc=True).dt.tz_convert(timezone)\n",
    "        except Exception:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "    # 2. Identify the start of a new session\n",
    "    df['id_change'] = df['id'].ne(df['id'].shift())\n",
    "    df['mode_change'] = df['mode'].ne(df['mode'].shift())\n",
    "    df['session_start'] = df['id_change'] | df['mode_change']\n",
    "    df['unique_session_id'] = df['session_start'].cumsum()\n",
    "\n",
    "    # 3. Aggregate Session Metrics (Non-bucket columns)\n",
    "    session_df = df.groupby('unique_session_id').agg(\n",
    "        id=('id', 'first'),\n",
    "        reg_num=('reg_num', 'first'),\n",
    "        customer=('customer', 'first'),\n",
    "        model=('model', 'first'),\n",
    "        mode=('mode', 'first'),\n",
    "        start_time=('timestamp', 'min'),\n",
    "        end_time=('timestamp', 'max'),\n",
    "    ) # Unique_session_id is the index\n",
    "\n",
    "    # 4. --- NEW: Calculate Percentage of Buckets per Session using FIXED CATEGORIES ---\n",
    "    bucket_cols = ['maxtemp_bucket', 'temp_delta_bucket', 'volt_delta_bucket']\n",
    "    agg_dfs = []\n",
    "    \n",
    "    for col in bucket_cols:\n",
    "        if col in df.columns and col in FIXED_CATEGORIES:\n",
    "            \n",
    "            # --- Key Fix: Convert to CategoricalDtype with fixed categories ---\n",
    "            # This ensures all categories are present, even those with zero count.\n",
    "            df[col] = pd.Categorical(df[col], categories=FIXED_CATEGORIES[col], ordered=True)\n",
    "            # -------------------------------------------------------------------\n",
    "            \n",
    "            # Calculate value counts normalized to get percentages\n",
    "            pct_series = round(df.groupby('unique_session_id')[col].value_counts(normalize=True) * 100,2)\n",
    "            \n",
    "            # Unstack the bucket labels to create new columns, fill missing (zero) categories\n",
    "            pivot_df = pct_series.unstack(fill_value=0)\n",
    "            \n",
    "            # Rename columns for clear identification and safe use (e.g., replaces 'â€“' with '_')\n",
    "            pivot_df.columns = [\n",
    "                f\"{col}_{str(c).replace('â€“', '_').replace('<', 'lt').replace('>', 'gt')}_pct\" \n",
    "                for c in pivot_df.columns\n",
    "            ]\n",
    "            agg_dfs.append(pivot_df)\n",
    "\n",
    "    # Merge all percentage tables back into the main session_df\n",
    "    if agg_dfs:\n",
    "        for agg_df in agg_dfs:\n",
    "            session_df = session_df.join(agg_df, how='left')\n",
    "    \n",
    "    session_df = session_df.reset_index(drop=True) # Now reset index\n",
    "\n",
    "    # 5. Final Calculations and Formatting\n",
    "    \n",
    "    # Calculate Duration in MINUTES (seconds / 60) and round to 2 decimal places\n",
    "    session_df['duration'] = (\n",
    "        (session_df['end_time'] - session_df['start_time']).dt.total_seconds() / 60\n",
    "    ).round(2)\n",
    "\n",
    "    # Add sequential session number (e.g., 1, 2, 3...) per vehicle ID\n",
    "    session_df['session'] = session_df.groupby('id').cumcount() + 1\n",
    "    \n",
    "    # 6. Final Output Selection and Formatting\n",
    "    \n",
    "    session_report = session_df.copy()\n",
    "\n",
    "    # Define the core columns to ensure they are first\n",
    "    core_cols = ['id', 'reg_num', 'customer', 'model', 'mode', 'session', 'start_time', 'end_time', 'duration']\n",
    "    \n",
    "    # Order columns: core columns first, then new percentage columns\n",
    "    new_cols = [c for c in session_report.columns if c not in core_cols]\n",
    "    session_report = session_report[core_cols + new_cols]\n",
    "    \n",
    "    # --- FINAL FORMATTING ---\n",
    "    # Round times to seconds precision (floor)\n",
    "    session_report['start_time'] = session_report['start_time'].dt.floor('S')\n",
    "    session_report['end_time'] = session_report['end_time'].dt.floor('S')\n",
    "    session_report['date_val'] = session_report['start_time'].dt.date\n",
    "\n",
    "    # Drop Timezone information (+05:30)\n",
    "    session_report['start_time'] = session_report['start_time'].dt.tz_localize(None)\n",
    "    session_report['end_time'] = session_report['end_time'].dt.tz_localize(None)\n",
    "    # -------------------------\n",
    "\n",
    "    return session_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb937bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 75-Day Session Analysis ---\n",
      "Processing range: 2025-09-01 to 2025-11-14\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Daily Sessions: 100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 75/75 [01:45<00:00,  1.40s/day, Date: 2025-11-14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "âœ… ANALYSIS COMPLETE\n",
      "Total time taken: 105.37 seconds (1.76 minutes)\n",
      "Days processed:   75\n",
      "Total Rows Ingested: 51,788,694\n",
      "Final Report Shape: (18507, 23) (Rows: 18507, Columns: 23)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import gc\n",
    "import logging # Required for managing log levels\n",
    "from tqdm.auto import tqdm \n",
    "import time # For timing the execution\n",
    "\n",
    "# NOTE: The create_session_report and read_parquet_subset functions \n",
    "# must be defined/imported before running this code.\n",
    "\n",
    "def run_multi_day_session_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Iteratively loads, processes, and aggregates session reports for a defined number of days,\n",
    "    displaying a progress bar and suppressing INFO logging during the process.\n",
    "    \"\"\"\n",
    "    all_reports = []\n",
    "    total_rows_ingested = 0 # <--- ADD THIS LINE HERE\n",
    "\n",
    "    # --- Logging Suppression: Temporarily set log level to WARNING to hide INFO messages ---\n",
    "    current_log_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING) \n",
    "    \n",
    "    try:\n",
    "        progress_bar = tqdm(range(num_days), desc=\"Processing Daily Sessions\", unit=\"day\", colour=\"cyan\") # Keeps the cyan color you had, distinct from the other function)\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            current_start_dt = start_date + timedelta(days=i)\n",
    "            current_end_dt = current_start_dt + timedelta(days=1)\n",
    "            \n",
    "            progress_bar.set_postfix_str(f\"Date: {current_start_dt.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # 1. Load the small subset (Logging is suppressed here)\n",
    "            df_subset = read_parquet_subset(\n",
    "                parquet_path=parquet_path,\n",
    "                start_dt=current_start_dt,\n",
    "                end_dt=current_end_dt\n",
    "            )\n",
    "            \n",
    "            if df_subset.empty:\n",
    "                del df_subset\n",
    "                gc.collect()\n",
    "                continue\n",
    "            \n",
    "            total_rows_ingested += len(df_subset)   #count rows being processed\n",
    "\n",
    "            # 2. Process the subset\n",
    "            session_report_chunk = create_session_report(df_subset)\n",
    "            \n",
    "            # 3. Aggregate the result\n",
    "            all_reports.append(session_report_chunk)\n",
    "            \n",
    "            # 4. Explicit Memory Management\n",
    "            del df_subset \n",
    "            gc.collect()  \n",
    "            \n",
    "    finally:\n",
    "        # --- Restore Logging Level ---\n",
    "        logging.getLogger().setLevel(current_log_level)\n",
    "        \n",
    "    # 5. Final Concatenation\n",
    "    if all_reports:\n",
    "        final_report_df = pd.concat(all_reports, ignore_index=True)\n",
    "        return final_report_df, total_rows_ingested\n",
    "    else:\n",
    "        return pd.DataFrame(), total_rows_ingested\n",
    "\n",
    "# =========================================================================\n",
    "# --- Execution Example ---\n",
    "# =========================================================================\n",
    "\n",
    "# Define the 75-day process window\n",
    "filter_start_date = datetime(2025, 9, 1) # Start date\n",
    "total_days_to_process = 75\n",
    "\n",
    "print(f\"--- Starting 75-Day Session Analysis ---\")\n",
    "print(f\"Processing range: {filter_start_date.strftime('%Y-%m-%d')} to {(filter_start_date + timedelta(days=total_days_to_process-1)).strftime('%Y-%m-%d')}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the iterative analysis\n",
    "session_report_df, total_rows = run_multi_day_session_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=filter_start_date,\n",
    "    num_days=total_days_to_process\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_sec = end_time - start_time\n",
    "elapsed_time_min = elapsed_time_sec / 60\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"âœ… ANALYSIS COMPLETE\")\n",
    "print(f\"Total time taken: {elapsed_time_sec:.2f} seconds ({elapsed_time_min:.2f} minutes)\")\n",
    "print(f\"Days processed:   {total_days_to_process}\")\n",
    "print(f\"Total Rows Ingested: {total_rows:,}\")\n",
    "print(f\"Final Report Shape: {session_report_df.shape} (Rows: {session_report_df.shape[0]}, Columns: {session_report_df.shape[1]})\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9607f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>session</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>duration</th>\n",
       "      <th>maxtemp_bucket_lt28_pct</th>\n",
       "      <th>maxtemp_bucket_28_32_pct</th>\n",
       "      <th>maxtemp_bucket_32_35_pct</th>\n",
       "      <th>maxtemp_bucket_35_40_pct</th>\n",
       "      <th>maxtemp_bucket_gt40_pct</th>\n",
       "      <th>temp_delta_bucket_lt2_pct</th>\n",
       "      <th>temp_delta_bucket_2_5_pct</th>\n",
       "      <th>temp_delta_bucket_5_8_pct</th>\n",
       "      <th>temp_delta_bucket_gt8_pct</th>\n",
       "      <th>volt_delta_bucket_0_10_pct</th>\n",
       "      <th>volt_delta_bucket_10_20_pct</th>\n",
       "      <th>volt_delta_bucket_20_30_pct</th>\n",
       "      <th>volt_delta_bucket_gt30_pct</th>\n",
       "      <th>date_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-01 05:30:00</td>\n",
       "      <td>2025-09-01 05:37:36</td>\n",
       "      <td>7.59</td>\n",
       "      <td>86.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.09</td>\n",
       "      <td>7.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-01 05:39:01</td>\n",
       "      <td>2025-09-01 06:13:04</td>\n",
       "      <td>34.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.89</td>\n",
       "      <td>61.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>99.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.15</td>\n",
       "      <td>92.80</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-01 06:13:05</td>\n",
       "      <td>2025-09-01 06:13:06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2025-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-09-01 06:13:07</td>\n",
       "      <td>2025-09-01 07:22:50</td>\n",
       "      <td>69.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.74</td>\n",
       "      <td>32.25</td>\n",
       "      <td>51.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.76</td>\n",
       "      <td>21.03</td>\n",
       "      <td>29.21</td>\n",
       "      <td>2025-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>DISCHARGING</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-09-01 07:23:05</td>\n",
       "      <td>2025-09-01 18:00:51</td>\n",
       "      <td>637.76</td>\n",
       "      <td>34.86</td>\n",
       "      <td>53.78</td>\n",
       "      <td>11.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>84.24</td>\n",
       "      <td>15.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.48</td>\n",
       "      <td>18.98</td>\n",
       "      <td>3.83</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2025-09-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     reg_num    customer model         mode  session          start_time  \\\n",
       "0  16  HR55AY9237  ZB Gurgaon  12.5  DISCHARGING        1 2025-09-01 05:30:00   \n",
       "1  16  HR55AY9237  ZB Gurgaon  12.5     CHARGING        2 2025-09-01 05:39:01   \n",
       "2  16  HR55AY9237  ZB Gurgaon  12.5  DISCHARGING        3 2025-09-01 06:13:05   \n",
       "3  16  HR55AY9237  ZB Gurgaon  12.5     CHARGING        4 2025-09-01 06:13:07   \n",
       "4  16  HR55AY9237  ZB Gurgaon  12.5  DISCHARGING        5 2025-09-01 07:23:05   \n",
       "\n",
       "             end_time  duration  maxtemp_bucket_lt28_pct  \\\n",
       "0 2025-09-01 05:37:36      7.59                    86.28   \n",
       "1 2025-09-01 06:13:04     34.05                     0.00   \n",
       "2 2025-09-01 06:13:06      0.02                     0.00   \n",
       "3 2025-09-01 07:22:50     69.71                     0.00   \n",
       "4 2025-09-01 18:00:51    637.76                    34.86   \n",
       "\n",
       "   maxtemp_bucket_28_32_pct  maxtemp_bucket_32_35_pct  \\\n",
       "0                      0.00                     13.72   \n",
       "1                      0.00                     38.89   \n",
       "2                      0.00                      0.00   \n",
       "3                     16.74                     32.25   \n",
       "4                     53.78                     11.36   \n",
       "\n",
       "   maxtemp_bucket_35_40_pct  maxtemp_bucket_gt40_pct  \\\n",
       "0                      0.00                      0.0   \n",
       "1                     61.11                      0.0   \n",
       "2                    100.00                      0.0   \n",
       "3                     51.02                      0.0   \n",
       "4                      0.00                      0.0   \n",
       "\n",
       "   temp_delta_bucket_lt2_pct  temp_delta_bucket_2_5_pct  \\\n",
       "0                      86.28                       0.00   \n",
       "1                       0.00                       0.05   \n",
       "2                       0.00                       0.00   \n",
       "3                       0.00                       0.00   \n",
       "4                       0.01                      84.24   \n",
       "\n",
       "   temp_delta_bucket_5_8_pct  temp_delta_bucket_gt8_pct  \\\n",
       "0                      13.72                        0.0   \n",
       "1                      99.95                        0.0   \n",
       "2                     100.00                        0.0   \n",
       "3                     100.00                        0.0   \n",
       "4                      15.75                        0.0   \n",
       "\n",
       "   volt_delta_bucket_0_10_pct  volt_delta_bucket_10_20_pct  \\\n",
       "0                       92.09                         7.91   \n",
       "1                        7.15                        92.80   \n",
       "2                        0.00                       100.00   \n",
       "3                        0.00                        49.76   \n",
       "4                       75.48                        18.98   \n",
       "\n",
       "   volt_delta_bucket_20_30_pct  volt_delta_bucket_gt30_pct    date_val  \n",
       "0                         0.00                        0.00  2025-09-01  \n",
       "1                         0.05                        0.00  2025-09-01  \n",
       "2                         0.00                        0.00  2025-09-01  \n",
       "3                        21.03                       29.21  2025-09-01  \n",
       "4                         3.83                        1.71  2025-09-01  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_report_df.to_csv('bcs_analysis_sessions.csv')\n",
    "session_report_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b806329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def weighted_avg_factory(df, weight_col='duration'):\n",
    "    \"\"\"\n",
    "    Creates a callable function for pandas aggregation that calculates \n",
    "    the weighted average of a series using a specific weight column from the \n",
    "    original DataFrame (df) based on the group's indices.\n",
    "    \"\"\"\n",
    "    def weighted_avg(series):\n",
    "        # Retrieve the original 'duration' (weights) for the rows in the current group (series)\n",
    "        weights = df.loc[series.index, weight_col]\n",
    "        \n",
    "        # Guard against division by zero (e.g., if total duration is 0)\n",
    "        if weights.sum() == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Weighted average calculation\n",
    "        return ((series * weights).sum() / weights.sum()).round(4)\n",
    "\n",
    "    return weighted_avg\n",
    "\n",
    "# --- ASSUMING session_report_df is your input DataFrame (the 75-day report) ---\n",
    "# NOTE: Replace 'session_report_df' with the actual variable name from your script (e.g., the output of run_multi_day_session_analysis)\n",
    "df = session_report_df \n",
    "\n",
    "# Ensure 'duration' is numeric and set up the weighted average function\n",
    "df['duration'] = pd.to_numeric(df['duration'], errors='coerce')\n",
    "w_avg = weighted_avg_factory(df, 'duration')\n",
    "\n",
    "# Define all percentage columns for aggregation\n",
    "pct_cols = [col for col in df.columns if col.endswith('_pct')]\n",
    "\n",
    "# Define the full aggregation dictionary\n",
    "agg_dict = {\n",
    "    'reg_num': 'first',\n",
    "    'customer': 'first',\n",
    "    'model': 'first',\n",
    "    'session': 'count',  # Becomes total_sessions\n",
    "    'duration': 'sum'   # Becomes total_duration\n",
    "}\n",
    "\n",
    "# Add weighted average for all percentage columns\n",
    "weighted_agg_dict = {col: w_avg for col in pct_cols}\n",
    "agg_dict.update(weighted_agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbbb1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perform aggregation grouped by 'id' and 'mode'\n",
    "report_agg = df.groupby(['id', 'mode', 'date_val']).agg(agg_dict).reset_index()\n",
    "\n",
    "# 2. Final Formatting and Column Renaming/Reordering\n",
    "report_agg = report_agg.rename(columns={'session': 'total_sessions', 'duration': 'total_duration'})\n",
    "report_agg['total_duration'] = report_agg['total_duration'].round(2)\n",
    "\n",
    "report_agg['maxtemp_bucket_lt28_pct'] = report_agg['maxtemp_bucket_lt28_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_28_32_pct'] = report_agg['maxtemp_bucket_28_32_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_32_35_pct'] = report_agg['maxtemp_bucket_32_35_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_35_40_pct'] = report_agg['maxtemp_bucket_35_40_pct'].round(2)\n",
    "report_agg['maxtemp_bucket_gt40_pct'] = report_agg['maxtemp_bucket_gt40_pct'].round(2)\n",
    "\n",
    "report_agg['temp_delta_bucket_lt2_pct'] = report_agg['temp_delta_bucket_lt2_pct'].round(2)\n",
    "report_agg['temp_delta_bucket_2_5_pct'] = report_agg['temp_delta_bucket_2_5_pct'].round(2)\n",
    "report_agg['temp_delta_bucket_5_8_pct'] = report_agg['temp_delta_bucket_5_8_pct'].round(2)\n",
    "report_agg['temp_delta_bucket_gt8_pct'] = report_agg['temp_delta_bucket_gt8_pct'].round(2)\n",
    "\n",
    "report_agg['volt_delta_bucket_0_10_pct'] = report_agg['volt_delta_bucket_0_10_pct'].round(2)\n",
    "report_agg['volt_delta_bucket_10_20_pct'] = report_agg['volt_delta_bucket_10_20_pct'].round(2)\n",
    "report_agg['volt_delta_bucket_20_30_pct'] = report_agg['volt_delta_bucket_20_30_pct'].round(2)\n",
    "report_agg['volt_delta_bucket_gt30_pct'] = report_agg['volt_delta_bucket_gt30_pct'].round(2)\n",
    "\n",
    "\n",
    "# Reorder columns as requested\n",
    "final_columns = [\n",
    "    'id', 'reg_num', 'customer', 'model', 'mode', 'date_val','total_sessions', 'total_duration'\n",
    "] + pct_cols\n",
    "\n",
    "report_agg = report_agg[final_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75b6d8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>date_val</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>total_duration</th>\n",
       "      <th>maxtemp_bucket_lt28_pct</th>\n",
       "      <th>maxtemp_bucket_28_32_pct</th>\n",
       "      <th>maxtemp_bucket_32_35_pct</th>\n",
       "      <th>maxtemp_bucket_35_40_pct</th>\n",
       "      <th>maxtemp_bucket_gt40_pct</th>\n",
       "      <th>temp_delta_bucket_lt2_pct</th>\n",
       "      <th>temp_delta_bucket_2_5_pct</th>\n",
       "      <th>temp_delta_bucket_5_8_pct</th>\n",
       "      <th>temp_delta_bucket_gt8_pct</th>\n",
       "      <th>volt_delta_bucket_0_10_pct</th>\n",
       "      <th>volt_delta_bucket_10_20_pct</th>\n",
       "      <th>volt_delta_bucket_20_30_pct</th>\n",
       "      <th>volt_delta_bucket_gt30_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>REGNUM_10</td>\n",
       "      <td>CUST_10</td>\n",
       "      <td>MDL_10</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>3</td>\n",
       "      <td>219.73</td>\n",
       "      <td>42.55</td>\n",
       "      <td>57.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.04</td>\n",
       "      <td>53.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.06</td>\n",
       "      <td>32.38</td>\n",
       "      <td>6.42</td>\n",
       "      <td>13.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>REGNUM_10</td>\n",
       "      <td>CUST_10</td>\n",
       "      <td>MDL_10</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>2</td>\n",
       "      <td>81.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.59</td>\n",
       "      <td>67.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.66</td>\n",
       "      <td>14.82</td>\n",
       "      <td>11.02</td>\n",
       "      <td>33.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>REGNUM_10</td>\n",
       "      <td>CUST_10</td>\n",
       "      <td>MDL_10</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>8</td>\n",
       "      <td>130.99</td>\n",
       "      <td>13.30</td>\n",
       "      <td>86.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.88</td>\n",
       "      <td>21.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>REGNUM_10</td>\n",
       "      <td>CUST_10</td>\n",
       "      <td>MDL_10</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-11-05</td>\n",
       "      <td>1</td>\n",
       "      <td>73.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77.14</td>\n",
       "      <td>22.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.38</td>\n",
       "      <td>31.60</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>REGNUM_10</td>\n",
       "      <td>CUST_10</td>\n",
       "      <td>MDL_10</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-11-06</td>\n",
       "      <td>4</td>\n",
       "      <td>116.11</td>\n",
       "      <td>36.43</td>\n",
       "      <td>40.49</td>\n",
       "      <td>23.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.43</td>\n",
       "      <td>38.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.02</td>\n",
       "      <td>17.84</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    reg_num customer   model      mode    date_val  total_sessions  \\\n",
       "0  10  REGNUM_10  CUST_10  MDL_10  CHARGING  2025-09-30               3   \n",
       "1  10  REGNUM_10  CUST_10  MDL_10  CHARGING  2025-10-28               2   \n",
       "2  10  REGNUM_10  CUST_10  MDL_10  CHARGING  2025-10-29               8   \n",
       "3  10  REGNUM_10  CUST_10  MDL_10  CHARGING  2025-11-05               1   \n",
       "4  10  REGNUM_10  CUST_10  MDL_10  CHARGING  2025-11-06               4   \n",
       "\n",
       "   total_duration  maxtemp_bucket_lt28_pct  maxtemp_bucket_28_32_pct  \\\n",
       "0          219.73                    42.55                     57.45   \n",
       "1           81.46                     0.00                    100.00   \n",
       "2          130.99                    13.30                     86.70   \n",
       "3           73.43                     0.00                     77.14   \n",
       "4          116.11                    36.43                     40.49   \n",
       "\n",
       "   maxtemp_bucket_32_35_pct  maxtemp_bucket_35_40_pct  \\\n",
       "0                      0.00                       0.0   \n",
       "1                      0.00                       0.0   \n",
       "2                      0.00                       0.0   \n",
       "3                     22.86                       0.0   \n",
       "4                     23.08                       0.0   \n",
       "\n",
       "   maxtemp_bucket_gt40_pct  temp_delta_bucket_lt2_pct  \\\n",
       "0                      0.0                      46.04   \n",
       "1                      0.0                      32.59   \n",
       "2                      0.0                       0.00   \n",
       "3                      0.0                     100.00   \n",
       "4                      0.0                      61.43   \n",
       "\n",
       "   temp_delta_bucket_2_5_pct  temp_delta_bucket_5_8_pct  \\\n",
       "0                      53.96                       0.00   \n",
       "1                      67.41                       0.00   \n",
       "2                      78.88                      21.12   \n",
       "3                       0.00                       0.00   \n",
       "4                      38.57                       0.00   \n",
       "\n",
       "   temp_delta_bucket_gt8_pct  volt_delta_bucket_0_10_pct  \\\n",
       "0                        0.0                       48.06   \n",
       "1                        0.0                       40.66   \n",
       "2                        0.0                      100.00   \n",
       "3                        0.0                       66.38   \n",
       "4                        0.0                       80.02   \n",
       "\n",
       "   volt_delta_bucket_10_20_pct  volt_delta_bucket_20_30_pct  \\\n",
       "0                        32.38                         6.42   \n",
       "1                        14.82                        11.02   \n",
       "2                         0.00                         0.00   \n",
       "3                        31.60                         2.02   \n",
       "4                        17.84                         2.14   \n",
       "\n",
       "   volt_delta_bucket_gt30_pct  \n",
       "0                       13.14  \n",
       "1                       33.50  \n",
       "2                        0.00  \n",
       "3                        0.00  \n",
       "4                        0.00  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_agg.to_csv('bcs_analysis_report_v1.csv')\n",
    "report_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9a55d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>date_val</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>total_duration</th>\n",
       "      <th>maxtemp_bucket_lt28_pct</th>\n",
       "      <th>maxtemp_bucket_28_32_pct</th>\n",
       "      <th>maxtemp_bucket_32_35_pct</th>\n",
       "      <th>maxtemp_bucket_35_40_pct</th>\n",
       "      <th>maxtemp_bucket_gt40_pct</th>\n",
       "      <th>temp_delta_bucket_lt2_pct</th>\n",
       "      <th>temp_delta_bucket_2_5_pct</th>\n",
       "      <th>temp_delta_bucket_5_8_pct</th>\n",
       "      <th>temp_delta_bucket_gt8_pct</th>\n",
       "      <th>volt_delta_bucket_0_10_pct</th>\n",
       "      <th>volt_delta_bucket_10_20_pct</th>\n",
       "      <th>volt_delta_bucket_20_30_pct</th>\n",
       "      <th>volt_delta_bucket_gt30_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>53.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.33</td>\n",
       "      <td>85.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.24</td>\n",
       "      <td>79.02</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-06</td>\n",
       "      <td>5</td>\n",
       "      <td>332.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>34.48</td>\n",
       "      <td>56.00</td>\n",
       "      <td>8.82</td>\n",
       "      <td>10.49</td>\n",
       "      <td>89.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.82</td>\n",
       "      <td>31.04</td>\n",
       "      <td>7.50</td>\n",
       "      <td>29.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-07</td>\n",
       "      <td>6</td>\n",
       "      <td>264.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>39.21</td>\n",
       "      <td>43.42</td>\n",
       "      <td>10.24</td>\n",
       "      <td>8.01</td>\n",
       "      <td>81.49</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.57</td>\n",
       "      <td>42.38</td>\n",
       "      <td>2.93</td>\n",
       "      <td>10.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>117</td>\n",
       "      <td>277.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>27.85</td>\n",
       "      <td>65.32</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.47</td>\n",
       "      <td>68.09</td>\n",
       "      <td>27.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.87</td>\n",
       "      <td>30.55</td>\n",
       "      <td>2.63</td>\n",
       "      <td>16.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>4</td>\n",
       "      <td>244.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.06</td>\n",
       "      <td>20.42</td>\n",
       "      <td>65.94</td>\n",
       "      <td>10.57</td>\n",
       "      <td>4.84</td>\n",
       "      <td>78.58</td>\n",
       "      <td>16.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.68</td>\n",
       "      <td>31.34</td>\n",
       "      <td>2.33</td>\n",
       "      <td>28.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     reg_num   customer model      mode    date_val  total_sessions  \\\n",
       "20  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-05               1   \n",
       "21  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-06               5   \n",
       "22  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-07               6   \n",
       "23  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-08             117   \n",
       "34  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-19               4   \n",
       "\n",
       "    total_duration  maxtemp_bucket_lt28_pct  maxtemp_bucket_28_32_pct  \\\n",
       "20           53.10                      0.0                      0.00   \n",
       "21          332.99                      0.0                      0.71   \n",
       "22          264.72                      0.0                      7.14   \n",
       "23          277.15                      0.0                      2.89   \n",
       "34          244.54                      0.0                      3.06   \n",
       "\n",
       "    maxtemp_bucket_32_35_pct  maxtemp_bucket_35_40_pct  \\\n",
       "20                     14.33                     85.67   \n",
       "21                     34.48                     56.00   \n",
       "22                     39.21                     43.42   \n",
       "23                     27.85                     65.32   \n",
       "34                     20.42                     65.94   \n",
       "\n",
       "    maxtemp_bucket_gt40_pct  temp_delta_bucket_lt2_pct  \\\n",
       "20                     0.00                       0.00   \n",
       "21                     8.82                      10.49   \n",
       "22                    10.24                       8.01   \n",
       "23                     3.94                       4.47   \n",
       "34                    10.57                       4.84   \n",
       "\n",
       "    temp_delta_bucket_2_5_pct  temp_delta_bucket_5_8_pct  \\\n",
       "20                     100.00                       0.00   \n",
       "21                      89.51                       0.00   \n",
       "22                      81.49                      10.49   \n",
       "23                      68.09                      27.44   \n",
       "34                      78.58                      16.59   \n",
       "\n",
       "    temp_delta_bucket_gt8_pct  volt_delta_bucket_0_10_pct  \\\n",
       "20                        0.0                       20.24   \n",
       "21                        0.0                       31.82   \n",
       "22                        0.0                       44.57   \n",
       "23                        0.0                       49.87   \n",
       "34                        0.0                       37.68   \n",
       "\n",
       "    volt_delta_bucket_10_20_pct  volt_delta_bucket_20_30_pct  \\\n",
       "20                        79.02                         0.74   \n",
       "21                        31.04                         7.50   \n",
       "22                        42.38                         2.93   \n",
       "23                        30.55                         2.63   \n",
       "34                        31.34                         2.33   \n",
       "\n",
       "    volt_delta_bucket_gt30_pct  \n",
       "20                        0.00  \n",
       "21                       29.64  \n",
       "22                       10.11  \n",
       "23                       16.94  \n",
       "34                       28.64  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_agg[(report_agg.maxtemp_bucket_35_40_pct>40)|(report_agg.maxtemp_bucket_gt40_pct>40)].to_csv('bcs_session_maxT.csv')\n",
    "report_agg[(report_agg.maxtemp_bucket_35_40_pct>40)|(report_agg.maxtemp_bucket_gt40_pct>40)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6f019fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>date_val</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>total_duration</th>\n",
       "      <th>maxtemp_bucket_lt28_pct</th>\n",
       "      <th>maxtemp_bucket_28_32_pct</th>\n",
       "      <th>maxtemp_bucket_32_35_pct</th>\n",
       "      <th>maxtemp_bucket_35_40_pct</th>\n",
       "      <th>maxtemp_bucket_gt40_pct</th>\n",
       "      <th>temp_delta_bucket_lt2_pct</th>\n",
       "      <th>temp_delta_bucket_2_5_pct</th>\n",
       "      <th>temp_delta_bucket_5_8_pct</th>\n",
       "      <th>temp_delta_bucket_gt8_pct</th>\n",
       "      <th>volt_delta_bucket_0_10_pct</th>\n",
       "      <th>volt_delta_bucket_10_20_pct</th>\n",
       "      <th>volt_delta_bucket_20_30_pct</th>\n",
       "      <th>volt_delta_bucket_gt30_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-04</td>\n",
       "      <td>4</td>\n",
       "      <td>201.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.41</td>\n",
       "      <td>43.60</td>\n",
       "      <td>49.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.29</td>\n",
       "      <td>50.29</td>\n",
       "      <td>47.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.91</td>\n",
       "      <td>54.73</td>\n",
       "      <td>5.20</td>\n",
       "      <td>7.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-06</td>\n",
       "      <td>7</td>\n",
       "      <td>278.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>22.80</td>\n",
       "      <td>21.52</td>\n",
       "      <td>44.23</td>\n",
       "      <td>11.43</td>\n",
       "      <td>6.38</td>\n",
       "      <td>41.26</td>\n",
       "      <td>52.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.35</td>\n",
       "      <td>38.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>5.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-13</td>\n",
       "      <td>7</td>\n",
       "      <td>283.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.61</td>\n",
       "      <td>25.78</td>\n",
       "      <td>50.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.89</td>\n",
       "      <td>55.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.30</td>\n",
       "      <td>36.46</td>\n",
       "      <td>3.21</td>\n",
       "      <td>26.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-15</td>\n",
       "      <td>3</td>\n",
       "      <td>147.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14.18</td>\n",
       "      <td>85.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>27.85</td>\n",
       "      <td>72.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.41</td>\n",
       "      <td>41.69</td>\n",
       "      <td>3.86</td>\n",
       "      <td>11.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-10-17</td>\n",
       "      <td>4</td>\n",
       "      <td>199.44</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.01</td>\n",
       "      <td>37.42</td>\n",
       "      <td>51.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>54.95</td>\n",
       "      <td>44.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.26</td>\n",
       "      <td>53.20</td>\n",
       "      <td>2.80</td>\n",
       "      <td>8.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     reg_num   customer model      mode    date_val  total_sessions  \\\n",
       "49  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-10-04               4   \n",
       "51  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-10-06               7   \n",
       "58  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-10-13               7   \n",
       "60  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-10-15               3   \n",
       "62  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-10-17               4   \n",
       "\n",
       "    total_duration  maxtemp_bucket_lt28_pct  maxtemp_bucket_28_32_pct  \\\n",
       "49          201.55                     0.00                      6.41   \n",
       "51          278.92                     0.01                     22.80   \n",
       "58          283.56                     0.00                     23.61   \n",
       "60          147.83                     0.00                      0.01   \n",
       "62          199.44                     0.63                     10.01   \n",
       "\n",
       "    maxtemp_bucket_32_35_pct  maxtemp_bucket_35_40_pct  \\\n",
       "49                     43.60                     49.99   \n",
       "51                     21.52                     44.23   \n",
       "58                     25.78                     50.61   \n",
       "60                     14.18                     85.81   \n",
       "62                     37.42                     51.94   \n",
       "\n",
       "    maxtemp_bucket_gt40_pct  temp_delta_bucket_lt2_pct  \\\n",
       "49                     0.00                       2.29   \n",
       "51                    11.43                       6.38   \n",
       "58                     0.00                       0.00   \n",
       "60                     0.00                       0.02   \n",
       "62                     0.00                       0.08   \n",
       "\n",
       "    temp_delta_bucket_2_5_pct  temp_delta_bucket_5_8_pct  \\\n",
       "49                      50.29                      47.42   \n",
       "51                      41.26                      52.36   \n",
       "58                      44.89                      55.11   \n",
       "60                      27.85                      72.13   \n",
       "62                      54.95                      44.97   \n",
       "\n",
       "    temp_delta_bucket_gt8_pct  volt_delta_bucket_0_10_pct  \\\n",
       "49                        0.0                       32.91   \n",
       "51                        0.0                       53.35   \n",
       "58                        0.0                       34.30   \n",
       "60                        0.0                       43.41   \n",
       "62                        0.0                       35.26   \n",
       "\n",
       "    volt_delta_bucket_10_20_pct  volt_delta_bucket_20_30_pct  \\\n",
       "49                        54.73                         5.20   \n",
       "51                        38.71                         2.08   \n",
       "58                        36.46                         3.21   \n",
       "60                        41.69                         3.86   \n",
       "62                        53.20                         2.80   \n",
       "\n",
       "    volt_delta_bucket_gt30_pct  \n",
       "49                        7.16  \n",
       "51                        5.86  \n",
       "58                       26.03  \n",
       "60                       11.04  \n",
       "62                        8.74  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_agg[(report_agg.temp_delta_bucket_5_8_pct>40)|(report_agg.temp_delta_bucket_gt8_pct>40)].to_csv('bcs_session_deltaT.csv')\n",
    "report_agg[(report_agg.temp_delta_bucket_5_8_pct>40)|(report_agg.temp_delta_bucket_gt8_pct>40)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91ab3446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>date_val</th>\n",
       "      <th>total_sessions</th>\n",
       "      <th>total_duration</th>\n",
       "      <th>maxtemp_bucket_lt28_pct</th>\n",
       "      <th>maxtemp_bucket_28_32_pct</th>\n",
       "      <th>maxtemp_bucket_32_35_pct</th>\n",
       "      <th>maxtemp_bucket_35_40_pct</th>\n",
       "      <th>maxtemp_bucket_gt40_pct</th>\n",
       "      <th>temp_delta_bucket_lt2_pct</th>\n",
       "      <th>temp_delta_bucket_2_5_pct</th>\n",
       "      <th>temp_delta_bucket_5_8_pct</th>\n",
       "      <th>temp_delta_bucket_gt8_pct</th>\n",
       "      <th>volt_delta_bucket_0_10_pct</th>\n",
       "      <th>volt_delta_bucket_10_20_pct</th>\n",
       "      <th>volt_delta_bucket_20_30_pct</th>\n",
       "      <th>volt_delta_bucket_gt30_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>2</td>\n",
       "      <td>757.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>78.02</td>\n",
       "      <td>21.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.86</td>\n",
       "      <td>25.97</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.20</td>\n",
       "      <td>5.51</td>\n",
       "      <td>0.60</td>\n",
       "      <td>73.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>4</td>\n",
       "      <td>305.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>56.51</td>\n",
       "      <td>43.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.12</td>\n",
       "      <td>72.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.61</td>\n",
       "      <td>16.01</td>\n",
       "      <td>1.35</td>\n",
       "      <td>61.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>2</td>\n",
       "      <td>787.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63.60</td>\n",
       "      <td>36.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.36</td>\n",
       "      <td>49.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.61</td>\n",
       "      <td>9.37</td>\n",
       "      <td>0.98</td>\n",
       "      <td>54.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>1</td>\n",
       "      <td>120.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>68.69</td>\n",
       "      <td>31.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>83.23</td>\n",
       "      <td>16.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.94</td>\n",
       "      <td>16.95</td>\n",
       "      <td>1.31</td>\n",
       "      <td>71.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "      <td>CHARGING</td>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>5</td>\n",
       "      <td>264.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>4.14</td>\n",
       "      <td>48.33</td>\n",
       "      <td>47.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.83</td>\n",
       "      <td>71.65</td>\n",
       "      <td>20.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.82</td>\n",
       "      <td>18.25</td>\n",
       "      <td>1.07</td>\n",
       "      <td>52.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     reg_num   customer model      mode    date_val  total_sessions  \\\n",
       "25  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-10               2   \n",
       "26  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-11               4   \n",
       "32  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-17               2   \n",
       "39  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-24               1   \n",
       "44  11  AP39WF8593  FB Guntur  12.5  CHARGING  2025-09-29               5   \n",
       "\n",
       "    total_duration  maxtemp_bucket_lt28_pct  maxtemp_bucket_28_32_pct  \\\n",
       "25          757.11                     0.00                     78.02   \n",
       "26          305.07                     0.01                     56.51   \n",
       "32          787.06                     0.00                     63.60   \n",
       "39          120.25                     0.00                      0.00   \n",
       "44          264.25                     0.30                      4.14   \n",
       "\n",
       "    maxtemp_bucket_32_35_pct  maxtemp_bucket_35_40_pct  \\\n",
       "25                     21.98                      0.00   \n",
       "26                     43.49                      0.00   \n",
       "32                     36.40                      0.00   \n",
       "39                     68.69                     31.31   \n",
       "44                     48.33                     47.22   \n",
       "\n",
       "    maxtemp_bucket_gt40_pct  temp_delta_bucket_lt2_pct  \\\n",
       "25                      0.0                      73.86   \n",
       "26                      0.0                      27.12   \n",
       "32                      0.0                      50.36   \n",
       "39                      0.0                       0.00   \n",
       "44                      0.0                       7.83   \n",
       "\n",
       "    temp_delta_bucket_2_5_pct  temp_delta_bucket_5_8_pct  \\\n",
       "25                      25.97                       0.16   \n",
       "26                      72.88                       0.00   \n",
       "32                      49.64                       0.00   \n",
       "39                      83.23                      16.77   \n",
       "44                      71.65                      20.52   \n",
       "\n",
       "    temp_delta_bucket_gt8_pct  volt_delta_bucket_0_10_pct  \\\n",
       "25                        0.0                       20.20   \n",
       "26                        0.0                       21.61   \n",
       "32                        0.0                       35.61   \n",
       "39                        0.0                        9.94   \n",
       "44                        0.0                       27.82   \n",
       "\n",
       "    volt_delta_bucket_10_20_pct  volt_delta_bucket_20_30_pct  \\\n",
       "25                         5.51                         0.60   \n",
       "26                        16.01                         1.35   \n",
       "32                         9.37                         0.98   \n",
       "39                        16.95                         1.31   \n",
       "44                        18.25                         1.07   \n",
       "\n",
       "    volt_delta_bucket_gt30_pct  \n",
       "25                       73.69  \n",
       "26                       61.03  \n",
       "32                       54.04  \n",
       "39                       71.80  \n",
       "44                       52.87  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_agg[(report_agg.volt_delta_bucket_20_30_pct>40)|(report_agg.volt_delta_bucket_gt30_pct>40)].to_csv('bcs_session_deltaV.csv')\n",
    "report_agg[(report_agg.volt_delta_bucket_20_30_pct>40)|(report_agg.volt_delta_bucket_gt30_pct>40)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f77176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_detailed_hotspot(df, item_id_col='tc_id', pack_id_col='pack_id'):\n",
    "    \"\"\"\n",
    "    Computes detailed hotspot analysis for a single vehicle on a single day.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame for a single vehicle and day.\n",
    "        item_id_col (str): The column representing the individual item ID \n",
    "                           (e.g., 'tc_id' or 'pack_id').\n",
    "        pack_id_col (str): The column representing the pack ID (e.g., 'pack_id').\n",
    "                           This is separated for TC-level grouping.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Detailed hotspot metrics grouped by item_id and pack_id.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Basic Info\n",
    "    date_val = df['date_val'].iloc[0]\n",
    "    vid = df['id'].iloc[0]\n",
    "    \n",
    "    # Pre-calculate total time for the day (in seconds)\n",
    "    total_day_sec = df['dt_sec'].sum()\n",
    "    # total_day_mins = min(total_day_sec / 60.0, 1440.0) # Clipping the daily duration\n",
    "    total_day_mins = total_day_sec / 60.0\n",
    "    \n",
    "    # --- Helper to aggregate ---\n",
    "    def get_agg(subset_df, id_col, val_col, pack_col):\n",
    "        if subset_df.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        # Group by the item ID and its associated Pack ID\n",
    "        return subset_df.groupby([id_col, pack_col])[val_col].sum()\n",
    "\n",
    "    # Determine the columns to group by (Max TC/Pack and Min TC/Pack)\n",
    "    # The columns in the raw data are fixed: batt_maxtemp_tc, pack_id_max, etc.\n",
    "    group_cols = [item_id_col, pack_id_col]\n",
    "\n",
    "    # Use the relevant columns from the raw data for aggregation:\n",
    "    c_df = df[df['mode'] == 'CHARGING']\n",
    "    dc_df = df[df['mode'] == 'DISCHARGING']\n",
    "    \n",
    "    # 1. C-Max (Charging, Max Temp)\n",
    "    s_cmax = get_agg(c_df, 'batt_maxtemp_tc', 'dt_sec', 'pack_id_max')\n",
    "    \n",
    "    # 2. DC-Max (Discharging, Max Temp)\n",
    "    s_dcmax = get_agg(dc_df, 'batt_maxtemp_tc', 'dt_sec', 'pack_id_max')\n",
    "    \n",
    "    # 3. C-Min (Charging, Min Temp)\n",
    "    s_cmin = get_agg(c_df, 'batt_mintemp_tc', 'dt_sec', 'pack_id_min')\n",
    "    \n",
    "    # 4. DC-Min (Discharging, Min Temp)\n",
    "    s_dcmin = get_agg(dc_df, 'batt_mintemp_tc', 'dt_sec', 'pack_id_min')\n",
    "    \n",
    "    # --- Merge all series ---\n",
    "    dfs = []\n",
    "    \n",
    "    # Combine the index names for consistent merging (TC ID and Pack ID)\n",
    "    index_names = ['tc_id', 'pack_id'] \n",
    "\n",
    "    if not s_cmax.empty:\n",
    "        s_cmax.index.names = index_names\n",
    "        dfs.append(s_cmax.rename('cmax_dur'))\n",
    "        \n",
    "    if not s_dcmax.empty:\n",
    "        s_dcmax.index.names = index_names\n",
    "        dfs.append(s_dcmax.rename('dcmax_dur'))\n",
    "        \n",
    "    if not s_cmin.empty:\n",
    "        s_cmin.index.names = index_names\n",
    "        dfs.append(s_cmin.rename('cmin_dur'))\n",
    "        \n",
    "    if not s_dcmin.empty:\n",
    "        s_dcmin.index.names = index_names\n",
    "        dfs.append(s_dcmin.rename('dcmin_dur'))\n",
    "        \n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    merged = pd.concat(dfs, axis=1).fillna(0)\n",
    "    \n",
    "    # Ensure all duration columns exist\n",
    "    target_dur_cols = ['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']\n",
    "    merged = merged.reindex(columns=list(set(merged.columns) | set(target_dur_cols)), fill_value=0)\n",
    "    \n",
    "    # --- Calculations ---\n",
    "    \n",
    "    # Assign the clipped total time to the daily_dur column\n",
    "    merged['daily_dur'] = total_day_mins \n",
    "\n",
    "    # Convert seconds to minutes for Duration columns\n",
    "    merged[['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']] = merged[['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']] / 60.0\n",
    "    \n",
    "    # Calculate Percentages against the Total Day Time (mode-agnostic)\n",
    "    if total_day_mins > 0:\n",
    "        for col in target_dur_cols:\n",
    "            merged[f'{col}_pct'] = (merged[col] / total_day_mins) * 100.0\n",
    "    else:\n",
    "        for col in target_dur_cols:\n",
    "            merged[f'{col}_pct'] = 0.0\n",
    "        \n",
    "    # --- Formatting ---\n",
    "    cols_to_round = ['daily_dur','cmax_dur', 'cmax_dur_pct', 'dcmax_dur', 'dcmax_dur_pct', \n",
    "                     'cmin_dur', 'cmin_dur_pct', 'dcmin_dur', 'dcmin_dur_pct']\n",
    "    \n",
    "    # Ensure all percentage columns exist\n",
    "    for col in cols_to_round:\n",
    "        if col not in merged.columns:\n",
    "             merged[col] = 0.0\n",
    "             \n",
    "    merged[cols_to_round] = merged[cols_to_round].round(2)\n",
    "    \n",
    "    # Reset index and insert identifier columns\n",
    "    merged = merged.reset_index()\n",
    "    merged.insert(0, 'id', vid)\n",
    "    merged.insert(0, 'date_val', date_val)\n",
    "    \n",
    "    # Reorder columns\n",
    "    desired_order = ['date_val', 'id', 'tc_id', 'pack_id', 'daily_dur',\n",
    "                     'cmax_dur', 'cmax_dur_pct', 'dcmax_dur', 'dcmax_dur_pct',\n",
    "                     'cmin_dur', 'cmin_dur_pct', 'dcmin_dur', 'dcmin_dur_pct']\n",
    "    \n",
    "    return merged[desired_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8a40dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub = read_parquet_subset(\"df_with_state.parquet\", datetime(2025, 9, 1), datetime(2025, 9, 2))\n",
    "# df_sub.head(500).to_csv('hotspot_ip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ba77310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_day_tc_hotspot_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    all_tables = []\n",
    "    total_rows = 0\n",
    "\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        for d in tqdm(range(num_days), desc=\"Processing Days\", ncols=70):\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            df_sub[\"date_val\"] = day_start.date()\n",
    "\n",
    "            # per-vehicle\n",
    "            for vid, g in df_sub.groupby(\"id\"):\n",
    "                res = compute_detailed_hotspot(g)\n",
    "                if not res.empty:\n",
    "                    res[\"id\"] = vid\n",
    "                    all_tables.append(res)\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    if not all_tables:\n",
    "        return pd.DataFrame(), total_rows\n",
    "\n",
    "    return pd.concat(all_tables, ignore_index=True), total_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d61cd0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Days: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [01:35<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     date_val  id  tc_id  pack_id  daily_dur  cmax_dur  cmax_dur_pct  \\\n",
      "0  2025-09-01  16      5        1     959.59      2.81          0.29   \n",
      "1  2025-09-01  16      8        1     959.59      0.04          0.00   \n",
      "2  2025-09-01  16     17        2     959.59      2.74          0.29   \n",
      "3  2025-09-01  16     73        9     959.59      0.05          0.01   \n",
      "4  2025-09-01  16     80        9     959.59    228.08         23.77   \n",
      "\n",
      "   dcmax_dur  dcmax_dur_pct  cmin_dur  cmin_dur_pct  dcmin_dur  dcmin_dur_pct  \n",
      "0       0.04           0.00       0.0           0.0      27.99           2.92  \n",
      "1       0.04           0.00       0.0           0.0      58.46           6.09  \n",
      "2       0.56           0.06       0.0           0.0       0.00           0.00  \n",
      "3      52.38           5.46       0.0           0.0       0.00           0.00  \n",
      "4     270.21          28.16       0.0           0.0       0.00           0.00  \n",
      "Total rows processed: 51788694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2025, 9, 1)\n",
    "\n",
    "tc_hotspot_df, total_rows = run_multi_day_tc_hotspot_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=start_date,\n",
    "    num_days=75\n",
    ")\n",
    "\n",
    "print(tc_hotspot_df.head())\n",
    "print(\"Total rows processed:\", total_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "846a2e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_val</th>\n",
       "      <th>id</th>\n",
       "      <th>tc_id</th>\n",
       "      <th>pack_id</th>\n",
       "      <th>daily_dur</th>\n",
       "      <th>cmax_dur</th>\n",
       "      <th>cmax_dur_pct</th>\n",
       "      <th>dcmax_dur</th>\n",
       "      <th>dcmax_dur_pct</th>\n",
       "      <th>cmin_dur</th>\n",
       "      <th>cmin_dur_pct</th>\n",
       "      <th>dcmin_dur</th>\n",
       "      <th>dcmin_dur_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>959.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.57</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>959.59</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.99</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>959.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.69</td>\n",
       "      <td>3.09</td>\n",
       "      <td>71.96</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>959.59</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58.46</td>\n",
       "      <td>6.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>959.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>189.47</td>\n",
       "      <td>19.74</td>\n",
       "      <td>510.59</td>\n",
       "      <td>53.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_val  id  tc_id  pack_id  daily_dur  cmax_dur  cmax_dur_pct  \\\n",
       "0  2025-09-01  16      3        1     959.59      0.00          0.00   \n",
       "1  2025-09-01  16      5        1     959.59      2.81          0.29   \n",
       "2  2025-09-01  16      7        1     959.59      0.00          0.00   \n",
       "3  2025-09-01  16      8        1     959.59      0.04          0.00   \n",
       "4  2025-09-01  16      9        1     959.59      0.00          0.00   \n",
       "\n",
       "   dcmax_dur  dcmax_dur_pct  cmin_dur  cmin_dur_pct  dcmin_dur  dcmin_dur_pct  \n",
       "0       6.57           0.68      0.00          0.00       0.00           0.00  \n",
       "1       0.04           0.00      0.00          0.00      27.99           2.92  \n",
       "2       0.00           0.00     29.69          3.09      71.96           7.50  \n",
       "3       0.04           0.00      0.00          0.00      58.46           6.09  \n",
       "4       0.00           0.00    189.47         19.74     510.59          53.21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort_cols = ['date_val', 'id', 'pack_id', 'tc_id']\n",
    "tc_hotspot_df = tc_hotspot_df.sort_values(by=sort_cols, ascending=True).reset_index(drop=True)\n",
    "tc_hotspot_df.to_csv('hotspot.csv')\n",
    "display(tc_hotspot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c7fec97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1756.42, 1460.31, 2382.81, 1918.67])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "tc_hotspot_df[tc_hotspot_df.daily_dur>1440].daily_dur.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7924e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100)\n",
    "# tc_hotspot_df.groupby(['date_val','id','daily_dur','pack_id'])[['cmax_dur_pct','dcmax_dur_pct','cmin_dur_pct','dcmin_dur_pct']].sum().round(2).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d2ff4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the TC-level aggregated data (hotspot.csv)\n",
    "df_hotspot = pd.read_csv('hotspot.csv')\n",
    "\n",
    "# 1. Group by pack_id and sum all duration columns (NUMERATOR)\n",
    "pack_agg = df_hotspot.groupby(['pack_id']).agg(\n",
    "    total_cmax_dur=('cmax_dur', 'sum'),\n",
    "    total_dcmax_dur=('dcmax_dur', 'sum'),\n",
    "    total_cmin_dur=('cmin_dur', 'sum'),\n",
    "    total_dcmin_dur=('dcmin_dur', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "# 2. Calculate the Grand Total for each duration column (DENOMINATOR)\n",
    "grand_totals = pack_agg[['total_cmax_dur', 'total_dcmax_dur', \n",
    "                         'total_cmin_dur', 'total_dcmin_dur']].sum()\n",
    "\n",
    "# 3. Calculate the percentage share for each pack\n",
    "for col in ['cmax_dur', 'dcmax_dur', 'cmin_dur', 'dcmin_dur']:\n",
    "    # Get the total duration for this specific metric\n",
    "    total_duration = grand_totals[f'total_{col}']\n",
    "    \n",
    "    if total_duration > 0:\n",
    "        # Calculate the pack's share of the total problem duration\n",
    "        pack_agg[f'{col}_Share_Pct'] = (\n",
    "            pack_agg[f'total_{col}'] / total_duration * 100\n",
    "        ).round(2)\n",
    "    else:\n",
    "        pack_agg[f'{col}_Share_Pct'] = 0.0\n",
    "\n",
    "# 4. Select the final output columns\n",
    "hotspot_pack_level = pack_agg[['pack_id', \n",
    "                             'cmax_dur_Share_Pct', 'dcmax_dur_Share_Pct',\n",
    "                             'cmin_dur_Share_Pct', 'dcmin_dur_Share_Pct']]\n",
    "\n",
    "hotspot_pack_level.to_csv('hotspot_pack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5426f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_high_delta_v_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes charging sessions for high DeltaV (> 20mV) and aggregates metrics \n",
    "    by SOC band across the entire fleet.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure necessary columns are correctly typed\n",
    "    df['volt_delta_mv'] = pd.to_numeric(df['volt_delta_mv'], errors='coerce')\n",
    "    df['total_battery_current'] = pd.to_numeric(df['total_battery_current'], errors='coerce')\n",
    "    if 'dt_sec' in df.columns:\n",
    "        df['dt_sec'] = pd.to_numeric(df['dt_sec'], errors='coerce')\n",
    "\n",
    "    # --- 1. Filter Data ---\n",
    "    df_charging = df[df['mode'] == 'CHARGING'].copy()\n",
    "    df_filtered = df_charging[df_charging['volt_delta_mv'] > 20].copy()\n",
    "    \n",
    "    # Prepare absolute current column\n",
    "    df_filtered['abs_current'] = df_filtered['total_battery_current'].abs()\n",
    "\n",
    "    # --- FIX 1: Update the column name in the final structure list ---\n",
    "    final_report_cols_structure = [\n",
    "        'total_ids', 'SOC band', '% Time', \n",
    "        'mv Imbalance (median)', 'mv Imbalance (P95)', # <--- FIXED: Now uses P95\n",
    "        'Charging Current (Median)', 'Charging Current (Max)'\n",
    "    ]\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        # Return an empty DataFrame with the correct structure\n",
    "        return pd.DataFrame(columns=final_report_cols_structure)\n",
    "\n",
    "    # --- 2. Calculate Total Vehicle Count ---\n",
    "    total_unique_vehicles = df_filtered['id'].nunique()\n",
    "\n",
    "    p95_func = lambda x: x.quantile(0.95)    \n",
    "\n",
    "    # --- 3. Grouping and Aggregation ---\n",
    "    grouped_by_soc = df_filtered.groupby('soc_band_bucket').agg(\n",
    "        **{\n",
    "            'mv Imbalance (median)': ('volt_delta_mv', 'median'),\n",
    "            'mv Imbalance (P95)': ('volt_delta_mv', p95_func), # P95 is calculated here\n",
    "            'Charging Current (Median)': ('abs_current', 'median'),\n",
    "            'Charging Current (Max)': ('abs_current', 'max'),\n",
    "            'Band Duration (sec)': ('dt_sec', 'sum')            \n",
    "        }\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- CALCULATE % TIME ---\n",
    "    total_time_filtered = grouped_by_soc['Band Duration (sec)'].sum()\n",
    "\n",
    "    if total_time_filtered > 0:\n",
    "        grouped_by_soc['% Time'] = (\n",
    "            grouped_by_soc['Band Duration (sec)'] / total_time_filtered * 100\n",
    "        ).round(2)\n",
    "    else:\n",
    "        grouped_by_soc['% Time'] = 0.0\n",
    "        \n",
    "    # Drop the temporary duration column\n",
    "    grouped_by_soc = grouped_by_soc.drop(columns=['Band Duration (sec)'])\n",
    "\n",
    "    # --- 4. Final Formatting ---\n",
    "    final_report = grouped_by_soc.rename(columns={'soc_band_bucket': 'SOC band'})\n",
    "    \n",
    "    # Insert the 'Vehicle ID' column and assign the total count\n",
    "    final_report.insert(0, 'total_ids', total_unique_vehicles)\n",
    "\n",
    "    # --- FIX 2: Update rounding logic to include P95 ---\n",
    "    for col in final_report.columns:\n",
    "        # Check for 'median', 'P95', 'Max', or 'Time' to round all metrics\n",
    "        if 'median' in col or 'P95' in col or 'Max' in col or 'Time' in col:\n",
    "            final_report[col] = final_report[col].round(2)\n",
    "    # ----------------------------------------------------\n",
    "            \n",
    "    # Ensure final column order is correct (now that P95 is in the structure list)\n",
    "    final_report = final_report[[col for col in final_report_cols_structure if col in final_report.columns]]\n",
    "\n",
    "    return final_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbdfb145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeltaV Analysis Days: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [01:10<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_multi_day_deltav_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads multi-day Parquet data and computes the fleet-wide high DeltaV report, \n",
    "    using a progress bar for the outer loop.\n",
    "    \"\"\"\n",
    "    raw_data_for_deltav = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Logging setup (remains the same to avoid NameError)\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        # --- CHANGE HERE: Wrap range(num_days) with tqdm ---\n",
    "        # NOTE: You need to have 'from tqdm import tqdm' imported in your environment\n",
    "        # Use a descriptive label like \"DeltaV Analysis Days\"\n",
    "        for d in tqdm(range(num_days), desc=\"DeltaV Analysis Days\", ncols=70): \n",
    "        # --------------------------------------------------\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            # --- Replace with your actual Parquet reading function ---\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            \n",
    "            # --- COLLECT RAW DATA ---\n",
    "            cols_needed = ['id', 'dt_sec','mode', 'volt_delta_mv', 'total_battery_current', 'soc_band_bucket']\n",
    "            cols_present = [col for col in cols_needed if col in df_sub.columns]\n",
    "            \n",
    "            raw_data_for_deltav.append(df_sub[cols_present].copy())\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # RUN FLEET-WIDE DELTAV ANALYSIS (ONCE)\n",
    "    # -------------------------------------------------------------\n",
    "    if raw_data_for_deltav:\n",
    "        full_raw_data_df = pd.concat(raw_data_for_deltav, ignore_index=True)\n",
    "        delta_v_report_df = generate_high_delta_v_report(full_raw_data_df)\n",
    "    else:\n",
    "        delta_v_report_df = generate_high_delta_v_report(pd.DataFrame())\n",
    "\n",
    "    return delta_v_report_df, total_rows\n",
    "\n",
    "\n",
    "final_report, total_rows = run_multi_day_deltav_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=datetime(2025, 9, 1),\n",
    "    num_days=75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e3494c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_ids</th>\n",
       "      <th>SOC band</th>\n",
       "      <th>% Time</th>\n",
       "      <th>mv Imbalance (median)</th>\n",
       "      <th>mv Imbalance (P95)</th>\n",
       "      <th>Charging Current (Median)</th>\n",
       "      <th>Charging Current (Max)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10â€“20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>370.399994</td>\n",
       "      <td>382.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>20â€“30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>22.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>367.299988</td>\n",
       "      <td>405.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>30-40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>22.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>366.799988</td>\n",
       "      <td>405.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>40-50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>365.600006</td>\n",
       "      <td>404.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>50-60</td>\n",
       "      <td>1.08</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>362.299988</td>\n",
       "      <td>502.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>60-70</td>\n",
       "      <td>1.06</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>316.600006</td>\n",
       "      <td>408.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>70-80</td>\n",
       "      <td>1.03</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>358.100006</td>\n",
       "      <td>492.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28</td>\n",
       "      <td>80-90</td>\n",
       "      <td>14.65</td>\n",
       "      <td>26.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>331.799988</td>\n",
       "      <td>503.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>90-100</td>\n",
       "      <td>81.40</td>\n",
       "      <td>60.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>493.899994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_ids SOC band  % Time  mv Imbalance (median)  mv Imbalance (P95)  \\\n",
       "0         28    10â€“20    0.09                   23.0                26.0   \n",
       "1         28    20â€“30    0.15                   22.0                28.0   \n",
       "2         28    30-40    0.18                   22.0                28.0   \n",
       "3         28    40-50    0.37                   22.0                24.0   \n",
       "4         28    50-60    1.08                   22.0                29.0   \n",
       "5         28    60-70    1.06                   22.0                29.0   \n",
       "6         28    70-80    1.03                   22.0                37.0   \n",
       "7         28    80-90   14.65                   26.0                80.0   \n",
       "8         28   90-100   81.40                   60.0               122.0   \n",
       "\n",
       "   Charging Current (Median)  Charging Current (Max)  \n",
       "0                 370.399994              382.899994  \n",
       "1                 367.299988              405.799988  \n",
       "2                 366.799988              405.500000  \n",
       "3                 365.600006              404.200012  \n",
       "4                 362.299988              502.899994  \n",
       "5                 316.600006              408.200012  \n",
       "6                 358.100006              492.600006  \n",
       "7                 331.799988              503.000000  \n",
       "8                  91.500000              493.899994  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8aa8f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report.to_csv('soc_band_mv_current_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28f6a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ThermalV Analysis Days: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [01:12<00:00,  1.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thermocouple ID</th>\n",
       "      <th>Max Temp Duration (min)</th>\n",
       "      <th>Min Temp Duration (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.71</td>\n",
       "      <td>4346.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>601.45</td>\n",
       "      <td>4320.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>554.84</td>\n",
       "      <td>131.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1456.65</td>\n",
       "      <td>26.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5227.75</td>\n",
       "      <td>7605.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Thermocouple ID  Max Temp Duration (min)  Min Temp Duration (min)\n",
       "0                1                    10.71                  4346.16\n",
       "1                2                   601.45                  4320.89\n",
       "2                3                   554.84                   131.61\n",
       "3                4                  1456.65                    26.34\n",
       "4                5                  5227.75                  7605.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_thermal_voltage_hotspot_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a report showing Thermocouple (TC) Hotspot exposure duration \n",
    "    (Max Temp/Min Temp) during high voltage imbalance (volt_delta_mv > 5).\n",
    "    \n",
    "    Required columns: dt_sec, volt_delta_mv, batt_maxtemp_tc, batt_mintemp_tc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Data Type Conversion and Preparation ---\n",
    "    df['volt_delta_mv'] = pd.to_numeric(df['volt_delta_mv'], errors='coerce')\n",
    "    \n",
    "    # Check for duration column and convert to minutes\n",
    "    if 'dt_sec' not in df.columns:\n",
    "        print(\"Error: 'dt_sec' column missing for duration calculation.\")\n",
    "        return pd.DataFrame() \n",
    "\n",
    "    df['dt_min'] = pd.to_numeric(df['dt_sec'], errors='coerce') / 60.0\n",
    "\n",
    "    # --- 2. Filtering ---\n",
    "    # Apply condition: volt_delta_mv > 5 (High Voltage Imbalance)\n",
    "    df_filtered = df[df['volt_delta_mv'] > 5].copy()\n",
    "    \n",
    "    final_report_cols_structure = [\n",
    "        'Thermocouple ID', 'Max Temp Duration (min)', 'Min Temp Duration (min)'\n",
    "    ]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        return pd.DataFrame(columns=final_report_cols_structure)\n",
    "\n",
    "    # --- 3. Aggregation for MAX Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MAX temp and sum the duration.\n",
    "    max_temp_df = df_filtered.groupby('batt_maxtemp_tc')['dt_min'].sum().reset_index()\n",
    "    max_temp_df = max_temp_df.rename(columns={\n",
    "        'batt_maxtemp_tc': 'Thermocouple ID',\n",
    "        'dt_min': 'Max Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 4. Aggregation for MIN Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MIN temp and sum the duration.\n",
    "    min_temp_df = df_filtered.groupby('batt_mintemp_tc')['dt_min'].sum().reset_index()\n",
    "    min_temp_df = min_temp_df.rename(columns={\n",
    "        'batt_mintemp_tc': 'Thermocouple ID',\n",
    "        'dt_min': 'Min Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 5. Merge and Cleanup ---\n",
    "    # Perform an Outer Join to combine both results, keeping all TCs\n",
    "    final_report = pd.merge(\n",
    "        max_temp_df, \n",
    "        min_temp_df, \n",
    "        on='Thermocouple ID', \n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs created by the outer merge with 0\n",
    "    final_report = final_report.fillna(0)\n",
    "\n",
    "    # Round all duration metrics\n",
    "    for col in final_report.columns:\n",
    "        if 'Duration' in col:\n",
    "            final_report[col] = final_report[col].round(2)\n",
    "            \n",
    "    # Filter out TCs with zero total exposure\n",
    "    final_report = final_report[\n",
    "        (final_report['Max Temp Duration (min)'] > 0) | \n",
    "        (final_report['Min Temp Duration (min)'] > 0)\n",
    "    ]\n",
    "    \n",
    "    # Ensure ID column is integer (since TC IDs are usually integers)\n",
    "    final_report['Thermocouple ID'] = final_report['Thermocouple ID'].astype(pd.Int64Dtype())\n",
    "\n",
    "    return final_report\n",
    "\n",
    "\n",
    "def run_multi_day_deltav_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads multi-day Parquet data and computes the fleet-wide high DeltaV report, \n",
    "    using a progress bar for the outer loop.\n",
    "    \"\"\"\n",
    "    raw_data = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Logging setup (remains the same to avoid NameError)\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        # --- CHANGE HERE: Wrap range(num_days) with tqdm ---\n",
    "        # NOTE: You need to have 'from tqdm import tqdm' imported in your environment\n",
    "        # Use a descriptive label like \"DeltaV Analysis Days\"\n",
    "        for d in tqdm(range(num_days), desc=\"ThermalV Analysis Days\", ncols=70): \n",
    "        # --------------------------------------------------\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            # --- Replace with your actual Parquet reading function ---\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            \n",
    "            # --- COLLECT RAW DATA ---\n",
    "            cols_needed = ['batt_maxtemp_tc','batt_mintemp_tc', 'dt_sec', 'volt_delta_mv']\n",
    "            cols_present = [col for col in cols_needed if col in df_sub.columns]\n",
    "            \n",
    "            raw_data.append(df_sub[cols_present].copy())\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # RUN FLEET-WIDE DELTAV ANALYSIS (ONCE)\n",
    "    # -------------------------------------------------------------\n",
    "    if raw_data:\n",
    "        full_raw_data_df = pd.concat(raw_data, ignore_index=True)\n",
    "        delta_v_report_df = generate_thermal_voltage_hotspot_report(full_raw_data_df)\n",
    "    else:\n",
    "        delta_v_report_df = generate_thermal_voltage_hotspot_report(pd.DataFrame())\n",
    "\n",
    "    return delta_v_report_df, total_rows\n",
    "\n",
    "\n",
    "thermal_v_tc_hotspot_report, total_rows = run_multi_day_deltav_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=datetime(2025, 9, 1),\n",
    "    num_days=75\n",
    ")\n",
    "\n",
    "display(thermal_v_tc_hotspot_report.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93b4e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_v_tc_hotspot_report.to_csv('hotspot_tc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "592dc10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ThermalV Analysis Days: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [01:12<00:00,  1.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pack ID</th>\n",
       "      <th>Max Temp Duration (min)</th>\n",
       "      <th>Min Temp Duration (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18123.82</td>\n",
       "      <td>455020.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9373.62</td>\n",
       "      <td>106823.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2706.24</td>\n",
       "      <td>16495.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1907.29</td>\n",
       "      <td>5676.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2467.23</td>\n",
       "      <td>1343.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5938.73</td>\n",
       "      <td>1370.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>12917.42</td>\n",
       "      <td>1002.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1317.52</td>\n",
       "      <td>1146.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>483002.32</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4037.12</td>\n",
       "      <td>4364.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1285.49</td>\n",
       "      <td>318.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>50500.31</td>\n",
       "      <td>12.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pack ID  Max Temp Duration (min)  Min Temp Duration (min)\n",
       "0         1                 18123.82                455020.17\n",
       "1         2                  9373.62                106823.98\n",
       "2         3                  2706.24                 16495.67\n",
       "3         4                  1907.29                  5676.68\n",
       "4         5                  2467.23                  1343.68\n",
       "5         6                  5938.73                  1370.54\n",
       "6         7                 12917.42                  1002.65\n",
       "7         8                  1317.52                  1146.07\n",
       "8         9                483002.32                     1.49\n",
       "9        10                  4037.12                  4364.84\n",
       "10       11                  1285.49                   318.99\n",
       "11       12                 50500.31                    12.36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_thermal_voltage_pack_hotspot_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a report showing Pack Hotspot exposure duration \n",
    "    (Max Temp/Min Temp) during high voltage imbalance (volt_delta_mv > 5).\n",
    "    \n",
    "    Required columns: dt_sec, volt_delta_mv, batt_maxtemp_tc, batt_mintemp_tc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Data Type Conversion and Preparation ---\n",
    "    df['volt_delta_mv'] = pd.to_numeric(df['volt_delta_mv'], errors='coerce')\n",
    "    \n",
    "    # Check for duration column and convert to minutes\n",
    "    if 'dt_sec' not in df.columns:\n",
    "        print(\"Error: 'dt_sec' column missing for duration calculation.\")\n",
    "        return pd.DataFrame() \n",
    "\n",
    "    df['dt_min'] = pd.to_numeric(df['dt_sec'], errors='coerce') / 60.0\n",
    "\n",
    "    # --- 2. Filtering ---\n",
    "    # Apply condition: volt_delta_mv > 5 (High Voltage Imbalance)\n",
    "    df_filtered = df[df['volt_delta_mv'] > 5].copy()\n",
    "    \n",
    "    final_report_cols_structure = [\n",
    "        'Pack ID', 'Max Temp Duration (min)', 'Min Temp Duration (min)'\n",
    "    ]\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        return pd.DataFrame(columns=final_report_cols_structure)\n",
    "\n",
    "    # --- 3. Aggregation for MAX Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MAX temp and sum the duration.\n",
    "    max_temp_df = df_filtered.groupby('pack_id_max')['dt_min'].sum().reset_index()\n",
    "    max_temp_df = max_temp_df.rename(columns={\n",
    "        'pack_id_max': 'Pack ID',\n",
    "        'dt_min': 'Max Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 4. Aggregation for MIN Temperature Duration ---\n",
    "    # Group by the TC ID that was reporting the MIN temp and sum the duration.\n",
    "    min_temp_df = df_filtered.groupby('pack_id_min')['dt_min'].sum().reset_index()\n",
    "    min_temp_df = min_temp_df.rename(columns={\n",
    "        'pack_id_min': 'Pack ID',\n",
    "        'dt_min': 'Min Temp Duration (min)'\n",
    "    })\n",
    "\n",
    "    # --- 5. Merge and Cleanup ---\n",
    "    # Perform an Outer Join to combine both results, keeping all TCs\n",
    "    final_report = pd.merge(\n",
    "        max_temp_df, \n",
    "        min_temp_df, \n",
    "        on='Pack ID', \n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "    # Fill NaNs created by the outer merge with 0\n",
    "    final_report = final_report.fillna(0)\n",
    "\n",
    "    # Round all duration metrics\n",
    "    for col in final_report.columns:\n",
    "        if 'Duration' in col:\n",
    "            final_report[col] = final_report[col].round(2)\n",
    "            \n",
    "    # Filter out TCs with zero total exposure\n",
    "    final_report = final_report[\n",
    "        (final_report['Max Temp Duration (min)'] > 0) | \n",
    "        (final_report['Min Temp Duration (min)'] > 0)\n",
    "    ]\n",
    "    \n",
    "    # Ensure ID column is integer (since TC IDs are usually integers)\n",
    "    final_report['Pack ID'] = final_report['Pack ID'].astype(pd.Int64Dtype())\n",
    "\n",
    "    return final_report\n",
    "\n",
    "\n",
    "def run_multi_day_deltav_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads multi-day Parquet data and computes the fleet-wide high DeltaV report, \n",
    "    using a progress bar for the outer loop.\n",
    "    \"\"\"\n",
    "    raw_data = []\n",
    "    total_rows = 0\n",
    "\n",
    "    # Logging setup (remains the same to avoid NameError)\n",
    "    old_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try: \n",
    "        # --- CHANGE HERE: Wrap range(num_days) with tqdm ---\n",
    "        # NOTE: You need to have 'from tqdm import tqdm' imported in your environment\n",
    "        # Use a descriptive label like \"DeltaV Analysis Days\"\n",
    "        for d in tqdm(range(num_days), desc=\"ThermalV Analysis Days\", ncols=70): \n",
    "        # --------------------------------------------------\n",
    "\n",
    "            day_start = start_date + timedelta(days=d)\n",
    "            day_end   = day_start + timedelta(days=1)\n",
    "\n",
    "            # --- Replace with your actual Parquet reading function ---\n",
    "            df_sub = read_parquet_subset(parquet_path, day_start, day_end)\n",
    "            if df_sub.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows += len(df_sub)\n",
    "            \n",
    "            # --- COLLECT RAW DATA ---\n",
    "            cols_needed = ['pack_id_max','pack_id_min', 'dt_sec', 'volt_delta_mv']\n",
    "            cols_present = [col for col in cols_needed if col in df_sub.columns]\n",
    "            \n",
    "            raw_data.append(df_sub[cols_present].copy())\n",
    "\n",
    "            del df_sub\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(old_level)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # RUN FLEET-WIDE DELTAV ANALYSIS (ONCE)\n",
    "    # -------------------------------------------------------------\n",
    "    if raw_data:\n",
    "        full_raw_data_df = pd.concat(raw_data, ignore_index=True)\n",
    "        delta_v_report_df = generate_thermal_voltage_pack_hotspot_report(full_raw_data_df)\n",
    "    else:\n",
    "        delta_v_report_df = generate_thermal_voltage_pack_hotspot_report(pd.DataFrame())\n",
    "\n",
    "    return delta_v_report_df, total_rows\n",
    "\n",
    "\n",
    "thermal_v_pack_hotspot_report, total_rows = run_multi_day_deltav_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=datetime(2025, 9, 1),\n",
    "    num_days=75\n",
    ")\n",
    "\n",
    "display(thermal_v_pack_hotspot_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b4207bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_v_pack_hotspot_report.to_csv('hotspot_pack.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naarni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
