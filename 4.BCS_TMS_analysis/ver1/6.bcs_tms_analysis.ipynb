{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5118c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_datetime64tz_dtype\n",
    "import platform\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import duckdb \n",
    "import warnings\n",
    "import fastparquet\n",
    "from tqdm import tqdm \n",
    "from typing import List, Optional, Union\n",
    "import psutil\n",
    "import time # For timing the execution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Optional: adjust pandas display for debugging; you can comment these out\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee48ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to python 6.bcs_tms_analysis.ipynb \\\n",
    "#     --TemplateExporter.exclude_markdown=True \\\n",
    "#     --TemplateExporter.exclude_output_prompt=True \\\n",
    "#     --TemplateExporter.exclude_input_prompt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3575c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mapping table with 27 entries.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>AP39WF8593</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>AP39WG0252</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>AP39WF8589</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>AP39WF8584</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>AP39WG0271</td>\n",
       "      <td>FB Guntur</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     reg_num   customer  model\n",
       "0  11  AP39WF8593  FB Guntur   12.5\n",
       "1   9  AP39WG0252  FB Guntur   12.5\n",
       "2   7  AP39WF8589  FB Guntur   12.5\n",
       "3  13  AP39WF8584  FB Guntur   12.5\n",
       "4  14  AP39WG0271  FB Guntur   12.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_file = \"../../../data_points/Naarni VehicleID_RegNo_links - Vehicle_mapping.csv\"\n",
    "try:\n",
    "    df_mapping = pd.read_csv(mapping_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mapping file '{mapping_file}' not found. Cannot enrich data.\")\n",
    "    # Create an empty mapping table to allow the rest of the script to run without crashing\n",
    "    df_mapping = pd.DataFrame(columns=[\"id\", \"reg_num\", \"customer\", \"model\"])\n",
    "else:\n",
    "    df_mapping = df_mapping.rename(columns={\n",
    "        \"Device No.\": \"id\",\n",
    "        \"Registration No\": \"reg_num\",\n",
    "        \"Customer\": \"customer\",\n",
    "        \"Model\": \"model\"\n",
    "    })\n",
    "    # Ensure the merge key ('id') is a string to match the chunks\n",
    "    if \"id\" in df_mapping.columns:\n",
    "        df_mapping[\"id\"] = df_mapping[\"id\"].astype(str)\n",
    "        df_mapping = df_mapping[[\"id\", \"reg_num\", \"customer\", \"model\"]]\n",
    "    else:\n",
    "        print(\"Warning: 'Device No.' column not found in mapping file.\")\n",
    "        df_mapping = pd.DataFrame(columns=[\"id\", \"reg_num\", \"customer\", \"model\"])\n",
    "\n",
    "print(f\"Loaded mapping table with {len(df_mapping)} entries.\")\n",
    "# df_mapping is now ready to be passed into the processing function\n",
    "\n",
    "df_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cf6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_COLS = [\n",
    "    \"id\", \"timestamp\", \"dt\",\n",
    "    \"vehiclereadycondition\", \"gun_connection_status\", \"ignitionstatus\",\"odometerreading\",\n",
    "    \"vehicle_speed_vcu\", \"gear_position\",\n",
    "    \"bat_soc\", \"soh\", \"total_battery_current\",\n",
    "    \"pack1_cellmax_temperature\", \"pack1_cell_min_temperature\",\n",
    "    \"pack1_maxtemperature_cell_number\", \"pack1_celltemperature_cellnumber\",\n",
    "    \"bat_voltage\", \"cellmax_voltagecellnumber\", \"cellminvoltagecellnumber\", \n",
    "    \"cell_min_voltage\",\"cell_max_voltage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7067d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_mem():\n",
    "    \"\"\"Try to return freed memory back to the OS (no-op on some platforms).\"\"\"\n",
    "    try:\n",
    "        libc = ctypes.CDLL(None)\n",
    "        if hasattr(libc, \"malloc_trim\"):\n",
    "            libc.malloc_trim(0)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35fd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_battery_temp_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Uses df.rename(inplace=False), creating one copy, which is fine for chunks\n",
    "    rename_map = {\n",
    "        \"pack1_cellmax_temperature\": \"batt_maxtemp\",\n",
    "        \"pack1_cell_min_temperature\": \"batt_mintemp\",\n",
    "        \"pack1_maxtemperature_cell_number\":\"batt_maxtemp_tc\", \n",
    "        \"pack1_celltemperature_cellnumber\":\"batt_mintemp_tc\",\n",
    "        \"cell_max_voltage\":\"batt_maxvolt\",\n",
    "        \"cellmax_voltagecellnumber\":\"batt_maxvolt_cell\",\n",
    "        \"cell_min_voltage\":\"batt_minvolt\",\n",
    "        \"cellminvoltagecellnumber\":\"batt_minvolt_cell\", \n",
    "    }\n",
    "    existing = {k: v for k, v in rename_map.items() if k in df.columns}\n",
    "    if not existing:\n",
    "        return df\n",
    "    return df.rename(columns=existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93dfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# CONFIG\n",
    "# =====================================================================\n",
    "\n",
    "MAX_SPEED_KMPH = 120.0      # physical upper bound (bus never >120 km/h)\n",
    "MAX_ODO_DIST_KM = 0.2       # max plausible odo jump per sample (~200 m)\n",
    "MAX_DT_SEC = 3.0            # dt_sec cap (you already use this)\n",
    "BIG_ODO_CAP = 1.0           # sanity cap for odo (km)\n",
    "DT_DISCONTINUITY_SEC = 180  # >3 min gap can be treated as discontinuity in Stage-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eddf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_odometer(df):\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\"):\n",
    "        idx = grp.index\n",
    "        odo = grp[\"odometer_final\"].to_numpy()\n",
    "\n",
    "        for i in range(1, len(odo)):\n",
    "            if odo[i] < odo[i-1]:\n",
    "                odo[i] = odo[i-1]\n",
    "\n",
    "        df.loc[idx, \"odometer_final\"] = odo\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfc668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_odometer(df, odo_col=\"odometerreading\"):\n",
    "    \"\"\"\n",
    "    TRUE, CORRECT, NULL-ONLY, SESSION-AWARE, 3-PASS ODOMETER IMPUTER.\n",
    "\n",
    "    Rules implemented exactly:\n",
    "\n",
    "      â€¢ PASS 1: Fix top/bottom NULL islands.\n",
    "      â€¢ PASS 2: SINGLE NULL: bracket logic + speed/dt estimate + clamping.\n",
    "      â€¢ PASS 3: MULTI NULL: iterative bounded fill, updating L â†’ new L.\n",
    "      â€¢ Session protection: If R < L â†’ treat as session break â†’ propagate L.\n",
    "      â€¢ STRICT: Never modify original *non-null* odometer readings.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "    df[\"odometer_final\"] = df[odo_col].astype(float)\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\"):\n",
    "        idx = grp.index\n",
    "        odo_raw = grp[odo_col].astype(float).to_numpy()\n",
    "        speed = grp[\"vehicle_speed_vcu\"].astype(float).to_numpy()\n",
    "        dt = grp[\"dt_sec\"].astype(float).to_numpy()\n",
    "\n",
    "        # Final output buffer:\n",
    "        fill = odo_raw.copy()\n",
    "\n",
    "        n = len(odo_raw)\n",
    "        i = 0\n",
    "\n",
    "        while i < n:\n",
    "            if not np.isnan(odo_raw[i]):\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Start of a null island\n",
    "            start = i\n",
    "            while i < n and np.isnan(odo_raw[i]):\n",
    "                i += 1\n",
    "            end = i - 1  # inclusive\n",
    "\n",
    "            prev_idx = start - 1\n",
    "            next_idx = end + 1\n",
    "\n",
    "            L = odo_raw[prev_idx] if prev_idx >= 0 else None\n",
    "            R = odo_raw[next_idx] if next_idx < n else None\n",
    "\n",
    "            # -------------------------------\n",
    "            # PASS 1: TOP & BOTTOM NULL ISLANDS\n",
    "            # -------------------------------\n",
    "            if L is None and R is not None:\n",
    "                # Top NULL block â†’ propagate next known value backward\n",
    "                for pos in range(start, end + 1):\n",
    "                    fill[pos] = R\n",
    "                continue\n",
    "\n",
    "            if R is None and L is not None:\n",
    "                # Bottom NULL block â†’ propagate previous known value forward\n",
    "                for pos in range(start, end + 1):\n",
    "                    fill[pos] = L\n",
    "                continue\n",
    "\n",
    "            # If both missing â†’ extremely rare but fallback to zero change\n",
    "            if L is None and R is None:\n",
    "                continue\n",
    "\n",
    "            # -------------------------------\n",
    "            # SESSION BREAK PROTECTION\n",
    "            # -------------------------------\n",
    "            if R < L:\n",
    "                # Monotonic break â†’ treat as end-of-session\n",
    "                for pos in range(start, end + 1):\n",
    "                    fill[pos] = L\n",
    "                continue\n",
    "\n",
    "            # -------------------------------\n",
    "            # PASS 2 & PASS 3 (Unified Engine)\n",
    "            # -------------------------------\n",
    "            gap = end - start + 1\n",
    "\n",
    "            # The active bounds shrink as we impute\n",
    "            curr_L = L\n",
    "            curr_R = R\n",
    "\n",
    "            for k in range(gap):\n",
    "                pos = start + k\n",
    "\n",
    "                if speed[pos] == 0:\n",
    "                    est = curr_L  # idle â†’ no movement\n",
    "                else:\n",
    "                    # compute movement in km\n",
    "                    est = curr_L + (speed[pos] * dt[pos] / 3600.0)\n",
    "\n",
    "                # Clamp within [curr_L, curr_R]\n",
    "                est_clamped = max(curr_L, min(est, curr_R))\n",
    "\n",
    "                # STRICT: only fill if original was NULL\n",
    "                if np.isnan(odo_raw[pos]):\n",
    "                    fill[pos] = est_clamped\n",
    "\n",
    "                # Shrink left boundary â†’ progressive update\n",
    "                curr_L = fill[pos]\n",
    "\n",
    "        df.loc[idx, \"odometer_final\"] = np.round(fill, 3)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c43ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean & impute all sensors EXCEPT odometer.\n",
    "\n",
    "    Includes:\n",
    "      - dt_sec sanitisation\n",
    "      - SOC fixing (SOC=0 â†’ NaN â†’ interpolated)\n",
    "      - temperature, voltage, TC/cell sanity\n",
    "      - refined battery current clamping\n",
    "      - ignition/ready/gun consistency\n",
    "      - charging-mode overrides\n",
    "      - parked-mode overrides\n",
    "      - speed imputation for all stable states\n",
    "      - gear correction\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # Round column 'odometerreading' to 3 decimal places, preserving NaNs\n",
    "    mask_odo = df['odometerreading'].notna() # Create a boolean mask for non-null values\n",
    "    df.loc[mask_odo, 'odometerreading'] = df.loc[mask_odo, 'odometerreading'].round(3)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 0. dt_sec calculation\n",
    "    # ----------------------------------------------------\n",
    "    df[\"dt_sec\"] = (\n",
    "        df.groupby(\"id\")[\"timestamp\"]\n",
    "          .diff()\n",
    "          .dt.total_seconds()\n",
    "          .fillna(0)\n",
    "    )\n",
    "    df.loc[df[\"dt_sec\"] > 3, \"dt_sec\"] = 0\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1. SANITISATION\n",
    "    # ----------------------------------------------------\n",
    "    # 1a temperature sanitisation\n",
    "    for col in [\"batt_maxtemp\", \"batt_mintemp\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[df[col] < -10, col] = pd.NA\n",
    "\n",
    "    # 1b voltage\n",
    "    for col in [\"batt_maxvolt\", \"batt_minvolt\", \"bat_voltage\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[df[col] <= 0, col] = pd.NA\n",
    "\n",
    "    # 1c thermocouple + cell\n",
    "    for col in [\"batt_maxtemp_tc\", \"batt_mintemp_tc\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[(df[col] < 1) | (df[col] > 108), col] = pd.NA\n",
    "\n",
    "    for col in [\"batt_maxvolt_cell\", \"batt_minvolt_cell\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[(df[col] < 1) | (df[col] > 576), col] = pd.NA\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1d SOC FIX â€” VERY IMPORTANT\n",
    "    # ----------------------------------------------------\n",
    "    df[\"bat_soc\"] = pd.to_numeric(df[\"bat_soc\"], errors=\"coerce\")\n",
    "\n",
    "    # SOC=0 is almost always a sensor glitch â†’ treat as missing\n",
    "    df.loc[df[\"bat_soc\"] == 0, \"bat_soc\"] = np.nan\n",
    "\n",
    "    # Interpolate SOC per vehicle\n",
    "    df[\"bat_soc\"] = (\n",
    "        df.groupby(\"id\")[\"bat_soc\"]\n",
    "        .transform(lambda s: s.interpolate(limit_direction=\"both\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    df[\"bat_soc\"] = df[\"bat_soc\"].clip(lower=0, upper=100)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1e refined battery current clamp\n",
    "    # ----------------------------------------------------\n",
    "    curr = pd.to_numeric(df[\"total_battery_current\"], errors=\"coerce\")\n",
    "    valid_mask = curr.abs().between(0, 2500)\n",
    "    valid_values = curr.where(valid_mask)\n",
    "\n",
    "    curr_ff = valid_values.ffill().fillna(0.0)\n",
    "    curr = curr.where(valid_mask, curr_ff)\n",
    "    df[\"total_battery_current\"] = curr\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2. GROUPWISE IMPUTATION (per vehicle)\n",
    "    # ----------------------------------------------------\n",
    "    impute_cols = [\n",
    "        (\"batt_maxtemp\", 80),\n",
    "        (\"batt_mintemp\", 80),\n",
    "        (\"batt_maxtemp_tc\", 80),\n",
    "        (\"batt_mintemp_tc\", 80),\n",
    "        (\"batt_maxvolt\", 30),\n",
    "        (\"batt_minvolt\", 30),\n",
    "        (\"batt_maxvolt_cell\", 30),\n",
    "        (\"batt_minvolt_cell\", 30),\n",
    "        (\"bat_voltage\", 20),\n",
    "        (\"bat_soc\", 300),   # now cleaned\n",
    "        (\"soh\", 300),\n",
    "    ]\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\"):\n",
    "        idx = grp.index\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2a forward/backfill for regular sensors\n",
    "        # -----------------------------------------\n",
    "        for col, limit in impute_cols:\n",
    "            df.loc[idx, col] = grp[col].ffill().bfill()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2b current interpolation for small gaps\n",
    "        # -----------------------------------------\n",
    "        df.loc[idx, \"total_battery_current\"] = grp[\"total_battery_current\"].interpolate(\n",
    "            limit=10, limit_direction=\"both\"\n",
    "        )\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2c BASIC READY + GUN fill\n",
    "        # -----------------------------------------\n",
    "        for col in [\"vehiclereadycondition\", \"gun_connection_status\"]:df.loc[idx, col] = grp[col].ffill().bfill()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2d IGNITIONSTATUS CLEANUP\n",
    "        # -----------------------------------------\n",
    "        ign = pd.to_numeric(grp[\"ignitionstatus\"], errors=\"coerce\")\n",
    "        ign = ign.ffill().bfill()\n",
    "\n",
    "        # Ready=1 & ignition null â†’ ignition=1\n",
    "        ready_mask = df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int).eq(1)\n",
    "        ign.loc[ready_mask & ign.isna()] = 1\n",
    "        df.loc[idx, \"ignitionstatus\"] = ign\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2e SPEED IMPUTATION\n",
    "        # -----------------------------------------\n",
    "        if \"vehicle_speed_vcu\" in grp.columns:\n",
    "            v = pd.to_numeric(grp[\"vehicle_speed_vcu\"], errors=\"coerce\")\n",
    "            v = v.where(v.between(0, 120), np.nan)\n",
    "\n",
    "            v = v.ffill().bfill()\n",
    "            df.loc[idx, \"vehicle_speed_vcu\"] = v.round(2)\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2f GEAR POSITION\n",
    "        # -----------------------------------------\n",
    "        if \"gear_position\" in grp.columns:\n",
    "            g = pd.to_numeric(grp[\"gear_position\"], errors=\"coerce\")\n",
    "            g = g.where(g.isin([0, 1, 2]), np.nan)\n",
    "\n",
    "            ready0 = df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int).eq(0)\n",
    "            ign0 = df.loc[idx, \"ignitionstatus\"].fillna(0).astype(int).eq(0)\n",
    "\n",
    "            force_neutral = ready0 | ign0\n",
    "            g[force_neutral] = 0\n",
    "\n",
    "            df.loc[idx, \"gear_position\"] = g.ffill().bfill().astype(\"Int64\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2g CHARGING STATE CONSISTENCY\n",
    "        # ----------------------------------------------------\n",
    "        charging = df.loc[idx, \"gun_connection_status\"].fillna(0).astype(int).eq(1)\n",
    "\n",
    "        # ignition ON during charging\n",
    "        df.loc[idx[charging], \"ignitionstatus\"] = 1\n",
    "        df.loc[idx[charging], \"vehiclereadycondition\"] = 0\n",
    "        df.loc[idx[charging], \"gear_position\"] = 0\n",
    "\n",
    "        # speed=0 when charging & missing\n",
    "        df.loc[idx[charging & df.loc[idx, \"vehicle_speed_vcu\"].isna()],\"vehicle_speed_vcu\"] = 0.0\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2h OFF/PARKED SPEED FIX\n",
    "        # gun=0 & ready=0 & ignition=0 & speed NA â†’ 0\n",
    "        # ----------------------------------------------------\n",
    "        off_mask = (\n",
    "            (df.loc[idx, \"gun_connection_status\"].fillna(0).astype(int) == 0) &\n",
    "            (df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int) == 0) &\n",
    "            (df.loc[idx, \"ignitionstatus\"].fillna(0).astype(int) == 0)\n",
    "        )\n",
    "\n",
    "        df.loc[idx[off_mask & df.loc[idx, \"vehicle_speed_vcu\"].isna()],\"vehicle_speed_vcu\"] = 0.0\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2i READY BUT IGNITION=0 (contradictory â†’ treat as stationary)\n",
    "        # ----------------------------------------------------\n",
    "        ready_ign_off = (\n",
    "            (df.loc[idx, \"gun_connection_status\"].fillna(0).astype(int) == 0) &\n",
    "            (df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int) == 1) &\n",
    "            (df.loc[idx, \"ignitionstatus\"].fillna(0).astype(int) == 0)\n",
    "        )\n",
    "\n",
    "        df.loc[idx[ready_ign_off & df.loc[idx, \"vehicle_speed_vcu\"].isna()],\"vehicle_speed_vcu\"] = 0.0\n",
    "        \n",
    "        df.vehicle_speed_vcu = df.vehicle_speed_vcu.round(2)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40517f63",
   "metadata": {},
   "source": [
    "Aï¸âƒ£ First: we derive a boolean signal: `gun_connected = (out[\"gun_connection_status\"] == 1)`\n",
    "\n",
    "ðŸ”‹ CHARGING_ACTIVE: `chg_active = (gun_connected) & (current > +5A)`\n",
    "- Battery is in CC/CV mode, current > 0, charger locked â†’ SOC must increase or stay flat.\n",
    "- If it drops â†’ timestamp glitch, SOC jitter, or packet misordering â†’ G L I T C H\n",
    "\n",
    "ðŸ”‹ CHARGING_MAINTAIN: `chg_maint = (gun_connected) & current.abs().between(0, 5)`\n",
    "- BMS balancing may cause Â±0.1â€“0.3 % SOC wobble.\n",
    "- Bigger drops = glitch.\n",
    "\n",
    "ðŸ”‹ CHARGING_IDLE: chg_idle = `(gun_connected) & (current > 5)`\n",
    "- Charger connected but no current. SOC may drift slightly due to temperature compensation. Â±0.5% jitter is normal.\n",
    "\n",
    "\n",
    "Bï¸âƒ£ Then DISCHARGING_* states\n",
    "- âœ” DISCHARGING_ACTIVE\n",
    "- dis_active = (\n",
    "    (~gun_connected) &\n",
    "    (vehicle_speed_vcu > 0.5) &\n",
    "    (gear_position in [1,2])\n",
    ")\n",
    "\n",
    "* DISCHARGING_IDLE\n",
    "- Everything else not covered by the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a1c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_df_with_state(df: pd.DataFrame, df_mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Make a copy (chunk-safe)\n",
    "    out = df.copy()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Merge mapping\n",
    "    # ------------------------------------------------------------------\n",
    "    out[\"id\"] = out[\"id\"].astype(str)\n",
    "    out = out.merge(df_mapping, on=\"id\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    # fill mapping fallbacks\n",
    "    out[\"reg_num\"] = out[\"reg_num\"].fillna(\"REGNUM_\" + out[\"id\"])\n",
    "    out[\"customer\"] = out[\"customer\"].fillna(\"CUST_\" + out[\"id\"])\n",
    "    out[\"model\"] = out[\"model\"].fillna(\"MDL_\" + out[\"id\"])\n",
    "\n",
    "    out[\"reg_num\"] = out[\"reg_num\"].astype(str)\n",
    "    out[\"customer\"] = out[\"customer\"].astype(str)\n",
    "    out[\"model\"] = out[\"model\"].astype(str)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2. Timestamp handling â€” THE SAFE VERSION\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # --- TIMESTAMP FIX (UTC â†’ IST) ---\n",
    "\n",
    "    # 1. Parse raw timestamp exactly as received\n",
    "    out[\"ts_utc\"] = pd.to_datetime(out[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # 2. Drop invalid rows\n",
    "    out = out.dropna(subset=[\"ts_utc\"])\n",
    "\n",
    "    # 3. Convert to Asia/Kolkata (IST)\n",
    "    out[\"timestamp\"] = out[\"ts_utc\"].dt.tz_convert(\"Asia/Kolkata\")\n",
    "\n",
    "    # 4. Remove timezone info from final timestamp if needed\n",
    "    #    (matplotlib, parquet, feather become safer with tz-naive)\n",
    "    out[\"timestamp\"] = out[\"timestamp\"].dt.tz_localize(None)\n",
    "\n",
    "    # 5. Sort by vehicle + IST timestamp\n",
    "    out = out.sort_values([\"id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Mode + alt_mode logic (unchanged)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Gun connection normalization\n",
    "    gcs_raw = out[\"gun_connection_status\"]\n",
    "    gcs_num = pd.to_numeric(gcs_raw, errors=\"coerce\")\n",
    "    gcs_str = gcs_raw.astype(str).str.lower().str.strip()\n",
    "\n",
    "    gun_connected = (gcs_num == 1) | gcs_str.isin({\"1\",\"true\",\"yes\",\"y\",\"connected\",\"on\"})\n",
    "    gun_connected = gun_connected.fillna(False)\n",
    "\n",
    "    # Vehicle readiness normalization\n",
    "    if \"vehiclereadycondition\" in out.columns:\n",
    "        vrc_raw = out[\"vehiclereadycondition\"]\n",
    "        vrc_num = pd.to_numeric(vrc_raw, errors=\"coerce\")\n",
    "        vrc_str = vrc_raw.astype(str).str.strip().str.lower()\n",
    "        vehicle_ready = (vrc_num == 1) | vrc_str.isin({\"1\",\"true\",\"yes\",\"y\",\"ready\",\"on\"})\n",
    "        vehicle_ready = vehicle_ready.fillna(False)\n",
    "    else:\n",
    "        vehicle_ready = pd.Series(False, index=out.index)\n",
    "\n",
    "    # Legacy mode column\n",
    "    # out[\"mode\"] = np.where(gun_connected, \"CHARGING\", \"DISCHARGING\")\n",
    "\n",
    "    # Rolling current for alt_mode\n",
    "    current_rm = (\n",
    "        out[\"total_battery_current\"]\n",
    "        .rolling(15, min_periods=1)\n",
    "        .mean()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # thresholds\n",
    "    ACTIVE_CHG_THRESH = -15\n",
    "    MAINTAIN_LOW = -15\n",
    "    MAINTAIN_HIGH = +2\n",
    "\n",
    "    # CHARGING states\n",
    "    chg_active = (gun_connected &(out[\"total_battery_current\"] < -5)).to_numpy(dtype=bool)\n",
    "    chg_maint = (gun_connected &(out[\"total_battery_current\"].abs().between(0, 5))).to_numpy(dtype=bool)\n",
    "    chg_idle = (gun_connected &(out[\"total_battery_current\"] > 5)).to_numpy(dtype=bool)\n",
    "\n",
    "    # # DISCHARGING states\n",
    "    dis_active = ((~gun_connected) &(out[\"vehicle_speed_vcu\"].gt(0.5).fillna(False)) &(out[\"gear_position\"].isin([1, 2]).fillna(False))).to_numpy(dtype=bool)\n",
    "    # dis_idle   = ((~gun_connected) & (~dis_active)).to_numpy(dtype=bool)\n",
    "\n",
    "    out[\"alt_mode\"] = np.select(\n",
    "        [chg_active, chg_maint, chg_idle, dis_active],\n",
    "        [\"CHARGING_ACTIVE\", \"CHARGING_MAINTAIN\", \"CHARGING_IDLE\", \"DISCHARGING_ACTIVE\"],\n",
    "        default=\"DISCHARGING_IDLE\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. Delta + buckets (unchanged)\n",
    "    # ------------------------------------------------------------------\n",
    "    for col in [\"batt_maxtemp\", \"batt_mintemp\", \"batt_maxvolt\", \"batt_minvolt\"]:\n",
    "        if col in out.columns:\n",
    "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "\n",
    "    out[\"batt_temp_delta\"] = out[\"batt_maxtemp\"] - out[\"batt_mintemp\"]\n",
    "    out[\"volt_delta_mv\"] = abs((out[\"batt_maxvolt\"] - out[\"batt_minvolt\"]) * 1000)  # absolute value since max < min can happen\n",
    "\n",
    "    out[\"date_val\"] = out[\"timestamp\"].dt.floor(\"D\")\n",
    "\n",
    "    # Bucketing\n",
    "    out[\"maxtemp_bucket\"] = pd.cut(\n",
    "        out[\"batt_maxtemp\"],\n",
    "        [-np.inf, 28, 32, 35, 40, np.inf],\n",
    "        labels=[\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"]\n",
    "    )\n",
    "\n",
    "    out[\"temp_delta_bucket\"] = pd.cut(\n",
    "        out[\"batt_temp_delta\"],\n",
    "        [-np.inf, 2, 5, 8, np.inf],\n",
    "        labels=[\"<2\", \"2â€“5\", \"5â€“8\", \">8\"]\n",
    "    )\n",
    "\n",
    "    out[\"volt_delta_bucket\"] = pd.cut(\n",
    "        out[\"volt_delta_mv\"],\n",
    "        [0, 10, 20, 30, np.inf],\n",
    "        labels=[\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    soc_bins = [0,10,20,30,40,50,60,70,80,90,np.inf]\n",
    "    soc_labels = [\"0â€“10\",\"10â€“20\",\"20â€“30\",\"30-40\",\"40-50\",\"50-60\",\"60-70\",\"70-80\",\"80-90\",\"90-100\"]\n",
    "\n",
    "    out[\"soc_band_bucket\"] = pd.cut(out[\"bat_soc\"], bins=soc_bins, labels=soc_labels)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. Select final columns\n",
    "    # ------------------------------------------------------------------\n",
    "    cols_keep = [\n",
    "        \"id\",\"reg_num\",\"customer\",\"model\",\n",
    "        \"timestamp\",\"date_val\",\"dt_sec\",\n",
    "        \"mode\",\"alt_mode\",\n",
    "        \"ignitionstatus\",\"vehiclereadycondition\",\"gun_connection_status\",\n",
    "        \"vehicle_speed_vcu\",\"gear_position\",\n",
    "        \"odometerreading\",\"odometer_final\",\n",
    "        \"batt_maxtemp\",\"batt_mintemp\",\"batt_temp_delta\",\n",
    "        \"maxtemp_bucket\",\"temp_delta_bucket\",\n",
    "        \"batt_maxvolt\",\"batt_minvolt\",\"volt_delta_mv\",\"volt_delta_bucket\",\n",
    "        \"batt_maxtemp_tc\",\"batt_mintemp_tc\",\n",
    "        \"pack_id_max\",\"pack_id_min\",\n",
    "        \"batt_maxvolt_cell\",\"batt_minvolt_cell\",\n",
    "        \"bat_voltage\",\"total_battery_current\",\n",
    "        \"bat_soc\",\"soc_band_bucket\",\"soh\"\n",
    "    ]\n",
    "\n",
    "    cols_keep = [c for c in cols_keep if c in out.columns]\n",
    "    out = out[cols_keep]\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2e67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DUCKDB CHUNK GENERATOR (Fixed) ---\n",
    "\n",
    "def duckdb_chunk_generator(conn, sql_query, chunk_size):\n",
    "    \"\"\"Generates Pandas DataFrames in chunks directly from DuckDB cursor.\"\"\"\n",
    "    cursor = conn.cursor() \n",
    "    cursor.execute(sql_query)\n",
    "    \n",
    "    while True:\n",
    "        # Uses the corrected method name: fetch_df_chunk\n",
    "        chunk = cursor.fetch_df_chunk(chunk_size) \n",
    "        if chunk is None or chunk.empty:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "# --- ROBUST FILE EXTRACTION (Fixed from OSErrors) ---\n",
    "\n",
    "def extract_files_to_disk(zip_path, output_dir):\n",
    "    \"\"\"Cleans directory and extracts all Parquet files from ZIP.\"\"\"\n",
    "    if output_dir.exists():\n",
    "        logging.info(f\"ðŸ§¹ Clearing existing directory: {output_dir.resolve()}\")\n",
    "        # Robust cleanup to avoid OS/lock issues\n",
    "        try:\n",
    "            shutil.rmtree(output_dir)\n",
    "        except OSError:\n",
    "             for item in output_dir.iterdir():\n",
    "                if item.is_dir():\n",
    "                    shutil.rmtree(item)\n",
    "                else:\n",
    "                    os.remove(item) \n",
    "             os.rmdir(output_dir)\n",
    "\n",
    "    output_dir.mkdir(parents=True)\n",
    "        \n",
    "    logging.info(\"ðŸ”„ Extracting ALL Parquet files from ZIP to disk...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            all_files_to_extract = [f for f in z.namelist() if f.endswith(\".parquet\")]\n",
    "            logging.info(f\"ðŸ”Ž Found {len(all_files_to_extract)} total Parquet files in archive.\")\n",
    "            for filename in all_files_to_extract:\n",
    "                z.extract(filename, path=output_dir)\n",
    "            return len(all_files_to_extract)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"âŒ ZIP file not found at: {zip_path}\") from None\n",
    "\n",
    "def setup_duckdb_query(output_dir, utc_start, utc_end, core_cols):\n",
    "    \"\"\"Sets up DuckDB connection and SQL query.\"\"\"\n",
    "    parquet_glob_path = str(output_dir.joinpath(\"**/*.parquet\"))\n",
    "    # Only select the columns you need for Stage 1 processing\n",
    "    column_list = \", \".join([f'\"{c}\"' for c in core_cols])\n",
    "    \n",
    "    # CRITICAL: Predicate Pushdown filter on the internal 'timestamp' column\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT {column_list}\n",
    "        FROM read_parquet('{parquet_glob_path}')\n",
    "        WHERE \n",
    "            \"timestamp\" >= '{utc_start.isoformat()}' AND \n",
    "            \"timestamp\" < '{utc_end.isoformat()}'\n",
    "    \"\"\"\n",
    "    return duckdb.connect(), sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dc1a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage1_data_setup(analysis_start_date_str: str, \n",
    "                          analysis_end_date_str: str, \n",
    "                          zip_path: Path, \n",
    "                          extraction_dir: Path,\n",
    "                          force_extraction: bool = False) -> tuple[datetime, datetime, int]:\n",
    "    \"\"\"\n",
    "    Handles date range setup, IST-to-UTC conversion, file extraction, \n",
    "    and checks if data is available for processing.\n",
    "    \n",
    "    Args:\n",
    "        analysis_start_date_str: Start date in YYYY-MM-DD format.\n",
    "        analysis_end_date_str: End date in YYYY-MM-DD format.\n",
    "        zip_path: Path to the source ZIP file.\n",
    "        extraction_dir: Target directory for extracted Parquet files.\n",
    "        force_extraction: If True, always clean and re-extract files. \n",
    "                          If False, skips extraction if the directory exists.\n",
    "    \n",
    "    Returns: (utc_start, utc_end, file_count)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Date Parsing and UTC Conversion (Assuming +5:30 IST offset)\n",
    "    target_date = datetime.strptime(analysis_start_date_str, \"%Y-%m-%d\").date()\n",
    "    ist_start = datetime.combine(target_date, datetime.min.time())\n",
    "    \n",
    "    end_date_obj = datetime.strptime(analysis_end_date_str, \"%Y-%m-%d\").date()\n",
    "    ist_end = datetime.combine(end_date_obj, datetime.min.time()) + timedelta(days=1)\n",
    "    \n",
    "    utc_start = ist_start - timedelta(hours=5, minutes=30)\n",
    "    utc_end = ist_end - timedelta(hours=5, minutes=30)\n",
    "    \n",
    "    logging.info(f\"ðŸ” Analysis window (UTC): {utc_start} â†’ {utc_end}\")\n",
    "\n",
    "    # 2. FILE EXTRACTION CONTROL\n",
    "    file_count = 0\n",
    "    \n",
    "    if extraction_dir.exists() and not force_extraction:\n",
    "        logging.info(\"â™»ï¸ Skipping file extraction: Directory exists and force_extraction=False.\")\n",
    "        # Recursively count all .parquet files in the existing directory\n",
    "        file_count = len(list(extraction_dir.rglob('*.parquet')))\n",
    "        if file_count > 0:\n",
    "             logging.info(f\"âœ… Found {file_count} existing files. Proceeding to DuckDB loading.\")\n",
    "        \n",
    "    else:\n",
    "        # If directory doesn't exist, or force_extraction is True, run the full extraction.\n",
    "        logging.info(\"ðŸ”„ Running full extraction (Cleanup + Extract)...\")\n",
    "        # This relies on the robust `extract_files_to_disk` function\n",
    "        file_count = extract_files_to_disk(zip_path, extraction_dir)\n",
    "        \n",
    "    # 3. Validation Check\n",
    "    if file_count == 0:\n",
    "        logging.warning(\"ðŸ›‘ Skipping analysis: No files were found.\")\n",
    "        sys.exit() # Exit the script cleanly if no files were found\n",
    "\n",
    "    return utc_start, utc_end, file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2aee4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 10:32:44 - INFO - ðŸ” Analysis window (UTC): 2025-08-31 18:30:00 â†’ 2025-11-15 18:30:00\n",
      "2025-12-03 10:32:44 - INFO - â™»ï¸ Skipping file extraction: Directory exists and force_extraction=False.\n",
      "2025-12-03 10:32:44 - INFO - âœ… Found 474 existing files. Proceeding to DuckDB loading.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION (Ensure these are defined at the top of your script) ---\n",
    "ZIP_FILE_PATH = \"../../../data_points/naarni75_cpoall.zip\" \n",
    "EXTRACTION_DIR = Path(\"../../../data_points/extracted_parts/cpo_all\") \n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- Set 75-Day Date Range (Using your target dates) ---\n",
    "analysis_start_date_str = \"2025-09-01\" \n",
    "# NOTE: Using 2025-11-14 since 75 days starts on 2025-09-01 and ends on 2025-11-14.\n",
    "# Using 2025-11-15 will include the start of the 76th day (if data exists).\n",
    "analysis_end_date_str = \"2025-11-15\"   \n",
    "# --------------------------------------------------------\n",
    "\n",
    "# NEW STAGE 1 EXECUTION:\n",
    "utc_start, utc_end, file_count = run_stage1_data_setup(\n",
    "    analysis_start_date_str=analysis_start_date_str,\n",
    "    analysis_end_date_str=analysis_end_date_str,\n",
    "    zip_path=ZIP_FILE_PATH,\n",
    "    extraction_dir=EXTRACTION_DIR,\n",
    "    force_extraction = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4e8cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 10:32:45 - INFO - âœ… Found 29 unique vehicle IDs in the dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '22', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '41', '42', '46', '6', '7', '9']\n"
     ]
    }
   ],
   "source": [
    "# 2. SETUP DUCKDB QUERY\n",
    "conn, sql_query = setup_duckdb_query(EXTRACTION_DIR, utc_start, utc_end, CORE_COLS)\n",
    "\n",
    "# A quick DuckDB query to get the distinct IDs from the 75-day filtered dataset\n",
    "get_ids_query = f\"\"\"\n",
    "    SELECT DISTINCT id \n",
    "    FROM ({sql_query})\n",
    "\"\"\"\n",
    "# Fetch the list of IDs (this is a very small amount of data)\n",
    "vehicle_ids = conn.execute(get_ids_query).fetchdf()[\"id\"].astype(str).tolist()\n",
    "# vehicle_ids = ['3','16','18','19','32','42','6','7','9','11','12','13','14','15','20','25','27','28','29','30','31','33','35','41','46']\n",
    "\n",
    "logging.info(f\"âœ… Found {len(vehicle_ids)} unique vehicle IDs in the dataset.\")\n",
    "print(sorted(vehicle_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa8a5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The functions 'duckdb_chunk_generator', 'rename_battery_temp_columns', \n",
    "# 'impute_missing_values', 'prepare_df_with_state', 'finalize_odometer', and 'free_mem' \n",
    "# MUST be defined in your Jupyter Notebook environment before calling this.\n",
    "\n",
    "def process_and_save_data(\n",
    "    conn: duckdb.DuckDBPyConnection, \n",
    "    sql_query: str, \n",
    "    chunk_size: int, \n",
    "    parquet_feather_path: str,\n",
    "    vehicle_ids: List[str],\n",
    "    df_mapping: pd.DataFrame,\n",
    "    extract_data: bool = False,\n",
    "    chunk_log_path: Optional[Union[str, Path]] = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Executes the memory-safe chunked processing loop for Stage 1.\n",
    "\n",
    "    MODIFIED (corrected odometer handling):\n",
    "      - Cross-chunk odometer continuity is tracked via last_known_odo.\n",
    "      - last_known_odo is now updated *after* finalize_odometer(), using\n",
    "        the finalized odometer_final values in df_chunk_state.\n",
    "      - This guarantees no backward odometer jumps across chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = Path(parquet_feather_path)\n",
    "    # Cross-chunk odometer cache: {vehicle_id: last_valid_odometer_final}\n",
    "    last_known_odo: Dict[str, float] = {}\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # SKIP IF FILE EXISTS\n",
    "    # -------------------------------------------------------\n",
    "    if not extract_data and output_path.exists():\n",
    "        conn_count = duckdb.connect()\n",
    "        try:\n",
    "            total_rows = conn_count.execute(\n",
    "                f\"SELECT count(*) FROM '{parquet_feather_path}'\"\n",
    "            ).fetchone()[0]\n",
    "            logging.info(f\"âœ… Skipping: File already exists with {total_rows:,} rows.\")\n",
    "            return total_rows\n",
    "        finally:\n",
    "            conn_count.close()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # INIT & PRE-CALCULATION\n",
    "    # -------------------------------------------------------\n",
    "    logging.info(f\"ðŸ§  Preparing data stream...\")\n",
    "\n",
    "    try:\n",
    "        total_input_rows = conn.execute(\n",
    "            f\"SELECT COUNT(*) FROM ({sql_query})\"\n",
    "        ).fetchone()[0]\n",
    "        logging.info(f\"ðŸ“Š Total rows to process: {total_input_rows:,}\")\n",
    "    except Exception as e:\n",
    "        logging.warning(\n",
    "            f\"Could not determine total row count: {e}. Progress bar will be indefinite.\"\n",
    "        )\n",
    "        total_input_rows = None\n",
    "\n",
    "    logging.info(f\"ðŸ’¾ Output path: {parquet_feather_path}\")\n",
    "\n",
    "    first_chunk = True\n",
    "    total_processed_rows = 0\n",
    "    chunk_index = 0\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    psutil.cpu_percent(interval=None)  # prime CPU sampling\n",
    "\n",
    "    # Chunk timing log setup\n",
    "    log_file = None\n",
    "    if chunk_log_path is not None:\n",
    "        log_file = Path(chunk_log_path)\n",
    "        if not log_file.exists():\n",
    "            with log_file.open(\"w\") as f:\n",
    "                f.write(\n",
    "                    \"timestamp,chunk_idx,chunk_rows,total_rows,\"\n",
    "                    \"duration_sec,rows_per_sec,cpu_pct,ram_mb\\n\"\n",
    "                )\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # LOGGING SUPPRESSION\n",
    "    # -------------------------------------------------------\n",
    "    current_log_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try:\n",
    "        # -------------------------------------------------------\n",
    "        # PROGRESS BAR\n",
    "        # -------------------------------------------------------\n",
    "        progress = tqdm(\n",
    "            total=total_input_rows,\n",
    "            desc=\"Processing Data\",\n",
    "            unit=\"row\",\n",
    "            mininterval=0.2,\n",
    "            dynamic_ncols=True,\n",
    "            colour=\"cyan\",\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # MAIN LOOP: STREAM CHUNKS\n",
    "        # -------------------------------------------------------\n",
    "        for chunk in duckdb_chunk_generator(conn, sql_query, chunk_size):\n",
    "            t0 = time.perf_counter()\n",
    "            chunk_index += 1\n",
    "\n",
    "            raw_chunk_len = len(chunk)\n",
    "            df_chunk = chunk\n",
    "\n",
    "            # --- PREP 1: Rename ---\n",
    "            df_chunk = rename_battery_temp_columns(df_chunk)\n",
    "\n",
    "            df_chunk['vehicle_speed_vcu'] = df_chunk['vehicle_speed_vcu'].round(2)\n",
    "\n",
    "            # --- PREP 2: Filter & Cast ---\n",
    "            if \"id\" in df_chunk.columns:\n",
    "                df_chunk[\"id\"] = df_chunk[\"id\"].astype(str)\n",
    "                if vehicle_ids:\n",
    "                    df_chunk = df_chunk[df_chunk[\"id\"].isin(vehicle_ids)]\n",
    "                df_chunk = df_chunk.convert_dtypes()\n",
    "\n",
    "            # --- PREP 2b: Cross-chunk odometer continuity (odometerreading) ---\n",
    "            if \"odometerreading\" in df_chunk.columns:\n",
    "                for vid in df_chunk[\"id\"].unique():\n",
    "                    if vid in last_known_odo:\n",
    "                        first_idx = df_chunk[df_chunk[\"id\"] == vid].index.min()\n",
    "                        if pd.isna(df_chunk.loc[first_idx, \"odometerreading\"]):\n",
    "                            df_chunk.loc[first_idx, \"odometerreading\"] = last_known_odo[vid]\n",
    "\n",
    "            # --- PREP 3: Time & Impute ---\n",
    "            df_chunk[\"timestamp\"] = pd.to_datetime(df_chunk[\"timestamp\"], errors=\"coerce\")\n",
    "            df_chunk = df_chunk.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "            df_chunk = impute_missing_values(df_chunk)\n",
    "            df_chunk = impute_odometer(df_chunk)\n",
    "\n",
    "            # # --- PREP 4: State Prep ---\n",
    "            df_chunk_state = prepare_df_with_state(df_chunk, df_mapping)\n",
    "\n",
    "            # --- PREP 4b: FINALIZE ODOMETER on state DataFrame ---\n",
    "            # This should enforce rounding + monotonicity.\n",
    "            df_chunk_state = finalize_odometer(df_chunk_state)\n",
    "\n",
    "            # --- PREP 4c: Update cross-chunk continuity AFTER finalization ---\n",
    "            if \"odometer_final\" in df_chunk_state.columns:\n",
    "                for vid, sub in df_chunk_state.groupby(\"id\"):\n",
    "                    sub_final = sub[\"odometer_final\"].dropna()\n",
    "                    if len(sub_final):\n",
    "                        # Use *finalized* odometer for continuity\n",
    "                        last_known_odo[vid] = float(sub_final.iloc[-1])\n",
    "\n",
    "            if df_chunk_state.empty:\n",
    "                progress.update(raw_chunk_len)\n",
    "                progress.set_postfix_str(\"Skipped empty chunk\")\n",
    "                del df_chunk, df_chunk_state\n",
    "                gc.collect()\n",
    "                free_mem()\n",
    "                continue\n",
    "\n",
    "            rows_saved_this_chunk = len(df_chunk_state)\n",
    "            total_processed_rows += rows_saved_this_chunk\n",
    "\n",
    "            # --- SAVE CHUNK ---\n",
    "            if first_chunk:\n",
    "                df_chunk_state.to_parquet(\n",
    "                    parquet_feather_path, compression=\"zstd\", index=False\n",
    "                )\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                fastparquet.write(\n",
    "                    parquet_feather_path,\n",
    "                    df_chunk_state,\n",
    "                    compression=\"zstd\",\n",
    "                    write_index=False,\n",
    "                    append=True,\n",
    "                )\n",
    "\n",
    "            # --- METRICS ---\n",
    "            t1 = time.perf_counter()\n",
    "            duration = t1 - t0\n",
    "            rows_per_sec = rows_saved_this_chunk / duration if duration > 0 else 0\n",
    "            cpu_pct = psutil.cpu_percent(interval=None)\n",
    "            ram_mb = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "            # --- UPDATE PROGRESS ---\n",
    "            progress.update(raw_chunk_len)\n",
    "            progress.set_postfix(\n",
    "                saved=f\"{total_processed_rows:,}\",\n",
    "                cpu=f\"{cpu_pct:4.1f}%\",\n",
    "                ram=f\"{ram_mb:6.1f}MB\",\n",
    "                speed=f\"{rows_per_sec:8.1f} r/s\",\n",
    "            )\n",
    "\n",
    "            # --- FILE LOGGING ---\n",
    "            if log_file is not None:\n",
    "                with log_file.open(\"a\") as f:\n",
    "                    f.write(\n",
    "                        f\"{datetime.now().isoformat()},{chunk_index},\"\n",
    "                        f\"{rows_saved_this_chunk},{total_processed_rows},\"\n",
    "                        f\"{duration:.3f},{rows_per_sec:.1f},\"\n",
    "                        f\"{cpu_pct:.1f},{ram_mb:.1f}\\n\"\n",
    "                    )\n",
    "\n",
    "            # --- CLEANUP ---\n",
    "            del df_chunk, df_chunk_state\n",
    "            gc.collect()\n",
    "            free_mem()\n",
    "\n",
    "    finally:\n",
    "        if \"progress\" in locals():\n",
    "            progress.close()\n",
    "\n",
    "        logging.getLogger().setLevel(current_log_level)\n",
    "        conn.close()\n",
    "\n",
    "    logging.info(f\"âœ… Finished processing. Total rows saved: {total_processed_rows:,}\")\n",
    "    return total_processed_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 10:32:45 - INFO - ðŸ§  Preparing data stream...\n",
      "2025-12-03 10:32:45 - INFO - ðŸ“Š Total rows to process: 52,888,998\n",
      "2025-12-03 10:32:45 - INFO - ðŸ’¾ Output path: ../df_with_state.parquet\n",
      "Processing Data: 100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 52888998/52888998 [09:13<00:00, 95607.21row/s, cpu=19.7%, ram=1899.7MB, saved=52,888,998, speed=95625.4 r/s]  \n",
      "2025-12-03 10:41:58 - INFO - âœ… Finished processing. Total rows saved: 52,888,998\n",
      "2025-12-03 10:41:58 - INFO - ðŸŽ‰ Final DataFrame saved. Total rows processed: 52,888,998\n",
      "2025-12-03 10:41:58 - INFO - âœ… Data Processing (Stage 1) complete. Feather file ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "# --- ASSUMING run_stage1_data_setup WAS CALLED AND RETURNED utc_start, utc_end ---\n",
    "# Example configuration that needs to be available:\n",
    "# EXTRACTION_DIR = Path(\"../extracted_parts\") \n",
    "# CORE_COLS = [...]\n",
    "# CRITICAL FIX: Drastically reduced chunk size to prevent memory spike\n",
    "CHUNK_SIZE = 500 # Process 50,000 rows max at any time\n",
    "\n",
    "\n",
    "# 1. Setup DuckDB Query (as shown previously)\n",
    "conn, sql_query = setup_duckdb_query(EXTRACTION_DIR, utc_start, utc_end, CORE_COLS)\n",
    "\n",
    "# 2. Define Inputs\n",
    "# output_feather_file = \"df_with_state_30days.feather\"\n",
    "output_parquet_file = \"../df_with_state.parquet\"\n",
    "\n",
    "# 3. Run the memory-safe processing loop\n",
    "total_rows = process_and_save_data(\n",
    "    conn=conn,\n",
    "    sql_query=sql_query,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    parquet_feather_path=output_parquet_file,\n",
    "    vehicle_ids=vehicle_ids,\n",
    "    df_mapping=df_mapping,\n",
    "    extract_data=False\n",
    ")\n",
    "\n",
    "logging.info(f\"ðŸŽ‰ Final DataFrame saved. Total rows processed: {total_rows:,}\")\n",
    "logging.info(\"âœ… Data Processing (Stage 1) complete. Feather file ready for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d803154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_subset(parquet_path: str, start_dt: datetime, end_dt: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a subset of the processed Feather file using a date filter\n",
    "    applied directly by DuckDB (predicate pushdown).\n",
    "    \n",
    "    Args:\n",
    "        feather_path: Path to the processed Feather file.\n",
    "        start_dt: Start datetime for the filter (inclusive).\n",
    "        end_dt: End datetime for the filter (exclusive).\n",
    "        \n",
    "    Returns:\n",
    "        A new DataFrame containing only the filtered data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading data subset from {start_dt} to {end_dt}...\")\n",
    "    \n",
    "    # Use DuckDB to query the Feather file directly on disk\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    # The SQL query filters rows on the disk file based on the 'timestamp' column.\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM read_parquet('{parquet_path}')\n",
    "        WHERE \n",
    "            \"timestamp\" >= '{start_dt.isoformat()}' AND \n",
    "            \"timestamp\" < '{end_dt.isoformat()}'        \n",
    "    \"\"\"\n",
    "    # AND \"id\" IN ('16')\n",
    "\n",
    "    # Fetch the filtered, smaller DataFrame\n",
    "    df_subset = con.execute(sql_query).fetchdf()\n",
    "    con.close()\n",
    "    \n",
    "    logging.info(f\"âœ… Loaded {len(df_subset):,} rows for the requested subset.\")\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a52164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 10:41:58 - INFO - Loading data subset from 2025-09-01 00:00:00 to 2025-09-02 00:00:00...\n",
      "2025-12-03 10:41:58 - INFO - âœ… Loaded 155,729 rows for the requested subset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reg_num</th>\n",
       "      <th>customer</th>\n",
       "      <th>model</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date_val</th>\n",
       "      <th>dt_sec</th>\n",
       "      <th>alt_mode</th>\n",
       "      <th>ignitionstatus</th>\n",
       "      <th>vehiclereadycondition</th>\n",
       "      <th>gun_connection_status</th>\n",
       "      <th>vehicle_speed_vcu</th>\n",
       "      <th>gear_position</th>\n",
       "      <th>odometerreading</th>\n",
       "      <th>odometer_final</th>\n",
       "      <th>batt_maxtemp</th>\n",
       "      <th>batt_mintemp</th>\n",
       "      <th>batt_temp_delta</th>\n",
       "      <th>maxtemp_bucket</th>\n",
       "      <th>temp_delta_bucket</th>\n",
       "      <th>batt_maxvolt</th>\n",
       "      <th>batt_minvolt</th>\n",
       "      <th>volt_delta_mv</th>\n",
       "      <th>volt_delta_bucket</th>\n",
       "      <th>batt_maxtemp_tc</th>\n",
       "      <th>batt_mintemp_tc</th>\n",
       "      <th>batt_maxvolt_cell</th>\n",
       "      <th>batt_minvolt_cell</th>\n",
       "      <th>bat_voltage</th>\n",
       "      <th>total_battery_current</th>\n",
       "      <th>bat_soc</th>\n",
       "      <th>soc_band_bucket</th>\n",
       "      <th>soh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 05:30:00.937</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>DISCHARGING_IDLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;28</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.288</td>\n",
       "      <td>3.278</td>\n",
       "      <td>9.99999</td>\n",
       "      <td>0â€“10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>374</td>\n",
       "      <td>480</td>\n",
       "      <td>525.0</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 05:30:02.016</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>1.079</td>\n",
       "      <td>DISCHARGING_IDLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;28</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.288</td>\n",
       "      <td>3.278</td>\n",
       "      <td>9.99999</td>\n",
       "      <td>0â€“10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>374</td>\n",
       "      <td>480</td>\n",
       "      <td>525.0</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 05:30:03.116</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>1.100</td>\n",
       "      <td>DISCHARGING_IDLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;28</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.288</td>\n",
       "      <td>3.278</td>\n",
       "      <td>9.99999</td>\n",
       "      <td>0â€“10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>374</td>\n",
       "      <td>480</td>\n",
       "      <td>525.0</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 05:30:04.196</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>1.080</td>\n",
       "      <td>DISCHARGING_IDLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;28</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.288</td>\n",
       "      <td>3.278</td>\n",
       "      <td>9.99999</td>\n",
       "      <td>0â€“10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>374</td>\n",
       "      <td>480</td>\n",
       "      <td>525.0</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>HR55AY9237</td>\n",
       "      <td>ZB Gurgaon</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2025-09-01 05:30:05.256</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>1.060</td>\n",
       "      <td>DISCHARGING_IDLE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>8679.0</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;28</td>\n",
       "      <td>&lt;2</td>\n",
       "      <td>3.288</td>\n",
       "      <td>3.278</td>\n",
       "      <td>9.99999</td>\n",
       "      <td>0â€“10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>374</td>\n",
       "      <td>480</td>\n",
       "      <td>525.0</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>30-40</td>\n",
       "      <td>99.599998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     reg_num    customer model               timestamp   date_val  \\\n",
       "0  16  HR55AY9237  ZB Gurgaon  12.5 2025-09-01 05:30:00.937 2025-09-01   \n",
       "1  16  HR55AY9237  ZB Gurgaon  12.5 2025-09-01 05:30:02.016 2025-09-01   \n",
       "2  16  HR55AY9237  ZB Gurgaon  12.5 2025-09-01 05:30:03.116 2025-09-01   \n",
       "3  16  HR55AY9237  ZB Gurgaon  12.5 2025-09-01 05:30:04.196 2025-09-01   \n",
       "4  16  HR55AY9237  ZB Gurgaon  12.5 2025-09-01 05:30:05.256 2025-09-01   \n",
       "\n",
       "   dt_sec          alt_mode  ignitionstatus  vehiclereadycondition  \\\n",
       "0   0.000  DISCHARGING_IDLE               0                      0   \n",
       "1   1.079  DISCHARGING_IDLE               0                      0   \n",
       "2   1.100  DISCHARGING_IDLE               0                      0   \n",
       "3   1.080  DISCHARGING_IDLE               0                      0   \n",
       "4   1.060  DISCHARGING_IDLE               0                      0   \n",
       "\n",
       "   gun_connection_status  vehicle_speed_vcu  gear_position  odometerreading  \\\n",
       "0                      0                0.0              0           8679.0   \n",
       "1                      0                0.0              0           8679.0   \n",
       "2                      0                0.0              0           8679.0   \n",
       "3                      0                0.0              0           8679.0   \n",
       "4                      0                0.0              0           8679.0   \n",
       "\n",
       "   odometer_final  batt_maxtemp  batt_mintemp  batt_temp_delta maxtemp_bucket  \\\n",
       "0          8679.0            28            27                1            <28   \n",
       "1          8679.0            28            27                1            <28   \n",
       "2          8679.0            28            27                1            <28   \n",
       "3          8679.0            28            27                1            <28   \n",
       "4          8679.0            28            27                1            <28   \n",
       "\n",
       "  temp_delta_bucket  batt_maxvolt  batt_minvolt  volt_delta_mv  \\\n",
       "0                <2         3.288         3.278        9.99999   \n",
       "1                <2         3.288         3.278        9.99999   \n",
       "2                <2         3.288         3.278        9.99999   \n",
       "3                <2         3.288         3.278        9.99999   \n",
       "4                <2         3.288         3.278        9.99999   \n",
       "\n",
       "  volt_delta_bucket  batt_maxtemp_tc  batt_mintemp_tc  batt_maxvolt_cell  \\\n",
       "0              0â€“10                3                9                374   \n",
       "1              0â€“10                3                9                374   \n",
       "2              0â€“10                3                9                374   \n",
       "3              0â€“10                3                9                374   \n",
       "4              0â€“10                3                9                374   \n",
       "\n",
       "   batt_minvolt_cell  bat_voltage  total_battery_current    bat_soc  \\\n",
       "0                480        525.0              72.800003  33.200001   \n",
       "1                480        525.0              72.800003  33.200001   \n",
       "2                480        525.0              72.800003  33.200001   \n",
       "3                480        525.0              72.800003  33.200001   \n",
       "4                480        525.0              72.800003  33.200001   \n",
       "\n",
       "  soc_band_bucket        soh  \n",
       "0           30-40  99.599998  \n",
       "1           30-40  99.599998  \n",
       "2           30-40  99.599998  \n",
       "3           30-40  99.599998  \n",
       "4           30-40  99.599998  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Timestamp('2025-09-01 05:30:00.937000'),\n",
       " Timestamp('2025-09-01 23:59:59.826000'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the 30-day filter window\n",
    "filter_start_date = datetime(2025, 9, 1)\n",
    "filter_end_date = datetime(2025, 9, 2) # Exclusive end date\n",
    "\n",
    "# 1. Load the filtered subset safely\n",
    "df_subset = read_parquet_subset(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_dt=filter_start_date,\n",
    "    end_dt=filter_end_date\n",
    ")\n",
    "\n",
    "# # 2. Sort the data by vehicle ID and timestamp\n",
    "# # This is CRUCIAL for the cumulative maximum to work correctly for each vehicle.\n",
    "df_subset = df_subset.sort_values(by=['id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# # 3. Apply the cumulative maximum, GROUPED BY 'id'\n",
    "# # This enforces monotonicity (non-decreasing readings) for the odometer.\n",
    "df_subset['odometer_final'] = df_subset.groupby('id')['odometer_final'].cummax()\n",
    "\n",
    "display(df_subset.head())\n",
    "\n",
    "# 3. Aggressive memory cleanup after use (CRITICAL)\n",
    "# del df_subset\n",
    "# gc.collect()\n",
    "# free_mem()\n",
    "\n",
    "\n",
    "df_subset.timestamp.min(),df_subset.timestamp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24e9f13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    155729.000000\n",
       "mean         60.122437\n",
       "std          22.557264\n",
       "min          22.400000\n",
       "0.5%         22.799999\n",
       "1%           23.600000\n",
       "5%           30.400000\n",
       "10%          36.000000\n",
       "25%          42.400002\n",
       "50%          52.400002\n",
       "75%          83.199997\n",
       "90%          96.400002\n",
       "95%          98.800003\n",
       "99%          99.599998\n",
       "max         100.000000\n",
       "Name: bat_soc, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.bat_soc.describe(percentiles=[0.005,0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fbd9813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "reg_num                      0\n",
       "customer                     0\n",
       "model                        0\n",
       "timestamp                    0\n",
       "date_val                     0\n",
       "dt_sec                       0\n",
       "alt_mode                     0\n",
       "ignitionstatus               0\n",
       "vehiclereadycondition        0\n",
       "gun_connection_status        0\n",
       "vehicle_speed_vcu            0\n",
       "gear_position                0\n",
       "odometerreading          31288\n",
       "odometer_final               0\n",
       "batt_maxtemp                 0\n",
       "batt_mintemp                 0\n",
       "batt_temp_delta              0\n",
       "maxtemp_bucket               0\n",
       "temp_delta_bucket            0\n",
       "batt_maxvolt                 0\n",
       "batt_minvolt                 0\n",
       "volt_delta_mv                0\n",
       "volt_delta_bucket            0\n",
       "batt_maxtemp_tc              0\n",
       "batt_mintemp_tc              0\n",
       "batt_maxvolt_cell            0\n",
       "batt_minvolt_cell            0\n",
       "bat_voltage                  0\n",
       "total_battery_current        0\n",
       "bat_soc                      0\n",
       "soc_band_bucket              0\n",
       "soh                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114457db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[df_subset.id == '16'].odometer_final.diff().describe(percentiles=[0.001,0.01,0.25,0.5,0.75,0.9,0.95,0.99,0.9995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cccff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df16 = df_subset[df_subset.id == '16'].copy()\n",
    "df16 = df16.reset_index(drop=True)  # CRITICAL FIX\n",
    "\n",
    "# find raw negative diffs\n",
    "neg_idx = df16.index[df16[\"odometer_final\"].diff() >110]\n",
    "\n",
    "# Build Â±2 context window\n",
    "context_idx = (\n",
    "    set(neg_idx - 3)\n",
    "    | set(neg_idx - 2)\n",
    "    | set(neg_idx - 1)\n",
    "    | set(neg_idx)\n",
    "    | set(neg_idx + 1)\n",
    "    | set(neg_idx + 2)\n",
    ")\n",
    "\n",
    "# Keep only valid iloc rows\n",
    "context_idx = [i for i in context_idx if 0 <= i < len(df16)]\n",
    "\n",
    "# Show the window\n",
    "df16.iloc[context_idx][[\n",
    "    \"timestamp\",\n",
    "    \"odometerreading\",\n",
    "    \"odometer_final\",\n",
    "    \"vehicle_speed_vcu\",\n",
    "    \"dt_sec\"\n",
    "]].sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3de2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df16.iloc[context_idx][[\n",
    "    \"timestamp\",\n",
    "    \"odometerreading\",\n",
    "    \"odometer_final\",\n",
    "    \"vehicle_speed_vcu\",\n",
    "    \"dt_sec\"\n",
    "]].sort_index().to_csv(\"df16_negative_odo_context.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.alt_mode.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9738e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_subset['timestamp'] = pd.to_datetime(df_subset['timestamp'], utc=True)\n",
    "# df_subset = df_subset.sort_values(by=['id', 'timestamp']).reset_index(drop=True)\n",
    "# df_subset['odometer_final'] = df_subset.groupby('id')['odometer_final'].cummax()\n",
    "# df_subset.volt_delta_mv.describe(percentiles=[0.9,0.95,0.9992])\n",
    "# df_subset[df_subset.batt_mintemp>-40].batt_mintemp.describe()\n",
    "# len(df_subset[(df_subset.odometer_final.isnull())&(df_subset.vehicle_speed_vcu>1)])\n",
    "# len(df_subset[df_subset.dt_sec==0])\n",
    "# df_subset[(df_subset.odometer_final.isnull())]#.vehicle_speed_vcu.describe()\n",
    "# df_subset[['timestamp','vehicle_speed_vcu','gear_position','odometerreading','odometer_final']].isnull().sum()*100.0/len(df_subset)\n",
    "# missing = df_subset[df_subset.odometer_final.isna()]\n",
    "# print(\"Total missing:\", len(missing))\n",
    "# print(missing[['timestamp','vehicle_speed_vcu','dt_sec','odometerreading']].head(20))\n",
    "# print(\"Speed null %:\", missing.vehicle_speed_vcu.isna().mean()*100)\n",
    "# print(\"dt null %:\", missing.dt_sec.isna().mean()*100)\n",
    "# df_subset[(df_subset.vehiclereadycondition == 0)&(df_subset.gun_connection_status == 1)]\n",
    "# df_subset[(df_subset.alt_mode=='CHARGING')&(df_subset.total_battery_current<-5)].total_battery_current.describe(percentiles=[0.7,0.8,0.9,0.95,0.98,0.99,0.999])\n",
    "# df_subset.groupby(['id','date_val','alt_mode'])['alt_mode'].count()\n",
    "# display(df_subset[(df_subset.mode=='CHARGING')&(df_subset.vehiclereadycondition=='1')&(df_subset.gun_connection_status=='1')].head(100))\n",
    "# df_subset.volt_delta_mv.max()\n",
    "# df_subset.total_battery_current[df_subset.total_battery_current>-3200].min()\n",
    "# pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "# pd.reset_option('display.float_format')\n",
    "# df_subset.head(1000).to_csv('data_ref.csv')\n",
    "# df_subset[df_subset.dt_sec>1].dt_sec.describe(percentiles=[0.9,0.95,0.99,0.995,0.997]).round(2)\n",
    "# df_subset.groupby(['id','date_val'])['dt_sec'].sum()/60.0\n",
    "\n",
    "# # chk = ((df_subset.groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()*100.0/60.0)/(df_subset.groupby(['date_val'])['dt_sec'].sum()/60.0)).sort_values()\n",
    "# chk = (\n",
    "#         (df_subset[df_subset['mode'] == 'CHARGING'].groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()*100.0/60.0) / \n",
    "#         (df_subset[df_subset['mode'] == 'CHARGING'].groupby(['date_val'])['dt_sec'].sum()/60.0)\n",
    "#       ).sort_index(level='batt_maxtemp_tc')\n",
    "# display(chk)\n",
    "\n",
    "\n",
    "# # chk = ((df_subset.groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()*100.0/60.0)/(df_subset.groupby(['date_val'])['dt_sec'].sum()/60.0)).sort_values()\n",
    "# chk2 = (\n",
    "#         (df_subset[df_subset['mode'] == 'CHARGING'].groupby(['date_val','mode','batt_maxtemp_tc','pack_id_max'])['dt_sec'].sum()/60.0)).sort_index(level='batt_maxtemp_tc')\n",
    "# display(chk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16711d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How close two charging blocks can be and still count as one session\n",
    "CHARGING_GAP_MERGE_MIN = 15.0   # minutes\n",
    "SPEED_MOTION_THRESHOLD = 0.5   # km/h, for motion_pct\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _get_charging_mask(df_vid: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns a boolean Series marking rows that belong to a *charging envelope*.\n",
    "\n",
    "    Priority:\n",
    "      1) If 'alt_mode' exists:\n",
    "           any alt_mode starting with 'CHARGING'\n",
    "           (CHARGING_ACTIVE / CHARGING_MAINTAIN / CHARGING_IDLE)\n",
    "           is treated as charging.\n",
    "      2) Else if 'mode' exists:\n",
    "           mode == 'CHARGING'.\n",
    "      3) Else: fall back to gun_connection_status / current-based heuristic.\n",
    "    \"\"\"\n",
    "    df_vid = df_vid.copy()\n",
    "\n",
    "    # --- Preferred: multi-state alt_mode ---\n",
    "    if \"alt_mode\" in df_vid.columns:\n",
    "        return df_vid[\"alt_mode\"].astype(str).str.startswith(\"CHARGING\")\n",
    "\n",
    "    # --- Fallback: simple mode ---\n",
    "    # if \"mode\" in df_vid.columns:\n",
    "    #     return df_vid[\"mode\"].astype(str).str.upper() == \"CHARGING\"\n",
    "\n",
    "    # --- Last resort: gun + current heuristic ---\n",
    "    gun = pd.Series(False, index=df_vid.index)\n",
    "    if \"gun_connection_status\" in df_vid.columns:\n",
    "        g = df_vid[\"gun_connection_status\"]\n",
    "        g_num = pd.to_numeric(g, errors=\"coerce\")\n",
    "        g_str = g.astype(str).str.strip().str.lower()\n",
    "        gun = (g_num == 1) | g_str.isin({\"1\", \"true\", \"yes\", \"y\", \"connected\", \"on\"})\n",
    "        gun = gun.fillna(False)\n",
    "\n",
    "    if \"total_battery_current\" in df_vid.columns:\n",
    "        cur = pd.to_numeric(df_vid[\"total_battery_current\"], errors=\"coerce\")\n",
    "        cur_cond = (cur < -5).fillna(False)\n",
    "    else:\n",
    "        cur_cond = pd.Series(False, index=df_vid.index)\n",
    "\n",
    "    return gun | cur_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_charging_sessions_for_vehicle(df_vid: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Builds stitched CHARGING envelopes for a single vehicle.\n",
    "\n",
    "    A charging envelope is any contiguous region where _get_charging_mask() is True,\n",
    "    with gaps â‰¤ CHARGING_GAP_MERGE_MIN minutes merged into a single session.\n",
    "\n",
    "    Returns a list of dicts:\n",
    "        { 'start_idx', 'end_idx', 'start_time', 'end_time' }\n",
    "    \"\"\"\n",
    "    df_vid = df_vid.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    is_chg = _get_charging_mask(df_vid)\n",
    "\n",
    "    if not is_chg.any():\n",
    "        return []\n",
    "\n",
    "    tmp = df_vid.copy()\n",
    "    tmp[\"is_chg\"] = is_chg\n",
    "    tmp[\"chg_block\"] = tmp[\"is_chg\"].ne(tmp[\"is_chg\"].shift()).cumsum()\n",
    "\n",
    "    raw_blocks: list[dict] = []\n",
    "    for block_id, g in tmp.groupby(\"chg_block\", sort=True):\n",
    "        # skip non-charging blocks\n",
    "        if not g[\"is_chg\"].iloc[0]:\n",
    "            continue\n",
    "\n",
    "        start_idx = int(g.index[0])\n",
    "        end_idx   = int(g.index[-1])\n",
    "\n",
    "        raw_blocks.append(\n",
    "            {\n",
    "                \"start_idx\": start_idx,\n",
    "                \"end_idx\": end_idx,\n",
    "                \"start_time\": df_vid.loc[start_idx, \"timestamp\"],\n",
    "                \"end_time\":   df_vid.loc[end_idx,   \"timestamp\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not raw_blocks:\n",
    "        return []\n",
    "\n",
    "    raw_blocks.sort(key=lambda b: b[\"start_idx\"])\n",
    "\n",
    "    stitched: list[dict] = []\n",
    "    current = raw_blocks[0].copy()\n",
    "\n",
    "    for nxt in raw_blocks[1:]:\n",
    "        gap_min = (nxt[\"start_time\"] - current[\"end_time\"]).total_seconds() / 60.0\n",
    "        if gap_min <= CHARGING_GAP_MERGE_MIN:\n",
    "            # merge into current envelope\n",
    "            current[\"end_idx\"] = nxt[\"end_idx\"]\n",
    "            current[\"end_time\"] = nxt[\"end_time\"]\n",
    "        else:\n",
    "            stitched.append(current)\n",
    "            current = nxt.copy()\n",
    "\n",
    "    stitched.append(current)\n",
    "    return stitched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_session_metrics(seg: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Given a contiguous segment (one session) for a single vehicle,\n",
    "    compute all requested metrics.\n",
    "    \"\"\"\n",
    "    seg = seg.copy()\n",
    "    seg = seg.sort_values(\"timestamp\")\n",
    "\n",
    "    if len(seg) < 2:\n",
    "        return None\n",
    "\n",
    "    # --- time deltas ---\n",
    "    seg[\"dt\"] = seg[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    # For safety: zero-out first dt so we don't pull in time before the session\n",
    "    seg.iloc[0, seg.columns.get_loc(\"dt\")] = 0.0\n",
    "\n",
    "    total_time = seg[\"dt\"].sum()\n",
    "    if total_time <= 0:\n",
    "        total_time = (seg[\"timestamp\"].iloc[-1] - seg[\"timestamp\"].iloc[0]).total_seconds()\n",
    "\n",
    "    # --- base metadata ---\n",
    "    id_val = seg[\"id\"].iloc[0]\n",
    "    reg_num = seg[\"reg_num\"].dropna().iloc[0] if \"reg_num\" in seg.columns and seg[\"reg_num\"].notna().any() else None\n",
    "    customer = seg[\"customer\"].dropna().iloc[0] if \"customer\" in seg.columns and seg[\"customer\"].notna().any() else None\n",
    "    model = seg[\"model\"].dropna().iloc[0] if \"model\" in seg.columns and seg[\"model\"].notna().any() else None\n",
    "\n",
    "    start_time = seg[\"timestamp\"].iloc[0]\n",
    "    end_time = seg[\"timestamp\"].iloc[-1]\n",
    "    duration_mins = round((end_time - start_time).total_seconds() / 60.0, 2)\n",
    "\n",
    "    # --- energy integration ---\n",
    "    # Ensure numeric\n",
    "    seg[\"bat_voltage\"] = pd.to_numeric(seg.get(\"bat_voltage\"), errors=\"coerce\")\n",
    "    seg[\"total_battery_current\"] = pd.to_numeric(seg.get(\"total_battery_current\"), errors=\"coerce\")\n",
    "\n",
    "    # kW\n",
    "    seg[\"power_kw\"] = (seg[\"bat_voltage\"] * seg[\"total_battery_current\"]) / 1000.0\n",
    "\n",
    "    # kWh components (sign-aware)\n",
    "    # charging (I < 0) â†’ energy INTO pack\n",
    "    mask_chg = seg[\"total_battery_current\"] < 0\n",
    "    mask_dis = seg[\"total_battery_current\"] > 0\n",
    "\n",
    "    seg[\"energy_kwh_chg\"] = 0.0\n",
    "    seg.loc[mask_chg, \"energy_kwh_chg\"] = -seg.loc[mask_chg, \"power_kw\"] * seg.loc[mask_chg, \"dt\"] / 3600.0\n",
    "\n",
    "    seg[\"energy_kwh_dis\"] = 0.0\n",
    "    seg.loc[mask_dis, \"energy_kwh_dis\"] = seg.loc[mask_dis, \"power_kw\"] * seg.loc[mask_dis, \"dt\"] / 3600.0\n",
    "\n",
    "    kwh_charging = round(seg[\"energy_kwh_chg\"].sum(), 2)\n",
    "    kwh_discharging = round(seg[\"energy_kwh_dis\"].sum(), 2)\n",
    "\n",
    "    # --- SOC metrics ---\n",
    "    soc_col = \"bat_soc\" if \"bat_soc\" in seg.columns else None\n",
    "    soc_start = soc_end = soc_gain = soc_drop = None\n",
    "    if soc_col:\n",
    "        soc_valid = seg[soc_col].dropna()\n",
    "        if not soc_valid.empty:\n",
    "            soc_start = soc_valid.iloc[0]\n",
    "            soc_end = soc_valid.iloc[-1]\n",
    "            soc_gain = max(soc_end - soc_start, 0)\n",
    "            soc_drop = max(soc_start - soc_end, 0)\n",
    "\n",
    "    # --- percentage metrics (time-weighted) ---\n",
    "    def pct(mask: pd.Series) -> float:\n",
    "        if total_time <= 0:\n",
    "            return 0.0\n",
    "        return round(100.0 * seg.loc[mask, \"dt\"].sum() / total_time, 2)\n",
    "\n",
    "    # charging / discharging % by current sign\n",
    "    charging_pct = pct(mask_chg)\n",
    "    discharging_pct = pct(mask_dis)\n",
    "\n",
    "    # motion_pct: vehicle_speed_vcu > SPEED_MOTION_THRESHOLD\n",
    "    if \"vehicle_speed_vcu\" in seg.columns:\n",
    "        speed = pd.to_numeric(seg[\"vehicle_speed_vcu\"], errors=\"coerce\")\n",
    "        motion_pct = pct(speed > SPEED_MOTION_THRESHOLD)\n",
    "    else:\n",
    "        motion_pct = np.nan\n",
    "\n",
    "    # lv_pct and off_pct\n",
    "    if all(col in seg.columns for col in [\"ignitionstatus\", \"gun_connection_status\", \"vehiclereadycondition\"]):\n",
    "        ign = pd.to_numeric(seg[\"ignitionstatus\"], errors=\"coerce\")\n",
    "        gun = pd.to_numeric(seg[\"gun_connection_status\"], errors=\"coerce\")\n",
    "        ready = pd.to_numeric(seg[\"vehiclereadycondition\"], errors=\"coerce\")\n",
    "\n",
    "        lv_mask = (ign == 1) & (gun == 0) & (ready == 0)\n",
    "        off_mask = (ign == 0) & (gun == 0) & (ready == 0)\n",
    "\n",
    "        lv_pct = pct(lv_mask)\n",
    "        off_pct = pct(off_mask)\n",
    "    else:\n",
    "        lv_pct = off_pct = np.nan\n",
    "\n",
    "    return {\n",
    "        \"id\": id_val,\n",
    "        \"reg_num\": reg_num,\n",
    "        \"customer\": customer,\n",
    "        \"model\": model,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"duration_mins\": duration_mins,\n",
    "        \"kwh_charging\": kwh_charging,\n",
    "        \"kwh_discharging\": kwh_discharging,\n",
    "        \"soc_start\": soc_start,\n",
    "        \"soc_end\": soc_end,\n",
    "        \"soc_gain\": soc_gain,\n",
    "        \"soc_drop\": soc_drop,\n",
    "        \"charging_pct\": charging_pct,\n",
    "        \"discharging_pct\": discharging_pct,\n",
    "        \"motion_pct\": motion_pct,\n",
    "        \"lv_pct\": lv_pct,\n",
    "        \"off_pct\": off_pct,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_sessions_for_vehicle(df_vid: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"\n",
    "    For a single vehicle, build sessions around stitched charging envelopes\n",
    "    and compute metrics for each slice.\n",
    "\n",
    "    Session boundaries:\n",
    "      - Charging envelopes from _build_charging_sessions_for_vehicle()\n",
    "      - Gaps between them\n",
    "      - Pre-first and post-last intervals\n",
    "\n",
    "    Session label:\n",
    "      - 'activity' is derived from the dominant `alt_mode` inside the slice:\n",
    "            CHARGING_ACTIVE / CHARGING_MAINTAIN / CHARGING_IDLE\n",
    "            DISCHARGING_ACTIVE / DISCHARGING_IDLE\n",
    "        falling back to simple CHARGING/DISCHARGING/UNKNOWN if needed.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "    if df_vid.empty:\n",
    "        return rows\n",
    "\n",
    "    df_vid = df_vid.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    n = len(df_vid)\n",
    "\n",
    "    charging_sessions = _build_charging_sessions_for_vehicle(df_vid)\n",
    "\n",
    "    BATT_KWH = 423.96  # for C-rate\n",
    "\n",
    "    def add_session(start_idx: int, end_idx: int):\n",
    "        \"\"\"Slice [start_idx, end_idx] â†’ metrics + activity.\"\"\"\n",
    "        if start_idx > end_idx or start_idx < 0 or end_idx >= n:\n",
    "            return\n",
    "\n",
    "        seg = df_vid.iloc[start_idx:end_idx + 1].copy()\n",
    "        if seg.empty:\n",
    "            return\n",
    "\n",
    "        metrics = _compute_session_metrics(seg)\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        # --- derive activity from alt_mode ---\n",
    "        activity = None\n",
    "        if \"alt_mode\" in seg.columns:\n",
    "            am = seg[\"alt_mode\"].dropna().astype(str)\n",
    "            if not am.empty:\n",
    "                activity = am.value_counts().idxmax()\n",
    "\n",
    "        if activity is None:\n",
    "            # fallback: sign of current\n",
    "            if \"total_battery_current\" in seg.columns:\n",
    "                cur = pd.to_numeric(seg[\"total_battery_current\"], errors=\"coerce\")\n",
    "                if cur.mean(skipna=True) < 0:\n",
    "                    activity = \"CHARGING\"\n",
    "                else:\n",
    "                    activity = \"DISCHARGING\"\n",
    "            else:\n",
    "                activity = \"UNKNOWN\"\n",
    "\n",
    "        # --- C-rate style metrics (same as earlier) ---\n",
    "        charge_rate = 0.0\n",
    "        discharge_rate = 0.0\n",
    "\n",
    "        if \"bat_voltage\" in seg.columns and \"total_battery_current\" in seg.columns:\n",
    "            seg[\"power_kw\"] = (seg[\"bat_voltage\"] * seg[\"total_battery_current\"]) / 1000.0\n",
    "\n",
    "            chg = seg.loc[seg[\"power_kw\"] < 0, \"power_kw\"]\n",
    "            if not chg.empty:\n",
    "                avg_chg_kw = abs(chg.mean())\n",
    "                charge_rate = avg_chg_kw / BATT_KWH\n",
    "\n",
    "            dch = seg.loc[seg[\"power_kw\"] > 0, \"power_kw\"]\n",
    "            if not dch.empty:\n",
    "                avg_dch_kw = dch.mean()\n",
    "                discharge_rate = avg_dch_kw / BATT_KWH\n",
    "\n",
    "        metrics[\"charge_rate\"] = round(charge_rate, 3) if charge_rate is not None else None\n",
    "        metrics[\"discharge_rate\"] = round(discharge_rate, 3) if discharge_rate is not None else None\n",
    "\n",
    "        metrics[\"activity\"] = activity\n",
    "        rows.append(metrics)\n",
    "\n",
    "    # --------------------------\n",
    "    # build segments around envelopes\n",
    "    # --------------------------\n",
    "    if not charging_sessions:\n",
    "        # whole day is a single discharging-family session\n",
    "        add_session(0, n - 1)\n",
    "        return rows\n",
    "\n",
    "    # pre-first-charging\n",
    "    first = charging_sessions[0]\n",
    "    if first[\"start_idx\"] > 0:\n",
    "        add_session(0, first[\"start_idx\"] - 1)\n",
    "\n",
    "    # each charging envelope + gap to next\n",
    "    for i, chg in enumerate(charging_sessions):\n",
    "        # charging region itself\n",
    "        add_session(chg[\"start_idx\"], chg[\"end_idx\"])\n",
    "\n",
    "        # gap (discharging region) to next envelope\n",
    "        if i < len(charging_sessions) - 1:\n",
    "            nxt = charging_sessions[i + 1]\n",
    "            gap_start = chg[\"end_idx\"] + 1\n",
    "            gap_end = nxt[\"start_idx\"] - 1\n",
    "            if gap_start <= gap_end:\n",
    "                add_session(gap_start, gap_end)\n",
    "\n",
    "    # post-last-charging\n",
    "    last = charging_sessions[-1]\n",
    "    if last[\"end_idx\"] < n - 1:\n",
    "        add_session(last[\"end_idx\"] + 1, n - 1)\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_discharging_metrics(\n",
    "    session_df: pd.DataFrame,\n",
    "    raw_df: pd.DataFrame,\n",
    "    max_kmph_for_physics: float = 120.0,\n",
    "    physics_tolerance: float = 1.3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Option 3: physics-aware, GLITCH-capable enrichment.\n",
    "\n",
    "    Adds per-session:\n",
    "      - dist_km (final, physics-sanitised)\n",
    "      - avg_speed, med_speed, max_speed\n",
    "      - avg/med/max/p95 volt_delta_mv\n",
    "      - avg/med/max/p95 batt_temp_delta\n",
    "      - energy_active_kwh, kwh_per_km\n",
    "      - odo_start, odo_end, net_odo_km\n",
    "      - dist_km_raw (pre-physics cumulative diffs)\n",
    "      - max_physical_km\n",
    "      - glitch_flag (bool) + glitch_reason (text)\n",
    "\n",
    "    If session_df has an 'activity' column, GLITCH sessions get\n",
    "    activity=\"GLITCH\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure time alignment\n",
    "    raw_df = raw_df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # --- Holder columns (base metrics) ---\n",
    "    session_df[\"dist_km\"] = 0.0\n",
    "    session_df[\"avg_speed\"] = 0.0\n",
    "    session_df[\"med_speed\"] = 0.0\n",
    "    session_df[\"max_speed\"] = 0.0\n",
    "\n",
    "    session_df[\"avg_volt_delta_mv\"] = 0.0\n",
    "    session_df[\"med_volt_delta_mv\"] = 0.0\n",
    "    session_df[\"max_volt_delta_mv\"] = 0.0\n",
    "    session_df[\"p95_volt_delta_mv\"] = 0.0\n",
    "\n",
    "    session_df[\"avg_batt_temp_delta\"] = 0.0\n",
    "    session_df[\"med_batt_temp_delta\"] = 0.0\n",
    "    session_df[\"max_batt_temp_delta\"] = 0.0\n",
    "    session_df[\"p95_batt_temp_delta\"] = 0.0\n",
    "\n",
    "    session_df[\"energy_active_kwh\"] = 0.0\n",
    "    session_df[\"kwh_per_km\"] = np.nan\n",
    "\n",
    "    # --- NEW diagnostic / physics fields ---\n",
    "    session_df[\"odo_start\"] = np.nan\n",
    "    session_df[\"odo_end\"]   = np.nan\n",
    "    session_df[\"net_odo_km\"] = 0.0      # odo_end - odo_start (clamped â‰¥ 0)\n",
    "    session_df[\"dist_km_raw\"] = 0.0     # sum of positive diffs before physics clamp\n",
    "    session_df[\"max_physical_km\"] = 0.0\n",
    "\n",
    "    session_df[\"glitch_flag\"] = False\n",
    "    session_df[\"glitch_reason\"] = \"\"\n",
    "\n",
    "    has_activity_col = \"activity\" in session_df.columns\n",
    "\n",
    "    # --- Loop through each session ---\n",
    "    for idx, row in session_df.iterrows():\n",
    "        vid = row[\"id\"]\n",
    "        t1  = row[\"start_time\"]\n",
    "        t2  = row[\"end_time\"]\n",
    "\n",
    "        # Keep your original intent: only DISCHARGING_ACTIVE are \"drive\" sessions\n",
    "        if has_activity_col and row[\"activity\"] != \"DISCHARGING_ACTIVE\":\n",
    "            continue\n",
    "\n",
    "        mask = (\n",
    "            (raw_df[\"id\"] == vid) &\n",
    "            (raw_df[\"timestamp\"] >= t1) &\n",
    "            (raw_df[\"timestamp\"] <= t2)\n",
    "        )\n",
    "        chunk = raw_df[mask].copy()\n",
    "\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # -------------------------------\n",
    "        # 0. Choose odometer source\n",
    "        # -------------------------------\n",
    "        if \"odometer_final\" in chunk.columns:\n",
    "            odo_series = chunk[\"odometer_final\"].astype(\"float64\")\n",
    "        # else:\n",
    "        #     odo_series = chunk[\"odometerreading\"].astype(\"float64\")\n",
    "\n",
    "        odo_series = odo_series.dropna()\n",
    "        if odo_series.empty:\n",
    "            # No odo â†’ skip distance & energy, but still do volt/temp/speed\n",
    "            odo_start = np.nan\n",
    "            odo_end = np.nan\n",
    "            net_odo = 0.0\n",
    "        else:\n",
    "            odo_start = float(odo_series.iloc[0])\n",
    "            odo_end   = float(odo_series.iloc[-1])\n",
    "            net_odo   = max(odo_end - odo_start, 0.0)\n",
    "\n",
    "        session_df.at[idx, \"odo_start\"] = odo_start\n",
    "        session_df.at[idx, \"odo_end\"]   = odo_end\n",
    "        session_df.at[idx, \"net_odo_km\"] = net_odo\n",
    "\n",
    "        # -------------------------------\n",
    "        # 1. Distance via forward-only diffs (raw)\n",
    "        # -------------------------------\n",
    "        if \"odometer_final\" in chunk.columns:\n",
    "            odo_full = chunk[\"odometer_final\"].astype(\"float64\")\n",
    "        # else:\n",
    "        #     odo_full = chunk[\"odometerreading\"].astype(\"float64\")\n",
    "\n",
    "        odo_diff = odo_full.diff()\n",
    "        # Keep only strictly positive increments\n",
    "        dist_km_raw = odo_diff[odo_diff > 0].sum(skipna=True)\n",
    "        if pd.isna(dist_km_raw):\n",
    "            dist_km_raw = 0.0\n",
    "        session_df.at[idx, \"dist_km_raw\"] = float(dist_km_raw)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 2. Speed stats\n",
    "        # -------------------------------\n",
    "        v = chunk[\"vehicle_speed_vcu\"].dropna()\n",
    "        if not v.empty:\n",
    "            avg_speed = float(v.mean())\n",
    "            med_speed = float(v.median())\n",
    "            max_speed = float(v.max())\n",
    "        else:\n",
    "            avg_speed = med_speed = max_speed = 0.0\n",
    "\n",
    "        session_df.at[idx, \"avg_speed\"] = round(avg_speed, 2)\n",
    "        session_df.at[idx, \"med_speed\"] = round(med_speed,2)\n",
    "        session_df.at[idx, \"max_speed\"] = round(max_speed, 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3. Voltage delta stats\n",
    "        # -------------------------------\n",
    "        vd = chunk[\"volt_delta_mv\"].dropna()\n",
    "        if not vd.empty:\n",
    "            session_df.at[idx, \"avg_volt_delta_mv\"] = round(float(vd.mean()), 2)\n",
    "            session_df.at[idx, \"med_volt_delta_mv\"] = round(float(vd.median()), 2)\n",
    "            session_df.at[idx, \"max_volt_delta_mv\"] = round(float(vd.max()),2)\n",
    "            session_df.at[idx, \"p95_volt_delta_mv\"] = round(float(vd.quantile(0.95)), 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 4. Temperature delta stats\n",
    "        # -------------------------------\n",
    "        td = chunk[\"batt_temp_delta\"].dropna()\n",
    "        if not td.empty:\n",
    "            session_df.at[idx, \"avg_batt_temp_delta\"] = round(float(td.mean()),2)\n",
    "            session_df.at[idx, \"med_batt_temp_delta\"] = round(float(td.median()),2)\n",
    "            session_df.at[idx, \"max_batt_temp_delta\"] = round(float(td.max()),2)\n",
    "            session_df.at[idx, \"p95_batt_temp_delta\"] = round(float(td.quantile(0.95)), 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 5. Energy integration (kWh)\n",
    "        # -------------------------------\n",
    "        # Power (kW) = V * I / 1000\n",
    "        # Energy (kWh) = Î£ power * (dt_sec / 3600)\n",
    "        chunk[\"power_kw\"] = round((\n",
    "            chunk[\"bat_voltage\"].astype(\"float64\") *\n",
    "            chunk[\"total_battery_current\"].astype(\"float64\")\n",
    "        ) / 1000.0, 2)\n",
    "\n",
    "        chunk[\"energy_kwh\"] = round(chunk[\"power_kw\"] * (\n",
    "            chunk[\"dt_sec\"].astype(\"float64\") / 3600.0\n",
    "        ), 2)\n",
    "\n",
    "        energy_active_kwh = chunk.loc[chunk[\"energy_kwh\"] > 0, \"energy_kwh\"].sum()\n",
    "        if pd.isna(energy_active_kwh):\n",
    "            energy_active_kwh = 0.0\n",
    "\n",
    "        session_df.at[idx, \"energy_active_kwh\"] = round(float(energy_active_kwh), 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 6. Physics model: max possible km\n",
    "        # -------------------------------\n",
    "        ts_min = chunk[\"timestamp\"].min()\n",
    "        ts_max = chunk[\"timestamp\"].max()\n",
    "        if pd.isna(ts_min) or pd.isna(ts_max):\n",
    "            duration_hr = 0.0\n",
    "        else:\n",
    "            duration_sec = (ts_max - ts_min).total_seconds()\n",
    "            duration_hr = max(duration_sec / 3600.0, 0.0)\n",
    "\n",
    "        # cap median speed by max_kmph_for_physics\n",
    "        eff_avg_speed = min((max_speed+med_speed)/2, max_kmph_for_physics)\n",
    "        max_physical_km = eff_avg_speed * duration_hr * physics_tolerance\n",
    "\n",
    "        session_df.at[idx, \"max_physical_km\"] = round(float(max_physical_km), 3)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 7. GLITCH detection (Option 3)\n",
    "        # -------------------------------\n",
    "        glitch_flag = False\n",
    "        reasons = []\n",
    "\n",
    "        eps = 1e-6\n",
    "\n",
    "        # A: net odometer itself exceeds physics limit\n",
    "        if net_odo > max_physical_km + eps:\n",
    "            glitch_flag = True\n",
    "            reasons.append(\n",
    "                f\"net_odo {net_odo:.3f}km > max_phys {max_physical_km:.3f}km\"\n",
    "            )\n",
    "\n",
    "        # B: raw cumulative distance exceeds physics limit dramatically\n",
    "        if dist_km_raw > max_physical_km + eps:\n",
    "            glitch_flag = True\n",
    "            reasons.append(\n",
    "                f\"dist_km_raw {dist_km_raw:.3f}km > max_phys {max_physical_km:.3f}km\"\n",
    "            )\n",
    "\n",
    "        # C: backward odometer (shouldn't happen after your finaliser, but guard anyway)\n",
    "        if odo_end is not np.nan and odo_start is not np.nan and odo_end + eps < odo_start:\n",
    "            glitch_flag = True\n",
    "            reasons.append(\n",
    "                f\"odo_end {odo_end:.3f} < odo_start {odo_start:.3f}\"\n",
    "            )\n",
    "\n",
    "        # -------------------------------\n",
    "        # 8. Final distance selection\n",
    "        # -------------------------------\n",
    "        # Start with raw cumulative\n",
    "        dist_final = float(dist_km_raw)\n",
    "\n",
    "        # If raw cumulative is significantly higher than net change, it's jitter\n",
    "        if net_odo > 0 and dist_final > net_odo * physics_tolerance:\n",
    "            reasons.append(\n",
    "                f\"dist_km_raw {dist_final:.3f}km >> net_odo {net_odo:.3f}km, using net_odo\"\n",
    "            )\n",
    "            dist_final = float(net_odo)\n",
    "\n",
    "        # If GLITCH due to physics but net_odo is still sane, keep net_odo as best guess\n",
    "        if glitch_flag:\n",
    "            if net_odo <= max_physical_km + eps:\n",
    "                dist_final = float(net_odo)\n",
    "            else:\n",
    "                # Completely impossible â†’ distance is untrustworthy\n",
    "                dist_final = 0.0\n",
    "\n",
    "        session_df.at[idx, \"dist_km\"] = dist_final\n",
    "\n",
    "        # -------------------------------\n",
    "        # 9. kWh/km using final distance\n",
    "        # -------------------------------\n",
    "        if dist_final > 0:\n",
    "            session_df.at[idx, \"kwh_per_km\"] = round(float(energy_active_kwh / dist_final),2)\n",
    "        else:\n",
    "            session_df.at[idx, \"kwh_per_km\"] = np.nan\n",
    "\n",
    "        # -------------------------------\n",
    "        # 10. Persist GLITCH info\n",
    "        # -------------------------------\n",
    "        if glitch_flag:\n",
    "            session_df.at[idx, \"glitch_flag\"] = True\n",
    "            session_df.at[idx, \"glitch_reason\"] = \"; \".join(reasons)\n",
    "            if has_activity_col:\n",
    "                session_df.at[idx, \"activity\"] = \"GLITCH\"\n",
    "\n",
    "    return session_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b03275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_charging_and_discharging_sessions(df_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build unified CHARGING / DISCHARGING sessions for a day's data.\n",
    "    Also adds bucket distributions and SOC stats.\n",
    "\n",
    "    Output columns:\n",
    "        id, reg_num, customer, model, activity, session, date,\n",
    "        start_time, end_time, duration_mins,\n",
    "        charging_pct, discharging_pct, motion_pct,\n",
    "        lv_pct, off_pct,\n",
    "        kwh_charging, kwh_discharging,\n",
    "        soc_start, soc_end, soc_gain, soc_drop,\n",
    "        ... + bucket percentage columns\n",
    "    \"\"\"\n",
    "\n",
    "    if df_day.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_day = df_day.sort_values([\"id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 1 â€” Build sessions for each vehicle\n",
    "    # -----------------------------------------------------------\n",
    "    all_rows = []\n",
    "    for vid, df_vid in df_day.groupby(\"id\"):\n",
    "        vid_rows = _build_sessions_for_vehicle(df_vid)   # <-- your existing function\n",
    "        all_rows.extend(vid_rows)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert list of dicts into DataFrame\n",
    "    sessions = pd.DataFrame(all_rows)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 2 â€” Sort & assign per-vehicle session number\n",
    "    # -----------------------------------------------------------\n",
    "    sessions = sessions.sort_values([\"id\", \"start_time\"]).reset_index(drop=True)\n",
    "    sessions[\"session\"] = sessions.groupby(\"id\").cumcount() + 1\n",
    "    sessions[\"date\"] = sessions[\"start_time\"].dt.date\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # NEW STEP 2B â€” SOC DROP GLITCH DETECTION\n",
    "    # -----------------------------------------------------------\n",
    "    THRESH_ACTIVE = 0.0    # no drop allowed during active charging\n",
    "    THRESH_MAINT  = 0.3    # small balancing jitter allowed\n",
    "    THRESH_IDLE   = 0.5    # small taper jitter allowed\n",
    "\n",
    "    if \"activity\" in sessions.columns:\n",
    "        sessions[\"glitch_flag\"] = False\n",
    "        sessions[\"glitch_reason\"] = \"\"\n",
    "\n",
    "        for idx, row in sessions.iterrows():\n",
    "            mode = row[\"activity\"]\n",
    "            if not isinstance(mode, str):\n",
    "                continue\n",
    "\n",
    "            # only apply to charging modes\n",
    "            if not mode.startswith(\"CHARGING\"):\n",
    "                continue\n",
    "\n",
    "            soc_drop = row.get(\"soc_drop\", 0) or 0\n",
    "\n",
    "            # CHARGING_ACTIVE â€“ absolutely no SOC drop expected\n",
    "            if mode == \"CHARGING_ACTIVE\" and soc_drop > THRESH_ACTIVE:\n",
    "                sessions.at[idx, \"glitch_flag\"] = True\n",
    "                sessions.at[idx, \"glitch_reason\"] = (\n",
    "                    f\"SOC dropped {soc_drop:.2f}% during CHARGING_ACTIVE\"\n",
    "                )\n",
    "                sessions.at[idx, \"activity\"] = \"GLITCH\"\n",
    "                continue\n",
    "\n",
    "            # CHARGING_MAINTAIN â€“ a tiny balancing drift is allowed\n",
    "            if mode == \"CHARGING_MAINTAIN\" and soc_drop > THRESH_MAINT:\n",
    "                sessions.at[idx, \"glitch_flag\"] = True\n",
    "                sessions.at[idx, \"glitch_reason\"] = (\n",
    "                    f\"SOC dropped {soc_drop:.2f}% during CHARGING_MAINTAIN\"\n",
    "                )\n",
    "                sessions.at[idx, \"activity\"] = \"GLITCH\"\n",
    "                continue\n",
    "\n",
    "            # CHARGING_IDLE â€“ some jitter possible due to sensor/rounding\n",
    "            if mode == \"CHARGING_IDLE\" and soc_drop > THRESH_IDLE:\n",
    "                sessions.at[idx, \"glitch_flag\"] = True\n",
    "                sessions.at[idx, \"glitch_reason\"] = (\n",
    "                    f\"SOC dropped {soc_drop:.2f}% during CHARGING_IDLE\"\n",
    "                )\n",
    "                sessions.at[idx, \"activity\"] = \"GLITCH\"\n",
    "                continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 3 â€” Push 'session' assignment back into raw df_day rows\n",
    "    # -----------------------------------------------------------\n",
    "    df_day[\"session\"] = None\n",
    "\n",
    "    for ses in sessions.itertuples(index=False):\n",
    "        mask = (\n",
    "            (df_day[\"id\"] == ses.id) &\n",
    "            (df_day[\"timestamp\"] >= ses.start_time) &\n",
    "            (df_day[\"timestamp\"] <= ses.end_time)\n",
    "        )\n",
    "        df_day.loc[mask, \"session\"] = ses.session\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 4 â€” Bucket distributions (TEMP, VOLT, SOC)\n",
    "    # -----------------------------------------------------------\n",
    "    BUCKET_MAP = {\n",
    "        'maxtemp_bucket': [\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"],\n",
    "        'temp_delta_bucket': [\"<2\", \"2â€“5\", \"5â€“8\", \">8\"],\n",
    "        'volt_delta_bucket': [\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"],\n",
    "        'soc_band_bucket': [\n",
    "            \"0â€“10\",\"10â€“20\",\"20â€“30\",\"30â€“40\",\"40â€“50\",\n",
    "            \"50â€“60\",\"60â€“70\",\"70â€“80\",\"80â€“90\",\"90â€“100\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for col, categories in BUCKET_MAP.items():\n",
    "        if col not in df_day.columns:\n",
    "            continue\n",
    "\n",
    "        # normalize category labels\n",
    "        df_day[col] = (\n",
    "            df_day[col].astype(str).str.replace(\"-\", \"â€“\")\n",
    "        )\n",
    "        df_day[col] = pd.Categorical(df_day[col], categories=categories, ordered=True)\n",
    "\n",
    "        # compute percentage distributions\n",
    "        pct = (\n",
    "            df_day.groupby(\"session\")[col]\n",
    "                .value_counts(normalize=True)\n",
    "                .mul(100)\n",
    "                .round(2)\n",
    "        )\n",
    "\n",
    "        pct_pivot = pct.unstack(fill_value=0)\n",
    "\n",
    "        pct_pivot.columns = [\n",
    "            f\"{col}_{str(c).replace('â€“','_').replace('<','lt').replace('>','gt')}_pct\"\n",
    "            for c in pct_pivot.columns\n",
    "        ]\n",
    "\n",
    "        sessions = sessions.join(pct_pivot, on=\"session\", how=\"left\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 5 â€” Round SOC columns\n",
    "    # -----------------------------------------------------------\n",
    "    for col in [\"soc_start\", \"soc_end\", \"soc_gain\", \"soc_drop\"]:\n",
    "        if col in sessions.columns:\n",
    "            sessions[col] = sessions[col].apply(\n",
    "                lambda x: round(x, 2) if pd.notna(x) else x\n",
    "            )\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 6 â€” Final ordering\n",
    "    # -----------------------------------------------------------\n",
    "    ordered_cols = [\n",
    "        \"id\", \"reg_num\", \"customer\", \"model\",\n",
    "        \"activity\", \"session\", \"date\",\n",
    "        \"start_time\", \"end_time\", \"duration_mins\",\n",
    "        \"charging_pct\", \"discharging_pct\", \"motion_pct\",\n",
    "        \"lv_pct\", \"off_pct\",\n",
    "        \"kwh_charging\", \"kwh_discharging\",\"charge_rate\",\"discharge_rate\",\n",
    "        \"soc_start\", \"soc_end\", \"soc_gain\", \"soc_drop\",\n",
    "    ]\n",
    "\n",
    "    # keep any extra bucket columns also\n",
    "    ordered_cols += [c for c in sessions.columns if c not in ordered_cols]\n",
    "\n",
    "    return sessions[ordered_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924133d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_day_session_analysis(\n",
    "    parquet_path: str, \n",
    "    start_date: datetime, \n",
    "    num_days: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Iteratively loads, processes, and aggregates session reports\n",
    "    (CHARGING_* / DISCHARGING_* via `activity`) for a defined\n",
    "    number of days across all vehicles.\n",
    "    \"\"\"\n",
    "    all_sessions: list[pd.DataFrame] = []\n",
    "    total_rows_ingested = 0\n",
    "\n",
    "    current_log_level = logging.getLogger().level\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "    try:\n",
    "        progress_bar = tqdm(\n",
    "            range(num_days),\n",
    "            desc=\"Processing Daily Sessions\",\n",
    "            unit=\"day\",\n",
    "            colour=\"cyan\",\n",
    "        )\n",
    "\n",
    "        for i in progress_bar:\n",
    "            current_start_dt = start_date + timedelta(days=i)\n",
    "            current_end_dt = current_start_dt + timedelta(days=1)\n",
    "\n",
    "            progress_bar.set_postfix_str(\n",
    "                f\"Date: {current_start_dt.strftime('%Y-%m-%d')}\"\n",
    "            )\n",
    "\n",
    "            df_subset = read_parquet_subset(\n",
    "                parquet_path=parquet_path,\n",
    "                start_dt=current_start_dt,\n",
    "                end_dt=current_end_dt,\n",
    "            )\n",
    "\n",
    "            if df_subset.empty:\n",
    "                continue\n",
    "\n",
    "            total_rows_ingested += len(df_subset)\n",
    "\n",
    "            # 1) Build unified sessions (activity comes from alt_mode)\n",
    "            day_sessions = build_charging_and_discharging_sessions(df_subset)\n",
    "\n",
    "            # 2) Enrich discharging-active sessions with distance / kWh/km\n",
    "            if not day_sessions.empty:\n",
    "                day_sessions = enrich_discharging_metrics(day_sessions, df_subset)\n",
    "                all_sessions.append(day_sessions)\n",
    "\n",
    "            del df_subset\n",
    "            gc.collect()\n",
    "\n",
    "    finally:\n",
    "        logging.getLogger().setLevel(current_log_level)\n",
    "\n",
    "    if all_sessions:\n",
    "        final_df = pd.concat(all_sessions, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "    return final_df, total_rows_ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# --- Execution Example ---\n",
    "# =========================================================================\n",
    "\n",
    "# Define the 75-day process window\n",
    "filter_start_date = datetime(2025, 9, 1) # Start date\n",
    "total_days_to_process = 75\n",
    "\n",
    "print(f\"--- Starting 75-Day Session Analysis ---\")\n",
    "print(f\"Processing range: {filter_start_date.strftime('%Y-%m-%d')} to {(filter_start_date + timedelta(days=total_days_to_process-1)).strftime('%Y-%m-%d')}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "charge_discharge_df, total_rows = run_multi_day_session_analysis(\n",
    "    parquet_path=\"../df_with_state.parquet\",\n",
    "    start_date=filter_start_date,\n",
    "    num_days=total_days_to_process\n",
    ")\n",
    "\n",
    "charge_discharge_df[\"charge_rate\"] = charge_discharge_df[\"charge_rate\"].round(3)\n",
    "charge_discharge_df[\"discharge_rate\"] = charge_discharge_df[\"discharge_rate\"].round(3)\n",
    "\n",
    "charge_discharge_df[\"start_time\"] = charge_discharge_df[\"start_time\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "charge_discharge_df[\"end_time\"] = charge_discharge_df[\"end_time\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time_sec = end_time - start_time\n",
    "elapsed_time_min = elapsed_time_sec / 60\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"âœ… ANALYSIS COMPLETE\")\n",
    "print(f\"Total time taken: {elapsed_time_sec:.2f} seconds ({elapsed_time_min:.2f} minutes)\")\n",
    "print(f\"Days processed:   {total_days_to_process}\")\n",
    "print(f\"Total Rows Ingested: {total_rows:,}\")\n",
    "print(f\"Final Report Shape: {charge_discharge_df.shape} (Rows: {charge_discharge_df.shape[0]}, Columns: {charge_discharge_df.shape[1]})\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27393d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df.to_parquet('charge_discharge_analysis.parquet', index=False)\n",
    "# charge_discharge_df = pd.read_parquet('charge_discharge_analysis.parquet')\n",
    "# charge_discharge_df['id'] = charge_discharge_df['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c10b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df[charge_discharge_df.id == '16'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df.to_excel('charge_discharge_analysis.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fcabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df[charge_discharge_df.activity == 'DISCHARGING_ACTIVE'].duration_mins.describe(percentiles=[0.5, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df[charge_discharge_df.activity == 'DISCHARGING_ACTIVE'].groupby(['id',])['duration_mins'].sum().describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df[charge_discharge_df.activity == 'CHARGING_ACTIVE'].kwh_charging.describe(percentiles=[0.5, 0.9, 0.95, 0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863edf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df[charge_discharge_df.activity == 'CHARGING_ACTIVE'][['charge_rate','discharge_rate']].describe(percentiles=[0.5, 0.9, 0.95, 0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df[charge_discharge_df.activity == 'DISCHARGING_ACTIVE'][['charge_rate','discharge_rate']].describe(percentiles=[0.5, 0.9, 0.95, 0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bucket_vehicle_lists(\n",
    "    df,\n",
    "    bucket_cols,\n",
    "    ranges,\n",
    "    id_col=\"id\",\n",
    "):\n",
    "    \"\"\"\n",
    "    df            : charge_discharge_df\n",
    "    bucket_cols   : list of bucket percentage columns (e.g., [\"temp_delta_bucket_5_8_pct\", \"temp_delta_bucket_gt8_pct\"])\n",
    "    ranges        : list of tuples (label, LOWER, UPPER or None)\n",
    "                     Example:\n",
    "                       [(\"20-30%\", 20, 30),\n",
    "                        (\"30-40%\", 30, 40),\n",
    "                        (\">40%\", 40, None)]\n",
    "\n",
    "    Returns: DataFrame with one row per range.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for label, low, high in ranges:\n",
    "        # Build condition for all bucket columns\n",
    "        cond = False\n",
    "        for col in bucket_cols:\n",
    "            if high is None:\n",
    "                cond |= (df[col] >= low)\n",
    "            else:\n",
    "                cond |= (df[col].between(low, high))\n",
    "\n",
    "        matched_ids = sorted(df.loc[cond, id_col].unique())\n",
    "\n",
    "        rows.append({\n",
    "            \"duration%\": label,\n",
    "            \"lower\": low,\n",
    "            \"upper\": high if high is not None else \"âˆž\",\n",
    "            \"num_vehicles\": len(matched_ids),\n",
    "            \"vehicle_ids\": \", \".join(matched_ids),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "##Delta Temperature Buckets\n",
    "bucket_cols = [\n",
    "    \"temp_delta_bucket_5_8_pct\",\n",
    "    \"temp_delta_bucket_gt8_pct\"\n",
    "]\n",
    "\n",
    "ranges = [\n",
    "    (\"20â€“30%\", 20, 30),\n",
    "    (\"30â€“40%\", 30, 40),\n",
    "    (\">40%\", 40, None)\n",
    "]\n",
    "\n",
    "result = extract_bucket_vehicle_lists(charge_discharge_df, bucket_cols, ranges)\n",
    "display(result)\n",
    "\n",
    "##Voltage Delta Buckets\n",
    "bucket_cols = [\n",
    "    \"volt_delta_bucket_20_30_pct\",\n",
    "    \"volt_delta_bucket_gt30_pct\"\n",
    "]\n",
    "ranges = [\n",
    "    (\"20â€“30%\", 20, 30),\n",
    "    (\"30â€“40%\", 30, 40),\n",
    "    (\">40%\", 40, None)\n",
    "]   \n",
    "result = extract_bucket_vehicle_lists(charge_discharge_df, bucket_cols, ranges)\n",
    "display(result) \n",
    "\n",
    "#maxtemp_bucket\n",
    "bucket_cols = [\n",
    "    \"maxtemp_bucket_35_40_pct\",\n",
    "    \"maxtemp_bucket_gt40_pct\"\n",
    "]\n",
    "ranges = [\n",
    "    (\"35â€“40%\", 35, 40),\n",
    "    (\">40%\", 40, None)\n",
    "]   \n",
    "result = extract_bucket_vehicle_lists(charge_discharge_df, bucket_cols, ranges)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_discharge_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffa852",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER = 40\n",
    "\n",
    "# -----------------------------\n",
    "# 1. TEMPERATURE HOT SESSIONS\n",
    "# -----------------------------\n",
    "hot_temp_sessions = charge_discharge_df[\n",
    "    (charge_discharge_df.activity == \"CHARGING_ACTIVE\") &\n",
    "    (\n",
    "        (charge_discharge_df.maxtemp_bucket_35_40_pct +\n",
    "         charge_discharge_df.maxtemp_bucket_gt40_pct) > LOWER\n",
    "    )\n",
    "]\n",
    "\n",
    "hot_temp_counts = (\n",
    "    hot_temp_sessions.groupby(\"id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_hot_temp_sessions\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. HIGH TEMPERATURE DELTA SESSIONS\n",
    "# -----------------------------\n",
    "hot_temp_delta_sessions = charge_discharge_df[\n",
    "    (charge_discharge_df.activity == \"CHARGING_ACTIVE\") &\n",
    "    (\n",
    "        (charge_discharge_df.temp_delta_bucket_5_8_pct +\n",
    "         charge_discharge_df.temp_delta_bucket_gt8_pct) > LOWER\n",
    "    )\n",
    "]\n",
    "\n",
    "hot_temp_delta_counts = (\n",
    "    hot_temp_delta_sessions.groupby(\"id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_hot_temp_delta_sessions\"))\n",
    "\n",
    "# -----------------------------\n",
    "# 2. VOLTAGE HOT SESSIONS\n",
    "# -----------------------------\n",
    "hot_volt_sessions = charge_discharge_df[\n",
    "    (charge_discharge_df.activity == \"CHARGING_ACTIVE\") &\n",
    "    (\n",
    "        (charge_discharge_df.volt_delta_bucket_20_30_pct +\n",
    "         charge_discharge_df.volt_delta_bucket_gt30_pct) > LOWER\n",
    "    )\n",
    "]\n",
    "\n",
    "hot_volt_counts = (\n",
    "    hot_volt_sessions.groupby(\"id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"num_hot_volt_sessions\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. TOTAL CHARGING SESSIONS\n",
    "# -----------------------------\n",
    "total_chg = (\n",
    "    charge_discharge_df[charge_discharge_df.activity == \"CHARGING_ACTIVE\"]\n",
    "    .groupby(\"id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"total_charging_sessions\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. TOTAL CHARGING DURATION\n",
    "# -----------------------------\n",
    "total_duration = (\n",
    "    charge_discharge_df[charge_discharge_df.activity == \"CHARGING_ACTIVE\"]\n",
    "    .groupby(\"id\")[\"duration_mins\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"total_duration_mins\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. MERGE EVERYTHING\n",
    "# -----------------------------\n",
    "result = (\n",
    "    total_chg\n",
    "    .merge(hot_temp_counts, on=\"id\", how=\"left\")\n",
    "    .merge(hot_temp_delta_counts, on=\"id\", how=\"left\")\n",
    "    .merge(hot_volt_counts, on=\"id\", how=\"left\")\n",
    "    .merge(total_duration, on=\"id\", how=\"left\")\n",
    ")\n",
    "\n",
    "\n",
    "# Replace NaN with 0 for hot session counts\n",
    "result[\"num_hot_temp_sessions\"] = result[\"num_hot_temp_sessions\"].fillna(0).astype(int)\n",
    "result[\"num_hot_temp_delta_sessions\"] = result[\"num_hot_temp_delta_sessions\"].fillna(0).astype(int)\n",
    "result[\"num_hot_volt_sessions\"] = result[\"num_hot_volt_sessions\"].fillna(0).astype(int)\n",
    "\n",
    "# Convert id to numeric for sorting\n",
    "result[\"id\"] = result[\"id\"].astype(\"int32\")\n",
    "\n",
    "result = result.sort_values(\"id\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87524524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_fleet_temp_table(df, threshold_pct=40):\n",
    "    df = df.copy()\n",
    "\n",
    "    df.id = df.id.astype('int32')\n",
    "    # Filter only charging-active sessions\n",
    "    d = df[df.activity == \"CHARGING_ACTIVE\"].copy()\n",
    "\n",
    "    # total >35Â°C exposure\n",
    "    d[\"pct_above_35\"] = (\n",
    "        d[\"maxtemp_bucket_35_40_pct\"].fillna(0) +\n",
    "        d[\"maxtemp_bucket_gt40_pct\"].fillna(0)\n",
    "    )\n",
    "\n",
    "    # high-temp flag\n",
    "    d[\"high_temp_flag\"] = d[\"pct_above_35\"] > threshold_pct\n",
    "\n",
    "    # Fleet-level aggregation\n",
    "    fleet_table = (\n",
    "        d.groupby(\"id\")\n",
    "         .agg(\n",
    "              total_sessions=(\"session\", \"count\"),\n",
    "              high_temp_count=(\"high_temp_flag\", \"sum\"),\n",
    "              mean_pct_above35=(\"pct_above_35\", \"mean\")\n",
    "         )\n",
    "         .reset_index()\n",
    "    )\n",
    "\n",
    "    # Convert to %\n",
    "    fleet_table[\"high_temp_pct\"] = (\n",
    "        100 * fleet_table[\"high_temp_count\"] / fleet_table[\"total_sessions\"]\n",
    "    ).round(2)\n",
    "\n",
    "    # Sort: worst offenders first\n",
    "    fleet_table = fleet_table.sort_values(\"high_temp_pct\", ascending=False)\n",
    "\n",
    "    return fleet_table\n",
    "\n",
    "\n",
    "\n",
    "fleet = build_fleet_temp_table(charge_discharge_df)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(\n",
    "    fleet.pivot_table(index=\"id\", values=\"high_temp_pct\"),\n",
    "    annot=True, fmt=\".1f\", cmap=\"Reds\", linewidths=0.5\n",
    ")\n",
    "plt.title(\"Fleet Heatmap: % of High-Temperature Charging Sessions (>35Â°C for >40% of session)\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Vehicle ID\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(\n",
    "    fleet.pivot_table(index=\"id\", values=\"mean_pct_above35\"),\n",
    "    annot=True, fmt=\".1f\", cmap=\"YlOrRd\", linewidths=0.5\n",
    ")\n",
    "plt.title(\"Fleet Heatmap: Mean % Time >35Â°C During Charging-Active Sessions\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Vehicle ID\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdff3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naarni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
