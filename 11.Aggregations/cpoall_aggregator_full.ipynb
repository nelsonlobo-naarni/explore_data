{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f540b25e-b32d-48a3-bdd8-14967ba9ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_datetime64tz_dtype\n",
    "import platform\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "# import duckdb \n",
    "import warnings\n",
    "# import fastparquet\n",
    "from tqdm import tqdm \n",
    "import psutil\n",
    "import time # For timing the execution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterable, Sequence, Tuple, Dict, List, Optional\n",
    "import argparse\n",
    "import pytz\n",
    "\n",
    "sys.path.append('..')\n",
    "from common import db_operations\n",
    "from common.db_operations import connect_to_trino, fetch_data_for_day_trino, fetch_distinct_device_ids, fetch_distinct_ids_for_day_trino, write_df_to_iceberg,execute_query\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Optional: adjust pandas display for debugging; you can comment these out\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\",datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5dd4d8-73fe-49c2-9089-681489f22392",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_COLS = [\n",
    "    \"id\", \"timestamp\", \"dt\",\n",
    "    \"vehiclereadycondition\", \"gun_connection_status\", \"ignitionstatus\",\"odometerreading\",\n",
    "    \"vehicle_speed_vcu\", \"gear_position\",\n",
    "    \"bat_soc\", \"soh\", \"total_battery_current\",\n",
    "    \"pack1_cellmax_temperature\", \"pack1_cell_min_temperature\",\n",
    "    \"pack1_maxtemperature_cell_number\", \"pack1_celltemperature_cellnumber\",\n",
    "    \"bat_voltage\", \"cellmax_voltagecellnumber\", \"cellminvoltagecellnumber\", \n",
    "    \"cell_min_voltage\",\"cell_max_voltage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95084e6-cd79-48aa-862e-77add9137657",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_COL_ORDER = [\n",
    "    # 1) Identity\n",
    "    \"id\", \"reg_num\", \"customer\", \"model\",\n",
    "\n",
    "    # 2) Session timeline\n",
    "    \"date\", \"activity\", \"session\",\n",
    "    \"start_time\", \"end_time\", \"duration_mins\",\n",
    "\n",
    "    # 3) Odometer & distance\n",
    "    \"odo_start\", \"odo_end\",\n",
    "    \"net_odo_km\", \"dist_km_raw\", \"dist_km\", \"max_physical_km\",\n",
    "\n",
    "    # 4) Energy & efficiency\n",
    "    \"kwh_charging\", \"kwh_discharging\",\n",
    "    \"energy_active_kwh\", \"kwh_per_km\",\"net_kwh_per_km\",\n",
    "    \"charge_rate\", \"discharge_rate\",\n",
    "\n",
    "    # 5) High-level utilisation\n",
    "    \"charging_pct\", \"discharging_pct\",\n",
    "    \"motion_pct\", \"lv_pct\", \"off_pct\",\n",
    "\n",
    "    # 8) SOC range\n",
    "    \"soc_start\", \"soc_end\", \"soc_gain\", \"soc_drop\",\n",
    "    \n",
    "    # 6) Speed stats\n",
    "    \"avg_speed\", \"med_speed\", \"max_speed\",\n",
    "\n",
    "    # 7) Delta stats (voltage & temp)\n",
    "    \"avg_volt_delta_mv\", \"med_volt_delta_mv\", \"p95_volt_delta_mv\", \"max_volt_delta_mv\",\n",
    "    \"avg_batt_temp_delta\", \"med_batt_temp_delta\", \"p95_batt_temp_delta\", \"max_batt_temp_delta\",\n",
    "\n",
    "    # 10) Temp buckets\n",
    "    \"maxtemp_bucket_lt28_pct\",\n",
    "    \"maxtemp_bucket_28_32_pct\",\n",
    "    \"maxtemp_bucket_32_35_pct\",\n",
    "    \"maxtemp_bucket_35_40_pct\",\n",
    "    \"maxtemp_bucket_gt40_pct\",\n",
    "\n",
    "    # 11) Temp delta buckets\n",
    "    \"temp_delta_bucket_lt2_pct\",\n",
    "    \"temp_delta_bucket_2_5_pct\",\n",
    "    \"temp_delta_bucket_5_8_pct\",\n",
    "    \"temp_delta_bucket_gt8_pct\",\n",
    "\n",
    "    # 12) Voltage delta buckets\n",
    "    \"volt_delta_bucket_0_10_pct\",\n",
    "    \"volt_delta_bucket_10_20_pct\",\n",
    "    \"volt_delta_bucket_20_30_pct\",\n",
    "    \"volt_delta_bucket_gt30_pct\",\n",
    "\n",
    "    # 13) SOC band buckets\n",
    "    \"soc_band_bucket_0_10_pct\",\n",
    "    \"soc_band_bucket_10_20_pct\",\n",
    "    \"soc_band_bucket_20_30_pct\",\n",
    "    \"soc_band_bucket_30_40_pct\",\n",
    "    \"soc_band_bucket_40_50_pct\",\n",
    "    \"soc_band_bucket_50_60_pct\",\n",
    "    \"soc_band_bucket_60_70_pct\",\n",
    "    \"soc_band_bucket_70_80_pct\",\n",
    "    \"soc_band_bucket_80_90_pct\",\n",
    "    \"soc_band_bucket_90_100_pct\",\n",
    "\n",
    "    # 9) Glitch flags\n",
    "    \"glitch_flag\", \"glitch_reason\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bace100-e171-4f52-ade9-e638bc484caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--start-date\", type=str, help=\"YYYY-MM-DD (IST)\")\n",
    "    parser.add_argument(\"--end-date\", type=str, help=\"YYYY-MM-DD (IST)\")\n",
    "    parser.add_argument(\"--yesterday\", action=\"store_true\", help=\"Run for yesterday in IST\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def resolve_dates(args):\n",
    "    if args.yesterday:\n",
    "        now_ist = datetime.now(IST)\n",
    "        y = (now_ist - timedelta(days=1)).date()\n",
    "        return str(y), str(y)\n",
    "\n",
    "    if args.start_date and args.end_date:\n",
    "        return args.start_date, args.end_date\n",
    "\n",
    "    raise ValueError(\"Provide --yesterday OR both --start-date and --end-date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26e7b73-d9f8-4392-aa48-14afb88ef025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_mem():\n",
    "    \"\"\"Try to return freed memory back to the OS (no-op on some platforms).\"\"\"\n",
    "    try:\n",
    "        libc = ctypes.CDLL(None)\n",
    "        if hasattr(libc, \"malloc_trim\"):\n",
    "            libc.malloc_trim(0)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec88a6f3-677d-4b74-85fd-000cb956f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_battery_temp_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Uses df.rename(inplace=False), creating one copy, which is fine for chunks\n",
    "    rename_map = {\n",
    "        \"pack1_cellmax_temperature\": \"batt_maxtemp\",\n",
    "        \"pack1_cell_min_temperature\": \"batt_mintemp\",\n",
    "        \"pack1_maxtemperature_cell_number\":\"batt_maxtemp_tc\", \n",
    "        \"pack1_celltemperature_cellnumber\":\"batt_mintemp_tc\",\n",
    "        \"cell_max_voltage\":\"batt_maxvolt\",\n",
    "        \"cellmax_voltagecellnumber\":\"batt_maxvolt_cell\",\n",
    "        \"cell_min_voltage\":\"batt_minvolt\",\n",
    "        \"cellminvoltagecellnumber\":\"batt_minvolt_cell\", \n",
    "    }\n",
    "    existing = {k: v for k, v in rename_map.items() if k in df.columns}\n",
    "    if not existing:\n",
    "        return df\n",
    "    return df.rename(columns=existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2cb8b0b-57a6-40f1-99ca-7187dc988233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# CONFIG\n",
    "# =====================================================================\n",
    "\n",
    "MAX_SPEED_KMPH = 120.0      # physical upper bound (bus never >120 km/h)\n",
    "MAX_ODO_DIST_KM = 0.2       # max plausible odo jump per sample (~200 m)\n",
    "MAX_DT_SEC = 3.0            # dt_sec cap (you already use this)\n",
    "BIG_ODO_CAP = 1.0           # sanity cap for odo (km)\n",
    "DT_DISCONTINUITY_SEC = 180  # >3 min gap can be treated as discontinuity in Stage-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77409e78-4b54-43b8-9026-c7e0af6d8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_odometer(df):\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\"):\n",
    "        idx = grp.index\n",
    "        odo = grp[\"odometer_final\"].to_numpy()\n",
    "\n",
    "        for i in range(1, len(odo)):\n",
    "            if odo[i] < odo[i-1]:\n",
    "                odo[i] = odo[i-1]\n",
    "\n",
    "        df.loc[idx, \"odometer_final\"] = odo\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830603d8-1d6d-476c-9c00-9d55b35cc495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_odometer(df, odo_col=\"odometerreading\"):\n",
    "    \"\"\"\n",
    "    TRUE, CORRECT, NULL-ONLY, SESSION-AWARE, 3-PASS ODOMETER IMPUTER.\n",
    "\n",
    "    Rules implemented exactly:\n",
    "\n",
    "      â€¢ PASS 1: Fix top/bottom NULL islands.\n",
    "      â€¢ PASS 2: SINGLE NULL: bracket logic + speed/dt estimate + clamping.\n",
    "      â€¢ PASS 3: MULTI NULL: iterative bounded fill, updating L â†’ new L.\n",
    "      â€¢ Session protection: If R < L â†’ treat as session break â†’ propagate L.\n",
    "      â€¢ STRICT: Never modify original *non-null* odometer readings.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "    df[\"odometer_final\"] = df[odo_col].astype(float)\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\"):\n",
    "        idx = grp.index\n",
    "        odo_raw = grp[odo_col].astype(float).to_numpy()\n",
    "        speed = grp[\"vehicle_speed_vcu\"].astype(float).to_numpy()\n",
    "        dt = grp[\"dt_sec\"].astype(float).to_numpy()\n",
    "\n",
    "        # Final output buffer:\n",
    "        fill = odo_raw.copy()\n",
    "\n",
    "        n = len(odo_raw)\n",
    "        i = 0\n",
    "\n",
    "        while i < n:\n",
    "            if not np.isnan(odo_raw[i]):\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Start of a null island\n",
    "            start = i\n",
    "            while i < n and np.isnan(odo_raw[i]):\n",
    "                i += 1\n",
    "            end = i - 1  # inclusive\n",
    "\n",
    "            prev_idx = start - 1\n",
    "            next_idx = end + 1\n",
    "\n",
    "            L = odo_raw[prev_idx] if prev_idx >= 0 else None\n",
    "            R = odo_raw[next_idx] if next_idx < n else None\n",
    "\n",
    "            # -------------------------------\n",
    "            # PASS 1: TOP & BOTTOM NULL ISLANDS\n",
    "            # -------------------------------\n",
    "            if L is None and R is not None:\n",
    "                # Top NULL block â†’ propagate next known value backward\n",
    "                for pos in range(start, end + 1):\n",
    "                    fill[pos] = R\n",
    "                continue\n",
    "\n",
    "            if R is None and L is not None:\n",
    "                # Bottom NULL block â†’ propagate previous known value forward\n",
    "                for pos in range(start, end + 1):\n",
    "                    fill[pos] = L\n",
    "                continue\n",
    "\n",
    "            # If both missing â†’ extremely rare but fallback to zero change\n",
    "            if L is None and R is None:\n",
    "                continue\n",
    "\n",
    "            # -------------------------------\n",
    "            # SESSION BREAK PROTECTION\n",
    "            # -------------------------------\n",
    "            if R < L:\n",
    "                # Monotonic break â†’ treat as end-of-session\n",
    "                for pos in range(start, end + 1):\n",
    "                    fill[pos] = L\n",
    "                continue\n",
    "\n",
    "            # -------------------------------\n",
    "            # PASS 2 & PASS 3 (Unified Engine)\n",
    "            # -------------------------------\n",
    "            gap = end - start + 1\n",
    "\n",
    "            # The active bounds shrink as we impute\n",
    "            curr_L = L\n",
    "            curr_R = R\n",
    "\n",
    "            for k in range(gap):\n",
    "                pos = start + k\n",
    "\n",
    "                if speed[pos] == 0:\n",
    "                    est = curr_L  # idle â†’ no movement\n",
    "                else:\n",
    "                    # compute movement in km\n",
    "                    est = curr_L + (speed[pos] * dt[pos] / 3600.0)\n",
    "\n",
    "                # Clamp within [curr_L, curr_R]\n",
    "                est_clamped = max(curr_L, min(est, curr_R))\n",
    "\n",
    "                # STRICT: only fill if original was NULL\n",
    "                if np.isnan(odo_raw[pos]):\n",
    "                    fill[pos] = est_clamped\n",
    "\n",
    "                # Shrink left boundary â†’ progressive update\n",
    "                curr_L = fill[pos]\n",
    "\n",
    "        df.loc[idx, \"odometer_final\"] = np.round(fill, 3)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49cfe529-6f53-4832-a1aa-eb861b93cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean & impute all sensors EXCEPT odometer.\n",
    "\n",
    "    Includes:\n",
    "      - dt_sec sanitisation\n",
    "      - SOC fixing (SOC=0 â†’ NaN â†’ interpolated)\n",
    "      - temperature, voltage, TC/cell sanity\n",
    "      - refined battery current clamping\n",
    "      - ignition/ready/gun consistency\n",
    "      - charging-mode overrides\n",
    "      - parked-mode overrides\n",
    "      - speed imputation for all stable states\n",
    "      - gear correction\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # Round column 'odometerreading' to 3 decimal places, preserving NaNs\n",
    "    mask_odo = df['odometerreading'].notna() # Create a boolean mask for non-null values\n",
    "    df.loc[mask_odo, 'odometerreading'] = df.loc[mask_odo, 'odometerreading'].round(3)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 0. dt_sec calculation\n",
    "    # ----------------------------------------------------\n",
    "    df[\"dt_sec\"] = (\n",
    "        df.groupby(\"id\")[\"timestamp\"]\n",
    "          .diff()\n",
    "          .dt.total_seconds()\n",
    "          .fillna(0)\n",
    "    )\n",
    "    df.loc[df[\"dt_sec\"] > 3, \"dt_sec\"] = 0\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1. SANITISATION\n",
    "    # ----------------------------------------------------\n",
    "    # 1a temperature sanitisation\n",
    "    for col in [\"batt_maxtemp\", \"batt_mintemp\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[df[col] < -10, col] = pd.NA\n",
    "\n",
    "    # 1b voltage\n",
    "    for col in [\"batt_maxvolt\", \"batt_minvolt\", \"bat_voltage\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[df[col] <= 0, col] = pd.NA\n",
    "\n",
    "    # 1c thermocouple + cell\n",
    "    for col in [\"batt_maxtemp_tc\", \"batt_mintemp_tc\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[(df[col] < 1) | (df[col] > 108), col] = pd.NA\n",
    "\n",
    "    for col in [\"batt_maxvolt_cell\", \"batt_minvolt_cell\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df.loc[(df[col] < 1) | (df[col] > 576), col] = pd.NA\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1d SOC FIX â€” VERY IMPORTANT\n",
    "    # ----------------------------------------------------\n",
    "    df[\"bat_soc\"] = pd.to_numeric(df[\"bat_soc\"], errors=\"coerce\")\n",
    "\n",
    "    # SOC=0 is almost always a sensor glitch â†’ treat as missing\n",
    "    df.loc[df[\"bat_soc\"] == 0, \"bat_soc\"] = np.nan\n",
    "\n",
    "    # Interpolate SOC per vehicle\n",
    "    df[\"bat_soc\"] = (\n",
    "        df.groupby(\"id\")[\"bat_soc\"]\n",
    "        .transform(lambda s: s.interpolate(limit_direction=\"both\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    df[\"bat_soc\"] = df[\"bat_soc\"].clip(lower=0, upper=100)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 1e refined battery current clamp\n",
    "    # ----------------------------------------------------\n",
    "    curr = pd.to_numeric(df[\"total_battery_current\"], errors=\"coerce\")\n",
    "    valid_mask = curr.abs().between(0, 1250)\n",
    "    valid_values = curr.where(valid_mask)\n",
    "\n",
    "    curr_ff = valid_values.ffill().fillna(0.0)\n",
    "    curr = curr.where(valid_mask, curr_ff)\n",
    "    df[\"total_battery_current\"] = curr\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2. GROUPWISE IMPUTATION (per vehicle)\n",
    "    # ----------------------------------------------------\n",
    "    impute_cols = [\n",
    "        (\"batt_maxtemp\", 80),\n",
    "        (\"batt_mintemp\", 80),\n",
    "        (\"batt_maxtemp_tc\", 80),\n",
    "        (\"batt_mintemp_tc\", 80),\n",
    "        (\"batt_maxvolt\", 30),\n",
    "        (\"batt_minvolt\", 30),\n",
    "        (\"batt_maxvolt_cell\", 30),\n",
    "        (\"batt_minvolt_cell\", 30),\n",
    "        (\"bat_voltage\", 20),\n",
    "        (\"bat_soc\", 300),   # now cleaned\n",
    "        (\"soh\", 300),\n",
    "    ]\n",
    "\n",
    "    for vid, grp in df.groupby(\"id\"):\n",
    "        idx = grp.index\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2a forward/backfill for regular sensors\n",
    "        # -----------------------------------------\n",
    "        for col, limit in impute_cols:\n",
    "            df.loc[idx, col] = grp[col].ffill().bfill()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2b current interpolation for small gaps\n",
    "        # -----------------------------------------\n",
    "        df.loc[idx, \"total_battery_current\"] = grp[\"total_battery_current\"].interpolate(\n",
    "            limit=10, limit_direction=\"both\"\n",
    "        )\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2c BASIC READY + GUN fill\n",
    "        # -----------------------------------------\n",
    "        for col in [\"vehiclereadycondition\", \"gun_connection_status\"]:df.loc[idx, col] = grp[col].ffill().bfill()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2d IGNITIONSTATUS CLEANUP\n",
    "        # -----------------------------------------\n",
    "        ign = pd.to_numeric(grp[\"ignitionstatus\"], errors=\"coerce\")\n",
    "        ign = ign.ffill().bfill()\n",
    "\n",
    "        # Ready=1 & ignition null â†’ ignition=1\n",
    "        ready_mask = df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int).eq(1)\n",
    "        ign.loc[ready_mask & ign.isna()] = 1\n",
    "        df.loc[idx, \"ignitionstatus\"] = ign\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2e SPEED IMPUTATION\n",
    "        # -----------------------------------------\n",
    "        if \"vehicle_speed_vcu\" in grp.columns:\n",
    "            v = pd.to_numeric(grp[\"vehicle_speed_vcu\"], errors=\"coerce\")\n",
    "            v = v.where(v.between(0, 120), np.nan)\n",
    "\n",
    "            v = v.ffill().bfill()\n",
    "            df.loc[idx, \"vehicle_speed_vcu\"] = v.round(2)\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2f GEAR POSITION\n",
    "        # -----------------------------------------\n",
    "        if \"gear_position\" in grp.columns:\n",
    "            g = pd.to_numeric(grp[\"gear_position\"], errors=\"coerce\")\n",
    "            g = g.where(g.isin([0, 1, 2]), np.nan)\n",
    "\n",
    "            ready0 = df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int).eq(0)\n",
    "            ign0 = df.loc[idx, \"ignitionstatus\"].fillna(0).astype(int).eq(0)\n",
    "\n",
    "            force_neutral = ready0 | ign0\n",
    "            g[force_neutral] = 0\n",
    "\n",
    "            df.loc[idx, \"gear_position\"] = g.ffill().bfill().astype(\"Int64\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2g CHARGING STATE CONSISTENCY\n",
    "        # ----------------------------------------------------\n",
    "        charging = df.loc[idx, \"gun_connection_status\"].fillna(0).astype(int).eq(1)\n",
    "\n",
    "        # ignition ON during charging\n",
    "        df.loc[idx[charging], \"ignitionstatus\"] = 1\n",
    "        df.loc[idx[charging], \"vehiclereadycondition\"] = 0\n",
    "        df.loc[idx[charging], \"gear_position\"] = 0\n",
    "\n",
    "        # speed=0 when charging & missing\n",
    "        df.loc[idx[charging & df.loc[idx, \"vehicle_speed_vcu\"].isna()],\"vehicle_speed_vcu\"] = 0.0\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2h OFF/PARKED SPEED FIX\n",
    "        # gun=0 & ready=0 & ignition=0 & speed NA â†’ 0\n",
    "        # ----------------------------------------------------\n",
    "        off_mask = (\n",
    "            (df.loc[idx, \"gun_connection_status\"].fillna(0).astype(int) == 0) &\n",
    "            (df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int) == 0) &\n",
    "            (df.loc[idx, \"ignitionstatus\"].fillna(0).astype(int) == 0)\n",
    "        )\n",
    "\n",
    "        df.loc[idx[off_mask & df.loc[idx, \"vehicle_speed_vcu\"].isna()],\"vehicle_speed_vcu\"] = 0.0\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # 2i READY BUT IGNITION=0 (contradictory â†’ treat as stationary)\n",
    "        # ----------------------------------------------------\n",
    "        ready_ign_off = (\n",
    "            (df.loc[idx, \"gun_connection_status\"].fillna(0).astype(int) == 0) &\n",
    "            (df.loc[idx, \"vehiclereadycondition\"].fillna(0).astype(int) == 1) &\n",
    "            (df.loc[idx, \"ignitionstatus\"].fillna(0).astype(int) == 0)\n",
    "        )\n",
    "\n",
    "        df.loc[idx[ready_ign_off & df.loc[idx, \"vehicle_speed_vcu\"].isna()],\"vehicle_speed_vcu\"] = 0.0\n",
    "        \n",
    "        df.vehicle_speed_vcu = df.vehicle_speed_vcu.round(2)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ab98f-7cf0-431a-93dc-56d1b1a4c809",
   "metadata": {},
   "source": [
    "Aï¸âƒ£ First: we derive a boolean signal: `gun_connected = (out[\"gun_connection_status\"] == 1)`\n",
    "\n",
    "ðŸ”‹ CHARGING_ACTIVE: `chg_active = (gun_connected) & (current > +5A)`\n",
    "- Battery is in CC/CV mode, current > 0, charger locked â†’ SOC must increase or stay flat.\n",
    "- If it drops â†’ timestamp glitch, SOC jitter, or packet misordering â†’ G L I T C H\n",
    "\n",
    "ðŸ”‹ CHARGING_MAINTAIN: `chg_maint = (gun_connected) & current.abs().between(0, 5)`\n",
    "- BMS balancing may cause Â±0.1â€“0.3 % SOC wobble.\n",
    "- Bigger drops = glitch.\n",
    "\n",
    "ðŸ”‹ CHARGING_IDLE: chg_idle = `(gun_connected) & (current > 5)`\n",
    "- Charger connected but no current. SOC may drift slightly due to temperature compensation. Â±0.5% jitter is normal.\n",
    "\n",
    "\n",
    "Bï¸âƒ£ Then DISCHARGING_* states\n",
    "- âœ” DISCHARGING_ACTIVE\n",
    "- dis_active = (\n",
    "    (~gun_connected) &\n",
    "    (vehicle_speed_vcu > 0.5) &\n",
    "    (gear_position in [1,2])\n",
    ")\n",
    "\n",
    "* DISCHARGING_IDLE\n",
    "- Everything else not covered by the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9298b78a-5611-4baa-9be7-cec4df092235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_with_state(df: pd.DataFrame, df_mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Make a copy (chunk-safe)\n",
    "    out = df.copy()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. Merge mapping\n",
    "    # ------------------------------------------------------------------\n",
    "    out[\"id\"] = out[\"id\"].astype(str)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # OPTIONAL: Merge mapping_df (reg_num, customer, model, etc.)\n",
    "    # -----------------------------------------------------------\n",
    "    if df_mapping is not None:\n",
    "        if not isinstance(df_mapping, pd.DataFrame):\n",
    "            raise TypeError(\"df_mapping must be a DataFrame or None\")\n",
    "    \n",
    "        # convert id to string on both sides\n",
    "        out[\"id\"] = out[\"id\"].astype(str)\n",
    "        df_mapping = df_mapping.copy()\n",
    "        df_mapping[\"id\"] = df_mapping[\"id\"].astype(str)\n",
    "    \n",
    "        # merge\n",
    "        out = out.merge(df_mapping, on=\"id\", how=\"left\", validate=\"m:1\")\n",
    "    \n",
    "    else:\n",
    "        # Mapping not supplied â†’ do NOT merge anything\n",
    "        # You can decide whether you want to create empty columns or skip them.\n",
    "        # Option A: create empty metadata columns (HARmless)\n",
    "        out[\"reg_num\"] = None\n",
    "        out[\"customer\"] = None\n",
    "        out[\"model\"] = None\n",
    "    \n",
    "        # If you prefer NOT to create them, tell me and Iâ€™ll remove these lines.\n",
    "\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # 2. Timestamp handling â€” THE SAFE VERSION\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    # --- TIMESTAMP FIX (UTC â†’ IST) ---\n",
    "\n",
    "    # 1. Parse raw timestamp exactly as received\n",
    "    out[\"ts_utc\"] = pd.to_datetime(out[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # 2. Drop invalid rows\n",
    "    out = out.dropna(subset=[\"ts_utc\"])\n",
    "\n",
    "    # 3. Convert to Asia/Kolkata (IST)\n",
    "    out[\"timestamp\"] = out[\"ts_utc\"].dt.tz_convert(\"Asia/Kolkata\")\n",
    "\n",
    "    # 4. Remove timezone info from final timestamp if needed\n",
    "    #    (matplotlib, parquet, feather become safer with tz-naive)\n",
    "    out[\"timestamp\"] = out[\"timestamp\"].dt.tz_localize(None)\n",
    "\n",
    "    # 5. Sort by vehicle + IST timestamp\n",
    "    out = out.sort_values([\"id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. Mode + alt_mode logic (unchanged)\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # Gun connection normalization\n",
    "    gcs_raw = out[\"gun_connection_status\"]\n",
    "    gcs_num = pd.to_numeric(gcs_raw, errors=\"coerce\")\n",
    "    gcs_str = gcs_raw.astype(str).str.lower().str.strip()\n",
    "\n",
    "    gun_connected = (gcs_num == 1) | gcs_str.isin({\"1\",\"true\",\"yes\",\"y\",\"connected\",\"on\"})\n",
    "    gun_connected = gun_connected.fillna(False)\n",
    "\n",
    "    # Vehicle readiness normalization\n",
    "    if \"vehiclereadycondition\" in out.columns:\n",
    "        vrc_raw = out[\"vehiclereadycondition\"]\n",
    "        vrc_num = pd.to_numeric(vrc_raw, errors=\"coerce\")\n",
    "        vrc_str = vrc_raw.astype(str).str.strip().str.lower()\n",
    "        vehicle_ready = (vrc_num == 1) | vrc_str.isin({\"1\",\"true\",\"yes\",\"y\",\"ready\",\"on\"})\n",
    "        vehicle_ready = vehicle_ready.fillna(False)\n",
    "    else:\n",
    "        vehicle_ready = pd.Series(False, index=out.index)\n",
    "\n",
    "    # Legacy mode column\n",
    "    # out[\"mode\"] = np.where(gun_connected, \"CHARGING\", \"DISCHARGING\")\n",
    "\n",
    "    # Rolling current for alt_mode\n",
    "    current_rm = (\n",
    "        out[\"total_battery_current\"]\n",
    "        .rolling(15, min_periods=1)\n",
    "        .mean()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # thresholds\n",
    "    ACTIVE_CHG_THRESH = -15\n",
    "    MAINTAIN_LOW = -15\n",
    "    MAINTAIN_HIGH = +2\n",
    "\n",
    "    # CHARGING states\n",
    "    chg_active = (gun_connected &(out[\"total_battery_current\"] < -5)).to_numpy(dtype=bool)\n",
    "    # chg_maint = (gun_connected &(out[\"total_battery_current\"].abs().between(0, 5))).to_numpy(dtype=bool)\n",
    "    chg_idle = (gun_connected &(out[\"total_battery_current\"] >= -5)).to_numpy(dtype=bool)\n",
    "\n",
    "    # # DISCHARGING states\n",
    "    dis_active = ((~gun_connected) &(out[\"vehicle_speed_vcu\"].gt(0.5).fillna(False)) &(out[\"gear_position\"].isin([1, 2]).fillna(False))).to_numpy(dtype=bool)\n",
    "    # dis_idle   = ((~gun_connected) & (~dis_active)).to_numpy(dtype=bool)\n",
    "\n",
    "    out[\"alt_mode\"] = np.select(\n",
    "        [chg_active, chg_idle, dis_active],\n",
    "        [\"CHARGING_ACTIVE\", \"CHARGING_IDLE\", \"DISCHARGING_ACTIVE\"],\n",
    "        default=\"DISCHARGING_IDLE\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. Delta + buckets (unchanged)\n",
    "    # ------------------------------------------------------------------\n",
    "    for col in [\"batt_maxtemp\", \"batt_mintemp\", \"batt_maxvolt\", \"batt_minvolt\"]:\n",
    "        if col in out.columns:\n",
    "            out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "\n",
    "    out[\"batt_temp_delta\"] = out[\"batt_maxtemp\"] - out[\"batt_mintemp\"]\n",
    "    out[\"volt_delta_mv\"] = abs((out[\"batt_maxvolt\"] - out[\"batt_minvolt\"]) * 1000)  # absolute value since max < min can happen\n",
    "\n",
    "    out[\"date_val\"] = out[\"timestamp\"].dt.floor(\"D\")\n",
    "\n",
    "    # Bucketing\n",
    "    out[\"maxtemp_bucket\"] = pd.cut(\n",
    "        out[\"batt_maxtemp\"],\n",
    "        [-np.inf, 28, 32, 35, 40, np.inf],\n",
    "        labels=[\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"]\n",
    "    )\n",
    "\n",
    "    out[\"temp_delta_bucket\"] = pd.cut(\n",
    "        out[\"batt_temp_delta\"],\n",
    "        [-np.inf, 2, 5, 8, np.inf],\n",
    "        labels=[\"<2\", \"2â€“5\", \"5â€“8\", \">8\"]\n",
    "    )\n",
    "\n",
    "    out[\"volt_delta_bucket\"] = pd.cut(\n",
    "        out[\"volt_delta_mv\"],\n",
    "        [0, 10, 20, 30, np.inf],\n",
    "        labels=[\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    soc_bins = [0,10,20,30,40,50,60,70,80,90,np.inf]\n",
    "    soc_labels = [\"0â€“10\",\"10â€“20\",\"20â€“30\",\"30-40\",\"40-50\",\"50-60\",\"60-70\",\"70-80\",\"80-90\",\"90-100\"]\n",
    "\n",
    "    out[\"soc_band_bucket\"] = pd.cut(out[\"bat_soc\"], bins=soc_bins, labels=soc_labels)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. Select final columns\n",
    "    # ------------------------------------------------------------------\n",
    "    cols_keep = [\n",
    "        \"id\",\"reg_num\",\"customer\",\"model\",\n",
    "        \"timestamp\",\"date_val\",\"dt_sec\",\n",
    "        \"mode\",\"alt_mode\",\n",
    "        \"ignitionstatus\",\"vehiclereadycondition\",\"gun_connection_status\",\n",
    "        \"vehicle_speed_vcu\",\"gear_position\",\n",
    "        \"odometerreading\",\"odometer_final\",\n",
    "        \"batt_maxtemp\",\"batt_mintemp\",\"batt_temp_delta\",\n",
    "        \"maxtemp_bucket\",\"temp_delta_bucket\",\n",
    "        \"batt_maxvolt\",\"batt_minvolt\",\"volt_delta_mv\",\"volt_delta_bucket\",\n",
    "        \"batt_maxtemp_tc\",\"batt_mintemp_tc\",\n",
    "        \"pack_id_max\",\"pack_id_min\",\n",
    "        \"batt_maxvolt_cell\",\"batt_minvolt_cell\",\n",
    "        \"bat_voltage\",\"total_battery_current\",\n",
    "        \"bat_soc\",\"soc_band_bucket\",\"soh\"\n",
    "    ]\n",
    "\n",
    "    cols_keep = [c for c in cols_keep if c in out.columns]\n",
    "    out = out[cols_keep]\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee0f74a-00b2-49ce-8629-a83098bb8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_vehicle_mapping(conn, schema=\"facts_prod\", table=\"device_mapping\"):\n",
    "    sql = f\"\"\"\n",
    "        SELECT \n",
    "            CAST(id AS VARCHAR) AS id,\n",
    "            bus_registration_number AS reg_num,\n",
    "            customer,\n",
    "            model\n",
    "        FROM {schema}.{table}\n",
    "        WHERE device_id IS NOT NULL\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    cols = [c[0] for c in cur.description]\n",
    "    return pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f32967-78c9-4ff7-bb2f-d42b13a256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_can_data_for_id_day(\n",
    "#     trino_conn,\n",
    "#     vehicle_id: str,\n",
    "#     date_str: str,\n",
    "#     schema=\"facts_prod\",\n",
    "#     table=\"can_parsed_output_all\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Fetches 1 day of data for a single ID from Trino,\n",
    "#     including timestamp IST conversion.\n",
    "#     \"\"\"\n",
    "\n",
    "#     sql = f\"\"\"\n",
    "#         SELECT \n",
    "#             id,\n",
    "#             timestamp,\n",
    "#             timestamp AT TIME ZONE 'Asia/Kolkata' AS timestamp_ist,\n",
    "#             *\n",
    "#         FROM {schema}.{table}\n",
    "#         WHERE id = CAST('{vehicle_id}' AS VARBINARY)\n",
    "#         AND DATE(timestamp) = DATE('{date_str}')\n",
    "#         ORDER BY timestamp\n",
    "#     \"\"\"\n",
    "\n",
    "#     cur = trino_conn.cursor()\n",
    "#     cur.execute(sql)\n",
    "#     rows = cur.fetchall()\n",
    "#     cols = [c[0] for c in cur.description]\n",
    "\n",
    "#     df = pd.DataFrame(rows, columns=cols)\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d9314-e8fb-4529-a467-da8b0e10a075",
   "metadata": {},
   "source": [
    "- A.reconstruct the entire bcs_tms_analysis pipeline\n",
    "\n",
    "- B. produce clean, production-ready versions of:\n",
    " - 1. build_tms_sessions\n",
    " - 2. compute_tms_stats\n",
    " - 3. build_pack_level_tables\n",
    " - 4. build_cell_level_tables\n",
    "\n",
    "- C. build a run_daily_tms_analysis() orchestrator like energy_mileage_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa26541-cc54-485a-bdc6-1eaf2929ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Canonical activity state (4 states only)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def add_activity_state(\n",
    "    df: pd.DataFrame,\n",
    "    current_col: str = \"total_battery_current\",\n",
    "    gun_col: str = \"gun_connection_status\",\n",
    "    active_threshold: float = 5.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a canonical 4-state 'activity' column based on gun connection\n",
    "    + total_battery_current sign/magnitude.\n",
    "\n",
    "    Rules (as per your spec):\n",
    "      - < 0 A means CHARGING domain\n",
    "      - > 0 A means DISCHARGING domain\n",
    "      - When gun_connection_status == 1:\n",
    "          * Anything that isn't CHARGING_ACTIVE is CHARGING_IDLE\n",
    "      - DISCHARGING logic mirrors CHARGING, but respects gun=0.\n",
    "\n",
    "    Final allowed states:\n",
    "        - CHARGING_ACTIVE\n",
    "        - CHARGING_IDLE\n",
    "        - DISCHARGING_ACTIVE\n",
    "        - DISCHARGING_IDLE\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    if gun_col not in out.columns:\n",
    "        raise KeyError(f\"{gun_col!r} not found in DataFrame\")\n",
    "\n",
    "    if current_col not in out.columns:\n",
    "        raise KeyError(f\"{current_col!r} not found in DataFrame\")\n",
    "\n",
    "    gun_raw = out[gun_col]\n",
    "    gun_num = pd.to_numeric(gun_raw, errors=\"coerce\")\n",
    "    gun_str = gun_raw.astype(str).str.strip().str.lower()\n",
    "\n",
    "    gun_connected = (gun_num == 1) | gun_str.isin({\"1\", \"true\", \"yes\", \"y\", \"connected\", \"on\"})\n",
    "    gun_connected = gun_connected.fillna(False)\n",
    "\n",
    "    curr = pd.to_numeric(out[current_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Charging / discharging domains based purely on gun\n",
    "    is_chg_domain = gun_connected\n",
    "    is_dis_domain = ~gun_connected\n",
    "\n",
    "    # Within domains, decide ACTIVE vs IDLE based on sign + magnitude\n",
    "    # NOTE: <0A => charging, >0A => discharging (per spec)\n",
    "    chg_active = is_chg_domain & (curr <= -active_threshold)\n",
    "    chg_idle   = is_chg_domain & ~chg_active\n",
    "\n",
    "    dis_active = is_dis_domain & (curr >= active_threshold)\n",
    "    dis_idle   = is_dis_domain & ~dis_active\n",
    "\n",
    "    activity = np.select(\n",
    "        [chg_active, chg_idle, dis_active],\n",
    "        [\"CHARGING_ACTIVE\", \"CHARGING_IDLE\", \"DISCHARGING_ACTIVE\"],\n",
    "        default=\"DISCHARGING_IDLE\",\n",
    "    )\n",
    "\n",
    "    out[\"activity\"] = activity.astype(\"object\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6338e8f8-041f-417b-85e0-2473d5112c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------------------------------------------------\n",
    "# # 2. Session builder\n",
    "# # ---------------------------------------------------------------------\n",
    "\n",
    "# def _ensure_dt_sec(df: pd.DataFrame) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Ensure there is a 'dt_sec' column with per-row duration in seconds.\n",
    "#     If already present, fill NaNs with 0.\n",
    "#     Otherwise compute from timestamp per id.\n",
    "#     \"\"\"\n",
    "#     if \"dt_sec\" in df.columns and not df[\"dt_sec\"].isna().all():\n",
    "#         return df[\"dt_sec\"].fillna(0.0)\n",
    "\n",
    "#     if \"timestamp\" not in df.columns:\n",
    "#         raise KeyError(\"'timestamp' column required to compute dt_sec\")\n",
    "\n",
    "#     if \"id\" not in df.columns:\n",
    "#         raise KeyError(\"'id' column required to compute dt_sec\")\n",
    "\n",
    "#     df_sorted = df.sort_values([\"id\", \"timestamp\"])\n",
    "#     dt = (\n",
    "#         df_sorted.groupby(\"id\")[\"timestamp\"]\n",
    "#         .diff()\n",
    "#         .dt.total_seconds()\n",
    "#         .fillna(0.0)\n",
    "#     )\n",
    "#     # align back to original order\n",
    "#     dt = dt.sort_index()\n",
    "#     return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd5a75b6-3280-4d31-882d-482e141e5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bucket_label_to_suffix(label: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert bucket labels like '<28', '28â€“32', '0â€“10', '>40'\n",
    "    into suffixes like 'lt28', '28_32', '0_10', 'gt40'.\n",
    "    \"\"\"\n",
    "    if pd.isna(label):\n",
    "        return \"unknown\"\n",
    "\n",
    "    s = str(label)\n",
    "    # Normalise Unicode dashes\n",
    "    s = s.replace(\"â€“\", \"-\").replace(\"â€”\", \"-\")\n",
    "\n",
    "    if s.startswith(\"<\"):\n",
    "        return \"lt\" + s[1:]\n",
    "    if s.startswith(\">\"):\n",
    "        return \"gt\" + s[1:]\n",
    "\n",
    "    # Ranges like '28-32' or '0-10'\n",
    "    s = s.replace(\" \", \"\")\n",
    "    if \"-\" in s:\n",
    "        lo, hi = s.split(\"-\", 1)\n",
    "        return f\"{lo}_{hi}\"\n",
    "\n",
    "    # Fallback\n",
    "    return s.replace(\"%\", \"\").replace(\"+\", \"p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ccd4a6-9f5a-4262-b0a4-6903efe51b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How close two charging blocks can be and still count as one session\n",
    "CHARGING_GAP_MERGE_MIN = 15.0   # minutes\n",
    "SPEED_MOTION_THRESHOLD = 0.5   # km/h, for motion_pct\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _get_charging_mask(df_vid: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns a boolean Series marking rows that belong to a *charging envelope*.\n",
    "\n",
    "    Priority:\n",
    "      1) If 'alt_mode' exists:\n",
    "           any alt_mode starting with 'CHARGING'\n",
    "           (CHARGING_ACTIVE / CHARGING_MAINTAIN / CHARGING_IDLE)\n",
    "           is treated as charging.\n",
    "      2) Else if 'mode' exists:\n",
    "           mode == 'CHARGING'.\n",
    "      3) Else: fall back to gun_connection_status / current-based heuristic.\n",
    "    \"\"\"\n",
    "    df_vid = df_vid.copy()\n",
    "\n",
    "    # --- Preferred: multi-state alt_mode ---\n",
    "    if \"alt_mode\" in df_vid.columns:\n",
    "        return df_vid[\"alt_mode\"].astype(str).str.startswith(\"CHARGING\")\n",
    "\n",
    "    # --- Fallback: simple mode ---\n",
    "    # if \"mode\" in df_vid.columns:\n",
    "    #     return df_vid[\"mode\"].astype(str).str.upper() == \"CHARGING\"\n",
    "\n",
    "    # --- Last resort: gun + current heuristic ---\n",
    "    gun = pd.Series(False, index=df_vid.index)\n",
    "    if \"gun_connection_status\" in df_vid.columns:\n",
    "        g = df_vid[\"gun_connection_status\"]\n",
    "        g_num = pd.to_numeric(g, errors=\"coerce\")\n",
    "        g_str = g.astype(str).str.strip().str.lower()\n",
    "        gun = (g_num == 1) | g_str.isin({\"1\", \"true\", \"yes\", \"y\", \"connected\", \"on\"})\n",
    "        gun = gun.fillna(False)\n",
    "\n",
    "    if \"total_battery_current\" in df_vid.columns:\n",
    "        cur = pd.to_numeric(df_vid[\"total_battery_current\"], errors=\"coerce\")\n",
    "        cur_cond = (cur < -5).fillna(False)\n",
    "    else:\n",
    "        cur_cond = pd.Series(False, index=df_vid.index)\n",
    "\n",
    "    return gun | cur_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad4663a-9894-4c7d-b50c-e4b2ff73cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_charging_sessions_for_vehicle(df_vid: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Builds stitched CHARGING envelopes for a single vehicle.\n",
    "\n",
    "    A charging envelope is any contiguous region where _get_charging_mask() is True,\n",
    "    with gaps â‰¤ CHARGING_GAP_MERGE_MIN minutes merged into a single session.\n",
    "\n",
    "    Returns a list of dicts:\n",
    "        { 'start_idx', 'end_idx', 'start_time', 'end_time' }\n",
    "    \"\"\"\n",
    "    df_vid = df_vid.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    is_chg = _get_charging_mask(df_vid)\n",
    "\n",
    "    if not is_chg.any():\n",
    "        return []\n",
    "\n",
    "    tmp = df_vid.copy()\n",
    "    tmp[\"is_chg\"] = is_chg\n",
    "    tmp[\"chg_block\"] = tmp[\"is_chg\"].ne(tmp[\"is_chg\"].shift()).cumsum()\n",
    "\n",
    "    raw_blocks: list[dict] = []\n",
    "    for block_id, g in tmp.groupby(\"chg_block\", sort=True):\n",
    "        # skip non-charging blocks\n",
    "        if not g[\"is_chg\"].iloc[0]:\n",
    "            continue\n",
    "\n",
    "        start_idx = int(g.index[0])\n",
    "        end_idx   = int(g.index[-1])\n",
    "\n",
    "        raw_blocks.append(\n",
    "            {\n",
    "                \"start_idx\": start_idx,\n",
    "                \"end_idx\": end_idx,\n",
    "                \"start_time\": df_vid.loc[start_idx, \"timestamp\"],\n",
    "                \"end_time\":   df_vid.loc[end_idx,   \"timestamp\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not raw_blocks:\n",
    "        return []\n",
    "\n",
    "    raw_blocks.sort(key=lambda b: b[\"start_idx\"])\n",
    "\n",
    "    stitched: list[dict] = []\n",
    "    current = raw_blocks[0].copy()\n",
    "\n",
    "    for nxt in raw_blocks[1:]:\n",
    "        gap_min = (nxt[\"start_time\"] - current[\"end_time\"]).total_seconds() / 60.0\n",
    "        if gap_min <= CHARGING_GAP_MERGE_MIN:\n",
    "            # merge into current envelope\n",
    "            current[\"end_idx\"] = nxt[\"end_idx\"]\n",
    "            current[\"end_time\"] = nxt[\"end_time\"]\n",
    "        else:\n",
    "            stitched.append(current)\n",
    "            current = nxt.copy()\n",
    "\n",
    "    stitched.append(current)\n",
    "    return stitched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c2e8510-3bb9-4de0-89cc-4155b1053981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_session_metrics(seg: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Given a contiguous segment (one session) for a single vehicle,\n",
    "    compute all requested metrics.\n",
    "    \"\"\"\n",
    "    seg = seg.copy()\n",
    "    seg = seg.sort_values(\"timestamp\")\n",
    "\n",
    "    if len(seg) < 2:\n",
    "        return None\n",
    "\n",
    "    # --- time deltas ---\n",
    "    seg[\"dt\"] = seg[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    # For safety: zero-out first dt so we don't pull in time before the session\n",
    "    seg.iloc[0, seg.columns.get_loc(\"dt\")] = 0.0\n",
    "\n",
    "    total_time = seg[\"dt\"].sum()\n",
    "    if total_time <= 0:\n",
    "        total_time = (seg[\"timestamp\"].iloc[-1] - seg[\"timestamp\"].iloc[0]).total_seconds()\n",
    "\n",
    "    # --- base metadata ---\n",
    "    id_val = seg[\"id\"].iloc[0]\n",
    "    reg_num = seg[\"reg_num\"].dropna().iloc[0] if \"reg_num\" in seg.columns and seg[\"reg_num\"].notna().any() else None\n",
    "    customer = seg[\"customer\"].dropna().iloc[0] if \"customer\" in seg.columns and seg[\"customer\"].notna().any() else None\n",
    "    model = seg[\"model\"].dropna().iloc[0] if \"model\" in seg.columns and seg[\"model\"].notna().any() else None\n",
    "\n",
    "    start_time = seg[\"timestamp\"].iloc[0]\n",
    "    end_time = seg[\"timestamp\"].iloc[-1]\n",
    "    duration_mins = round((end_time - start_time).total_seconds() / 60.0, 2)\n",
    "\n",
    "    # --- energy integration ---\n",
    "    # Ensure numeric\n",
    "    seg[\"bat_voltage\"] = pd.to_numeric(seg.get(\"bat_voltage\"), errors=\"coerce\")\n",
    "    seg[\"total_battery_current\"] = pd.to_numeric(seg.get(\"total_battery_current\"), errors=\"coerce\")\n",
    "\n",
    "    # kW\n",
    "    seg[\"power_kw\"] = (seg[\"bat_voltage\"] * seg[\"total_battery_current\"]) / 1000.0\n",
    "\n",
    "    # kWh components (sign-aware)\n",
    "    # charging (I < 0) â†’ energy INTO pack\n",
    "    mask_chg = seg[\"total_battery_current\"] < 0\n",
    "    mask_dis = seg[\"total_battery_current\"] > 0\n",
    "\n",
    "    seg[\"energy_kwh_chg\"] = 0.0\n",
    "    seg.loc[mask_chg, \"energy_kwh_chg\"] = -seg.loc[mask_chg, \"power_kw\"] * seg.loc[mask_chg, \"dt\"] / 3600.0\n",
    "\n",
    "    seg[\"energy_kwh_dis\"] = 0.0\n",
    "    seg.loc[mask_dis, \"energy_kwh_dis\"] = seg.loc[mask_dis, \"power_kw\"] * seg.loc[mask_dis, \"dt\"] / 3600.0\n",
    "\n",
    "    kwh_charging = round(seg[\"energy_kwh_chg\"].sum(), 2)\n",
    "    kwh_discharging = round(seg[\"energy_kwh_dis\"].sum(), 2)\n",
    "\n",
    "    # --- SOC metrics ---\n",
    "    soc_col = \"bat_soc\" if \"bat_soc\" in seg.columns else None\n",
    "    soc_start = soc_end = soc_gain = soc_drop = None\n",
    "    if soc_col:\n",
    "        soc_valid = seg[soc_col].dropna()\n",
    "        if not soc_valid.empty:\n",
    "            soc_start = soc_valid.iloc[0]\n",
    "            soc_end = soc_valid.iloc[-1]\n",
    "            soc_gain = max(soc_end - soc_start, 0)\n",
    "            soc_drop = max(soc_start - soc_end, 0)\n",
    "\n",
    "    # --- percentage metrics (time-weighted) ---\n",
    "    def pct(mask: pd.Series) -> float:\n",
    "        if total_time <= 0:\n",
    "            return 0.0\n",
    "        return round(100.0 * seg.loc[mask, \"dt\"].sum() / total_time, 2)\n",
    "\n",
    "    # charging / discharging % by current sign\n",
    "    charging_pct = pct(mask_chg)\n",
    "    discharging_pct = pct(mask_dis)\n",
    "\n",
    "    # motion_pct: vehicle_speed_vcu > SPEED_MOTION_THRESHOLD\n",
    "    if \"vehicle_speed_vcu\" in seg.columns:\n",
    "        speed = pd.to_numeric(seg[\"vehicle_speed_vcu\"], errors=\"coerce\")\n",
    "        motion_pct = pct(speed > SPEED_MOTION_THRESHOLD)\n",
    "    else:\n",
    "        motion_pct = np.nan\n",
    "\n",
    "    # lv_pct and off_pct\n",
    "    if all(col in seg.columns for col in [\"ignitionstatus\", \"gun_connection_status\", \"vehiclereadycondition\"]):\n",
    "        ign = pd.to_numeric(seg[\"ignitionstatus\"], errors=\"coerce\")\n",
    "        gun = pd.to_numeric(seg[\"gun_connection_status\"], errors=\"coerce\")\n",
    "        ready = pd.to_numeric(seg[\"vehiclereadycondition\"], errors=\"coerce\")\n",
    "\n",
    "        lv_mask = (ign == 1) & (gun == 0) & (ready == 0)\n",
    "        off_mask = (ign == 0) & (gun == 0) & (ready == 0)\n",
    "\n",
    "        lv_pct = pct(lv_mask)\n",
    "        off_pct = pct(off_mask)\n",
    "    else:\n",
    "        lv_pct = off_pct = np.nan\n",
    "\n",
    "    return {\n",
    "        \"id\": id_val,\n",
    "        \"reg_num\": reg_num,\n",
    "        \"customer\": customer,\n",
    "        \"model\": model,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"duration_mins\": duration_mins,\n",
    "        \"kwh_charging\": kwh_charging,\n",
    "        \"kwh_discharging\": kwh_discharging,\n",
    "        \"soc_start\": soc_start,\n",
    "        \"soc_end\": soc_end,\n",
    "        \"soc_gain\": soc_gain,\n",
    "        \"soc_drop\": soc_drop,\n",
    "        \"charging_pct\": charging_pct,\n",
    "        \"discharging_pct\": discharging_pct,\n",
    "        \"motion_pct\": motion_pct,\n",
    "        \"lv_pct\": lv_pct,\n",
    "        \"off_pct\": off_pct,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90dbe075-d83a-4c7d-9ecf-3d32225be2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_sessions_for_vehicle(df_vid: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Build stitched charging + discharging sessions for one vehicle,\n",
    "    using your original envelope logic but with 4-state activity and\n",
    "    micro-idle suppression.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "    if df_vid.empty:\n",
    "        return rows\n",
    "\n",
    "    df_vid = df_vid.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    df_vid = df_vid.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    \n",
    "    # STEP 1: activity must already exist\n",
    "    df_vid = add_activity_state(df_vid)\n",
    "    \n",
    "    # STEP 2: NOW apply smoothing to entire day\n",
    "    df_vid = fix_micro_idle_blips(df_vid, threshold_sec=60)\n",
    "\n",
    "    n = len(df_vid)\n",
    "    \n",
    "    # STEP 3: NOW detect charging envelopes\n",
    "    charging_sessions = _build_charging_sessions_for_vehicle(df_vid)\n",
    "    \n",
    "    # charging_sessions = _build_charging_sessions_for_vehicle(df_vid)\n",
    "    BATT_KWH = 423.96\n",
    "\n",
    "    def add_session(start_idx: int, end_idx: int):\n",
    "        \"\"\"Slice [start_idx, end_idx] â†’ metrics + activity.\"\"\"\n",
    "        if start_idx > end_idx or start_idx < 0 or end_idx >= n:\n",
    "            return\n",
    "\n",
    "        seg = df_vid.iloc[start_idx:end_idx + 1].copy()\n",
    "        if seg.empty:\n",
    "            return\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # Step 1: derive activity (your 4-state logic)\n",
    "        # ------------------------------------------\n",
    "        activity = None\n",
    "\n",
    "        # from alt_mode if available\n",
    "        if \"alt_mode\" in seg.columns:\n",
    "            am = seg[\"alt_mode\"].dropna().astype(str)\n",
    "            if not am.empty:\n",
    "                activity = am.value_counts().idxmax()\n",
    "\n",
    "        # derive gun_connected\n",
    "        if \"gun_connection_status\" in seg.columns:\n",
    "            g = pd.to_numeric(seg[\"gun_connection_status\"], errors=\"coerce\")\n",
    "            gun_connected = (g == 1).mean() > 0.5\n",
    "        else:\n",
    "            gun_connected = False\n",
    "\n",
    "        # derive mean_current\n",
    "        if \"total_battery_current\" in seg.columns:\n",
    "            cur = pd.to_numeric(seg[\"total_battery_current\"], errors=\"coerce\")\n",
    "            mean_cur = cur.mean(skipna=True)\n",
    "        else:\n",
    "            mean_cur = 0.0\n",
    "\n",
    "        # your updated rule:\n",
    "        #   <0A = CHARGING_ACTIVE\n",
    "        #   >0A = DISCHARGING_ACTIVE\n",
    "        #   otherwise IDLE depending on gun connection\n",
    "        if gun_connected:\n",
    "            if mean_cur < 0:\n",
    "                activity = \"CHARGING_ACTIVE\"\n",
    "            else:\n",
    "                activity = \"CHARGING_IDLE\"\n",
    "        else:\n",
    "            if mean_cur > 0:\n",
    "                activity = \"DISCHARGING_ACTIVE\"\n",
    "            else:\n",
    "                activity = \"DISCHARGING_IDLE\"\n",
    "\n",
    "        seg[\"activity\"] = activity\n",
    "\n",
    "        # recompute dominant activity after smoothing\n",
    "        activity = seg[\"activity\"].value_counts().idxmax()\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # Step 3: compute metrics\n",
    "        # ------------------------------------------\n",
    "        # Recompute dt_sec fresh for THIS segment ONLY\n",
    "        seg = seg.copy().sort_values(\"timestamp\").reset_index(drop=True)\n",
    "        seg[\"dt_sec\"] = seg[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "        seg[\"dt_sec\"] = seg[\"dt_sec\"].clip(lower=0)  # just in case\n",
    "        \n",
    "        metrics = _compute_session_metrics(seg)\n",
    "        if metrics is None:\n",
    "            return\n",
    "\n",
    "        # C-rate\n",
    "        charge_rate = 0.0\n",
    "        discharge_rate = 0.0\n",
    "\n",
    "        if \"bat_voltage\" in seg.columns and \"total_battery_current\" in seg.columns:\n",
    "            seg[\"power_kw\"] = (seg[\"bat_voltage\"] * seg[\"total_battery_current\"]) / 1000.0\n",
    "\n",
    "            chg = seg.loc[seg[\"power_kw\"] < 0, \"power_kw\"]\n",
    "            if not chg.empty:\n",
    "                charge_rate = abs(chg.mean()) / BATT_KWH\n",
    "\n",
    "            dch = seg.loc[seg[\"power_kw\"] > 0, \"power_kw\"]\n",
    "            if not dch.empty:\n",
    "                discharge_rate = dch.mean() / BATT_KWH\n",
    "\n",
    "        metrics[\"charge_rate\"] = round(charge_rate, 3)\n",
    "        metrics[\"discharge_rate\"] = round(discharge_rate, 3)\n",
    "\n",
    "        metrics[\"activity\"] = activity\n",
    "        rows.append(metrics)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Envelope stitching logic below remains same\n",
    "    # ------------------------------------------\n",
    "    if not charging_sessions:\n",
    "        add_session(0, n - 1)\n",
    "        return rows\n",
    "\n",
    "    first = charging_sessions[0]\n",
    "    if first[\"start_idx\"] > 0:\n",
    "        add_session(0, first[\"start_idx\"] - 1)\n",
    "\n",
    "    for i, chg in enumerate(charging_sessions):\n",
    "        add_session(chg[\"start_idx\"], chg[\"end_idx\"])\n",
    "\n",
    "        if i < len(charging_sessions) - 1:\n",
    "            nxt = charging_sessions[i + 1]\n",
    "            gap_start = chg[\"end_idx\"] + 1\n",
    "            gap_end = nxt[\"start_idx\"] - 1\n",
    "            if gap_start <= gap_end:\n",
    "                add_session(gap_start, gap_end)\n",
    "\n",
    "    last = charging_sessions[-1]\n",
    "    if last[\"end_idx\"] < n - 1:\n",
    "        add_session(last[\"end_idx\"] + 1, n - 1)\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2906c82a-6c47-46f8-afeb-1510f9247bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enrich_discharging_metrics(sessions_df: pd.DataFrame,\n",
    "#                                df_state: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Enrich per-session table with:\n",
    "#       - distance & kWh/km (for DISCHARGING_ACTIVE sessions)\n",
    "#       - speed stats (all sessions)\n",
    "#       - voltage delta stats (all sessions)\n",
    "#       - temp delta stats (all sessions)\n",
    "\n",
    "#     Assumes df_state has, at minimum:\n",
    "#       - id\n",
    "#       - session\n",
    "#       - timestamp_ist (or timestamp)\n",
    "#       - vehicle_speed_vcu\n",
    "#       - odometer_final (or odometerreading)\n",
    "#       - bat_voltage\n",
    "#       - total_battery_current\n",
    "#       - volt_delta_mv\n",
    "#       - batt_temp_delta\n",
    "#       - dt_sec  (if missing, we'll compute per (id, session))\n",
    "#     \"\"\"\n",
    "#     if sessions_df.empty:\n",
    "#         return sessions_df\n",
    "\n",
    "#     sess = sessions_df.copy()\n",
    "\n",
    "#     # Ensure dt_sec in df_state\n",
    "#     df = df_state.copy()\n",
    "#     if \"dt_sec\" not in df.columns:\n",
    "#         df = df.sort_values([\"id\", \"timestamp_ist\"])\n",
    "#         df[\"dt_sec\"] = (\n",
    "#             df.groupby(\"id\", observed=False)[\"timestamp_ist\"]\n",
    "#               .diff()\n",
    "#               .dt.total_seconds()\n",
    "#               .fillna(0)\n",
    "#         )\n",
    "\n",
    "#     # Make sure all enrichment columns exist\n",
    "#     enrich_cols = [\n",
    "#         \"net_odo_km\", \"dist_km_raw\", \"dist_km\", \"max_physical_km\",\n",
    "#         \"kwh_charging\", \"kwh_discharging\", \"energy_active_kwh\", \"kwh_per_km\",\n",
    "#         \"avg_speed\", \"med_speed\", \"max_speed\",\n",
    "#         \"avg_volt_delta_mv\", \"med_volt_delta_mv\",\n",
    "#         \"p95_volt_delta_mv\", \"max_volt_delta_mv\",\n",
    "#         \"avg_batt_temp_delta\", \"med_batt_temp_delta\",\n",
    "#         \"p95_batt_temp_delta\", \"max_batt_temp_delta\",\n",
    "#     ]\n",
    "#     for c in enrich_cols:\n",
    "#         if c not in sess.columns:\n",
    "#             sess[c] = np.nan\n",
    "\n",
    "#     # Group state df by (id, session)\n",
    "#     grp = df.groupby([\"id\", \"session\"], observed=False)\n",
    "\n",
    "#     BATT_KWH = 423.96  # for C-rate\n",
    "\n",
    "#     for idx, row in sess.iterrows():\n",
    "#         vid = row[\"id\"]\n",
    "#         sess_id = row[\"session\"]\n",
    "\n",
    "#         key = (vid, sess_id)\n",
    "#         if key not in grp.groups:\n",
    "#             continue\n",
    "\n",
    "#         seg = grp.get_group(key).copy()\n",
    "#         if seg.empty:\n",
    "#             continue\n",
    "\n",
    "#         # ---------------------------------------------------\n",
    "#         # 1) Speed stats (all sessions)\n",
    "#         # ---------------------------------------------------\n",
    "#         # v_all = pd.to_numeric(seg[\"vehicle_speed_vcu\"], errors=\"coerce\").dropna()\n",
    "\n",
    "#         # # \"Moving\" threshold to de-zero the median while stopped\n",
    "#         # v_mov = sorted(v_all[v_all > 0.1])\n",
    "\n",
    "#         # if not v_all.empty:\n",
    "#         #     avg_speed = float(v_all.mean())\n",
    "#         #     max_speed = float(v_all.max())\n",
    "#         # else:\n",
    "#         #     avg_speed = max_speed = 0.0\n",
    "\n",
    "#         # if not v_mov.empty:\n",
    "#         #     med_speed = float(v_mov.median())\n",
    "#         # else:\n",
    "#         #     med_speed = 0.0\n",
    "\n",
    "#         # sess.at[idx, \"avg_speed\"] = round(avg_speed, 2)\n",
    "#         # sess.at[idx, \"med_speed\"] = round(med_speed, 2)\n",
    "#         # sess.at[idx, \"max_speed\"] = round(max_speed, 2)\n",
    "#         v_all = chunk[\"vehicle_speed_vcu\"].dropna().astype(\"float64\")\n",
    "        \n",
    "#         if not v_all.empty:\n",
    "#             # 1) Average over all samples\n",
    "#             avg_speed = float(v_all.mean())\n",
    "        \n",
    "#             # 2) Median of *moving* speeds (> 0.1 km/h)\n",
    "#             v_mov = v_all[v_all > 0.1]\n",
    "        \n",
    "#             if not v_mov.empty:\n",
    "#                 med_speed = float(v_mov.median())\n",
    "#             else:\n",
    "#                 # fallback: everything was 0 or NaN â†’ median over all\n",
    "#                 med_speed = float(v_all.median())\n",
    "        \n",
    "#             # 3) Max speed as usual\n",
    "#             max_speed = float(v_all.max())\n",
    "#         else:\n",
    "#             avg_speed = med_speed = max_speed = 0.0\n",
    "        \n",
    "#         sess.at[idx, \"avg_speed\"] = round(avg_speed, 2)\n",
    "#         sess.at[idx, \"med_speed\"] = round(med_speed, 2)\n",
    "#         sess.at[idx, \"max_speed\"] = round(max_speed, 2)\n",
    "\n",
    "\n",
    "#         # ---------------------------------------------------\n",
    "#         # 2) Voltage delta stats (all sessions)\n",
    "#         # ---------------------------------------------------\n",
    "#         if \"volt_delta_mv\" in seg.columns:\n",
    "#             vd = pd.to_numeric(seg[\"volt_delta_mv\"], errors=\"coerce\").dropna()\n",
    "#         else:\n",
    "#             vd = pd.Series([], dtype=\"float64\")\n",
    "\n",
    "#         if not vd.empty:\n",
    "#             sess.at[idx, \"avg_volt_delta_mv\"] = float(vd.mean())\n",
    "#             sess.at[idx, \"med_volt_delta_mv\"] = float(vd.median())\n",
    "#             sess.at[idx, \"p95_volt_delta_mv\"] = float(vd.quantile(0.95))\n",
    "#             sess.at[idx, \"max_volt_delta_mv\"] = float(vd.max())\n",
    "\n",
    "#         # ---------------------------------------------------\n",
    "#         # 3) Battery temp delta stats (all sessions)\n",
    "#         # ---------------------------------------------------\n",
    "#         # adjust column name if you use something slightly different\n",
    "#         temp_col = \"batt_temp_delta\"\n",
    "#         if temp_col in seg.columns:\n",
    "#             td = pd.to_numeric(seg[temp_col], errors=\"coerce\").dropna()\n",
    "#         else:\n",
    "#             td = pd.Series([], dtype=\"float64\")\n",
    "\n",
    "#         if not td.empty:\n",
    "#             sess.at[idx, \"avg_batt_temp_delta\"] = float(td.mean())\n",
    "#             sess.at[idx, \"med_batt_temp_delta\"] = float(td.median())\n",
    "#             sess.at[idx, \"p95_batt_temp_delta\"] = float(td.quantile(0.95))\n",
    "#             sess.at[idx, \"max_batt_temp_delta\"] = float(td.max())\n",
    "\n",
    "#         # ---------------------------------------------------\n",
    "#         # 4) Distance & kWh/km (DISCHARGING_ACTIVE only)\n",
    "#         # ---------------------------------------------------\n",
    "#         if row[\"activity\"] != \"DISCHARGING_ACTIVE\":\n",
    "#             # keep distance / kWh fields as NaN or 0 (whatever initialized)\n",
    "#             continue\n",
    "\n",
    "#         # Odometer-based distance\n",
    "#         odo_col = \"odometer_final\" if \"odometer_final\" in seg.columns else \"odometerreading\"\n",
    "#         odo = pd.to_numeric(seg[odo_col], errors=\"coerce\").dropna()\n",
    "\n",
    "#         if not odo.empty:\n",
    "#             odo_start = float(odo.min())\n",
    "#             odo_end = float(odo.max())\n",
    "#             net_odo = max(0.0, odo_end - odo_start)  # enforce non-negative\n",
    "#         else:\n",
    "#             odo_start = odo_end = net_odo = 0.0\n",
    "\n",
    "#         sess.at[idx, \"odo_start\"] = odo_start\n",
    "#         sess.at[idx, \"odo_end\"] = odo_end\n",
    "#         sess.at[idx, \"net_odo_km\"] = net_odo\n",
    "\n",
    "#         # Raw distance candidate\n",
    "#         dist_km_raw = net_odo\n",
    "\n",
    "#         # Max physical distance\n",
    "#         duration_hr = row[\"duration_mins\"] / 60.0 if row[\"duration_mins\"] is not None else 0.0\n",
    "#         vmax = max_speed if max_speed is not None else 0.0\n",
    "#         # small safety margin on vmax\n",
    "#         max_phys = vmax * duration_hr * 1.1 if duration_hr > 0 else 0.0\n",
    "\n",
    "#         # Clip distance to physics\n",
    "#         if dist_km_raw < 0:\n",
    "#             dist_km = 0.0\n",
    "#         elif max_phys > 0 and dist_km_raw > max_phys:\n",
    "#             dist_km = max_phys\n",
    "#         else:\n",
    "#             dist_km = dist_km_raw\n",
    "\n",
    "#         sess.at[idx, \"dist_km_raw\"] = dist_km_raw\n",
    "#         sess.at[idx, \"dist_km\"] = dist_km\n",
    "#         sess.at[idx, \"max_physical_km\"] = max_phys\n",
    "\n",
    "#         # ---------------------------------------------------\n",
    "#         # 5) Energy counters + kWh/km for DISCHARGING_ACTIVE\n",
    "#         # ---------------------------------------------------\n",
    "#         # power_kw = V * I / 1000\n",
    "#         if (\"bat_voltage\" in seg.columns) and (\"total_battery_current\" in seg.columns):\n",
    "#             vbat = pd.to_numeric(seg[\"bat_voltage\"], errors=\"coerce\")\n",
    "#             cur = pd.to_numeric(seg[\"total_battery_current\"], errors=\"coerce\")\n",
    "#             dt_sec = pd.to_numeric(seg[\"dt_sec\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "#             seg[\"power_kw\"] = (vbat * cur) / 1000.0\n",
    "\n",
    "#             # charging: power_kw < 0\n",
    "#             chg = seg.loc[seg[\"power_kw\"] < 0].copy()\n",
    "#             dchg = seg.loc[seg[\"power_kw\"] > 0].copy()\n",
    "\n",
    "#             # energy in kWh = âˆ‘ (P_kW * dt_sec / 3600)\n",
    "#             kwh_ch = 0.0\n",
    "#             kwh_dch = 0.0\n",
    "\n",
    "#             if not chg.empty:\n",
    "#                 kwh_ch = float((chg[\"power_kw\"] * chg[\"dt_sec\"] / 3600.0).sum())\n",
    "\n",
    "#             if not dchg.empty:\n",
    "#                 kwh_dch = float((dchg[\"power_kw\"] * dchg[\"dt_sec\"] / 3600.0).sum())\n",
    "\n",
    "#             energy_active_kwh = abs(kwh_dch)  # main interest is discharge side\n",
    "\n",
    "#             sess.at[idx, \"kwh_charging\"] = kwh_ch\n",
    "#             sess.at[idx, \"kwh_discharging\"] = kwh_dch\n",
    "#             sess.at[idx, \"energy_active_kwh\"] = energy_active_kwh\n",
    "\n",
    "#             if dist_km > 0:\n",
    "#                 sess.at[idx, \"kwh_per_km\"] = energy_active_kwh / dist_km\n",
    "#             else:\n",
    "#                 sess.at[idx, \"kwh_per_km\"] = 0 \n",
    "#             # C-rate style view\n",
    "#             if energy_active_kwh > 0 and duration_hr > 0:\n",
    "#                 # avg_kW = energy / time (kWh / h)\n",
    "#                 avg_dch_kw = energy_active_kwh / duration_hr\n",
    "#                 discharge_rate = avg_dch_kw / BATT_KWH\n",
    "#                 sess.at[idx, \"discharge_rate\"] = round(discharge_rate, 3)\n",
    "\n",
    "#     return sess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0988a3ec-35dc-4a02-b98f-c494f5084d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_micro_idle_blips(df: pd.DataFrame, threshold_sec: float = 60.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Smooth out micro-blips in activity classification.\n",
    "    \n",
    "    If there is a short segment of CHARGING_IDLE or DISCHARGING_IDLE\n",
    "    sandwiched between two segments that belong to the same DOMAIN \n",
    "    (charging or discharging), and its duration is below threshold_sec,\n",
    "    then rewrite its activity to match the surrounding domain.\n",
    "\n",
    "    This restores continuous charging/discharging sessions and prevents\n",
    "    fragmentation such as:\n",
    "        CH_ACTIVE â†’ DIS_IDLE(20s) â†’ CH_ACTIVE.\n",
    "    \"\"\"\n",
    "    if \"activity\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    out = df.copy()\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Utility: determine domain\n",
    "    # ------------------------------------------------------------------\n",
    "    def domain(act: str):\n",
    "        if act.startswith(\"CHARGING\"):\n",
    "            return \"CH\"\n",
    "        elif act.startswith(\"DISCHARGING\"):\n",
    "            return \"DIS\"\n",
    "        return None\n",
    "\n",
    "    acts = out[\"activity\"].astype(str).values\n",
    "    ts = out[\"timestamp\"].values\n",
    "    n = len(out)\n",
    "\n",
    "    # compute row-wise durations\n",
    "    dt = np.zeros(n)\n",
    "    dt[1:] = (out[\"timestamp\"].iloc[1:].values - out[\"timestamp\"].iloc[:-1].values).astype(\"timedelta64[s]\").astype(float)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Scan for 3-point patterns: X â†’ idle-blip â†’ X\n",
    "    # but broaden to domain-aware detection\n",
    "    # ------------------------------------------------------------------\n",
    "    for i in range(1, n - 1):\n",
    "        a_prev = acts[i-1]\n",
    "        a_mid  = acts[i]\n",
    "        a_next = acts[i+1]\n",
    "\n",
    "        d_prev = domain(a_prev)\n",
    "        d_mid  = domain(a_mid)\n",
    "        d_next = domain(a_next)\n",
    "\n",
    "        # must be same domain before & after\n",
    "        if d_prev is None or d_next is None:\n",
    "            continue\n",
    "        if d_prev != d_next:\n",
    "            continue\n",
    "\n",
    "        # middle must be micro-blip AND opposite domain\n",
    "        if d_mid == d_prev:\n",
    "            continue  # not a blip\n",
    "\n",
    "        # micro duration?\n",
    "        if dt[i] <= threshold_sec:\n",
    "            # rewrite the blip to match surrounding domain\n",
    "            if d_prev == \"CH\":\n",
    "                acts[i] = \"CHARGING_ACTIVE\"\n",
    "            else:\n",
    "                acts[i] = \"DISCHARGING_ACTIVE\"\n",
    "\n",
    "    out[\"activity\"] = acts\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ce37537-a8b4-4def-8c5c-6f2032cee221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_missing_info_sessions(\n",
    "#     sessions: pd.DataFrame,\n",
    "#     min_gap_mins: float = 2.0,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     For each vehicle, detect gaps between consecutive sessions and\n",
    "#     insert a synthetic 'MISSING_INFO' session when the gap exceeds\n",
    "#     min_gap_mins.\n",
    "\n",
    "#     The missing-info session:\n",
    "#       - activity = 'MISSING_INFO'\n",
    "#       - start_time = previous end_time\n",
    "#       - end_time   = next start_time\n",
    "#       - duration_mins = gap in minutes\n",
    "#       - most other metrics left as NaN\n",
    "#     \"\"\"\n",
    "#     if sessions.empty:\n",
    "#         return sessions\n",
    "\n",
    "#     new_rows = []\n",
    "\n",
    "#     # Work per-vehicle\n",
    "#     for vid, grp in sessions.groupby(\"id\"):\n",
    "#         grp = grp.sort_values(\"start_time\")\n",
    "#         for i in range(len(grp) - 1):\n",
    "#             row_cur = grp.iloc[i]\n",
    "#             row_next = grp.iloc[i + 1]\n",
    "\n",
    "#             gap_sec = (row_next[\"start_time\"] - row_cur[\"end_time\"]).total_seconds()\n",
    "#             gap_mins = gap_sec / 60.0\n",
    "\n",
    "#             if gap_mins >= min_gap_mins:\n",
    "#                 new_rows.append({\n",
    "#                     \"id\": row_cur[\"id\"],\n",
    "#                     \"reg_num\": row_cur.get(\"reg_num\", None),\n",
    "#                     \"customer\": row_cur.get(\"customer\", None),\n",
    "#                     \"model\": row_cur.get(\"model\", None),\n",
    "#                     \"activity\": \"MISSING_INFO\",\n",
    "#                     \"start_time\": row_cur[\"end_time\"],\n",
    "#                     \"end_time\": row_next[\"start_time\"],\n",
    "#                     \"duration_mins\": round(gap_mins, 2),\n",
    "\n",
    "#                     # leave energy and SOC as NaN (unknown)\n",
    "#                     \"kwh_charging\": np.nan,\n",
    "#                     \"kwh_discharging\": np.nan,\n",
    "#                     \"charge_rate\": np.nan,\n",
    "#                     \"discharge_rate\": np.nan,\n",
    "#                     \"soc_start\": np.nan,\n",
    "#                     \"soc_end\": np.nan,\n",
    "#                     \"soc_gain\": np.nan,\n",
    "#                     \"soc_drop\": np.nan,\n",
    "#                     \"charging_pct\": np.nan,\n",
    "#                     \"discharging_pct\": np.nan,\n",
    "#                     \"motion_pct\": np.nan,\n",
    "#                     \"lv_pct\": np.nan,\n",
    "#                     \"off_pct\": np.nan,\n",
    "#                     \"glitch_flag\": False,\n",
    "#                     \"glitch_reason\": \"MISSING_INFO_GAP\",\n",
    "#                 })\n",
    "\n",
    "#     if not new_rows:\n",
    "#         return sessions\n",
    "\n",
    "#     sessions = pd.concat([sessions, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "#     return sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "859b1da1-936d-4fee-b5ac-a1e7753c6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _append_missing_segment(\n",
    "    ref_row: pd.Series,\n",
    "    gap_start: pd.Timestamp,\n",
    "    gap_end: pd.Timestamp,\n",
    "    df_day: pd.DataFrame,\n",
    "    min_gap_mins: float,\n",
    "    reason: str,\n",
    "    new_rows: list,\n",
    "):\n",
    "    \"\"\"Helper: build a MISSING_INFO session for one gap.\"\"\"\n",
    "    gap_sec = (gap_end - gap_start).total_seconds()\n",
    "    gap_mins = gap_sec / 60.0\n",
    "    if gap_mins < min_gap_mins:\n",
    "        return\n",
    "\n",
    "    # choose the right timestamp column (IST preferred)\n",
    "    ts_col = \"timestamp_ist\" if \"timestamp_ist\" in df_day.columns else \"timestamp\"\n",
    "\n",
    "    seg = df_day[\n",
    "        (df_day[\"id\"] == ref_row[\"id\"])\n",
    "        & (df_day[ts_col] >= gap_start)\n",
    "        & (df_day[ts_col] < gap_end)\n",
    "    ].copy()\n",
    "\n",
    "    # CASE 1: true blackout â†’ no rows inside the gap\n",
    "    # we only know duration; everything else stays NaN\n",
    "    if seg.empty:\n",
    "        new_rows.append({\n",
    "            \"id\": ref_row[\"id\"],\n",
    "            \"reg_num\": ref_row.get(\"reg_num\", None),\n",
    "            \"customer\": ref_row.get(\"customer\", None),\n",
    "            \"model\": ref_row.get(\"model\", None),\n",
    "            \"activity\": \"MISSING_INFO\",\n",
    "            \"start_time\": gap_start,\n",
    "            \"end_time\": gap_end,\n",
    "            \"duration_mins\": round(gap_mins, 2),\n",
    "            # top-level metrics left NaN; bucket % will also be NaN because\n",
    "            # no rows will carry this session id\n",
    "            \"kwh_charging\": np.nan,\n",
    "            \"kwh_discharging\": np.nan,\n",
    "            \"charge_rate\": np.nan,\n",
    "            \"discharge_rate\": np.nan,\n",
    "            \"soc_start\": np.nan,\n",
    "            \"soc_end\": np.nan,\n",
    "            \"soc_gain\": np.nan,\n",
    "            \"soc_drop\": np.nan,\n",
    "            \"charging_pct\": np.nan,\n",
    "            \"discharging_pct\": np.nan,\n",
    "            \"motion_pct\": np.nan,\n",
    "            \"lv_pct\": np.nan,\n",
    "            \"off_pct\": np.nan,\n",
    "            \"glitch_flag\": False,\n",
    "            \"glitch_reason\": reason,\n",
    "        })\n",
    "        return\n",
    "\n",
    "    # CASE 2: there *are* rows â†’ compute full metrics from seg\n",
    "    metrics = _compute_session_metrics(seg)\n",
    "    if metrics is None:\n",
    "        return\n",
    "\n",
    "    # override / fill fields\n",
    "    metrics[\"id\"] = ref_row[\"id\"]\n",
    "    metrics[\"reg_num\"] = ref_row.get(\"reg_num\", None)\n",
    "    metrics[\"customer\"] = ref_row.get(\"customer\", None)\n",
    "    metrics[\"model\"] = ref_row.get(\"model\", None)\n",
    "\n",
    "    # keep the time window aligned to the gap we defined\n",
    "    metrics[\"start_time\"] = gap_start\n",
    "    metrics[\"end_time\"] = gap_end\n",
    "    metrics[\"duration_mins\"] = round(gap_mins, 2)\n",
    "\n",
    "    metrics[\"activity\"] = \"MISSING_INFO\"\n",
    "    metrics[\"glitch_flag\"] = False\n",
    "    metrics[\"glitch_reason\"] = reason\n",
    "\n",
    "    new_rows.append(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d0822f4-c18d-4260-9447-74f6381881da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_missing_info_sessions_for_day(\n",
    "    sessions: pd.DataFrame,\n",
    "    df_day: pd.DataFrame,\n",
    "    min_gap_mins: float = 2.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each vehicle + day, detect gaps and insert MISSING_INFO sessions.\n",
    "\n",
    "    - Before first session: [midnight â†’ first.start_time)\n",
    "    - Between sessions:      [end_time_i â†’ start_time_{i+1})\n",
    "    - After last session:    [last.end_time â†’ next midnight)\n",
    "\n",
    "    If there are rows in df_day inside a gap, we compute full metrics\n",
    "    from those rows; otherwise only duration is known.\n",
    "    \"\"\"\n",
    "    if sessions.empty:\n",
    "        return sessions\n",
    "\n",
    "    # infer local day boundaries from df_day\n",
    "    if \"timestamp_ist\" in df_day.columns:\n",
    "        ts = df_day[\"timestamp_ist\"]\n",
    "    else:\n",
    "        ts = df_day[\"timestamp\"]\n",
    "    day_date = ts.dt.date.min()\n",
    "    day_start = pd.to_datetime(str(day_date))\n",
    "    day_end = day_start + pd.Timedelta(days=1)\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for vid, grp in sessions.groupby(\"id\"):\n",
    "        grp = grp.sort_values(\"start_time\")\n",
    "\n",
    "        # pre-first gap\n",
    "        first = grp.iloc[0]\n",
    "        _append_missing_segment(\n",
    "            ref_row=first,\n",
    "            gap_start=day_start,\n",
    "            gap_end=first[\"start_time\"],\n",
    "            df_day=df_day,\n",
    "            min_gap_mins=min_gap_mins,\n",
    "            reason=\"MISSING_INFO_EDGE_START\",\n",
    "            new_rows=new_rows,\n",
    "        )\n",
    "\n",
    "        # internal gaps\n",
    "        for i in range(len(grp) - 1):\n",
    "            cur = grp.iloc[i]\n",
    "            nxt = grp.iloc[i + 1]\n",
    "            _append_missing_segment(\n",
    "                ref_row=cur,\n",
    "                gap_start=cur[\"end_time\"],\n",
    "                gap_end=nxt[\"start_time\"],\n",
    "                df_day=df_day,\n",
    "                min_gap_mins=min_gap_mins,\n",
    "                reason=\"MISSING_INFO_GAP\",\n",
    "                new_rows=new_rows,\n",
    "            )\n",
    "\n",
    "        # post-last gap\n",
    "        last = grp.iloc[-1]\n",
    "        _append_missing_segment(\n",
    "            ref_row=last,\n",
    "            gap_start=last[\"end_time\"],\n",
    "            gap_end=day_end,\n",
    "            df_day=df_day,\n",
    "            min_gap_mins=min_gap_mins,\n",
    "            reason=\"MISSING_INFO_EDGE_END\",\n",
    "            new_rows=new_rows,\n",
    "        )\n",
    "\n",
    "    if not new_rows:\n",
    "        return sessions\n",
    "\n",
    "    sessions = pd.concat([sessions, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    return sessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d277da2f-58c2-4678-8d17-88e4fa4b2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_charging_and_discharging_sessions(df_day: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build unified CHARGING / DISCHARGING sessions for a day's data.\n",
    "    Also adds bucket distributions and SOC stats.\n",
    "\n",
    "    Output columns:\n",
    "        id, reg_num, customer, model, activity, session, date,\n",
    "        start_time, end_time, duration_mins,\n",
    "        charging_pct, discharging_pct, motion_pct,\n",
    "        lv_pct, off_pct,\n",
    "        kwh_charging, kwh_discharging,\n",
    "        soc_start, soc_end, soc_gain, soc_drop,\n",
    "        ... + bucket percentage columns\n",
    "    \"\"\"\n",
    "    if df_day.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 1 â€” Build sessions for each vehicle\n",
    "    # -----------------------------------------------------------\n",
    "    # all_rows: list[dict] = []\n",
    "    # for vid, df_vid in df_day.groupby(\"id\"):\n",
    "    #     vid_rows = _build_sessions_for_vehicle(df_vid)\n",
    "    #     all_rows.extend(vid_rows)\n",
    "\n",
    "    # if not all_rows:\n",
    "    #     return pd.DataFrame()\n",
    "\n",
    "    # sessions = pd.DataFrame(all_rows)\n",
    "    all_rows: list[dict] = []\n",
    "    for vid, df_vid in df_day.groupby(\"id\"):\n",
    "        vid_rows = _build_sessions_for_vehicle(df_vid)\n",
    "        all_rows.extend(vid_rows)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sessions = pd.DataFrame(all_rows)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1B â€” Drop micro-sessions (< 2 min) for real activity\n",
    "    # -----------------------------------------------------------\n",
    "    if \"duration_mins\" in sessions.columns:\n",
    "        sessions = sessions[sessions[\"duration_mins\"] >= 2.0].copy()\n",
    "    if sessions.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1C â€” Insert MISSING_INFO for day edges + internal gaps\n",
    "    # -----------------------------------------------------------\n",
    "    sessions = insert_missing_info_sessions_for_day(sessions=sessions,df_day=df_day,min_gap_mins=2.0,)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2 â€” Sort & assign per-vehicle session number\n",
    "    # -----------------------------------------------------------\n",
    "    sessions = sessions.sort_values([\"id\", \"start_time\"]).reset_index(drop=True)\n",
    "    sessions[\"session\"] = sessions.groupby(\"id\").cumcount() + 1\n",
    "    sessions[\"date\"] = sessions[\"start_time\"].dt.date\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 2B â€” SOC DROP GLITCH DETECTION (unchanged)\n",
    "    # -----------------------------------------------------------\n",
    "    THRESH_ACTIVE = 0.0    # no drop allowed during active charging\n",
    "    THRESH_MAINT  = 0.3    # small balancing jitter allowed\n",
    "    THRESH_IDLE   = 0.5    # small taper jitter allowed\n",
    "\n",
    "    if \"activity\" in sessions.columns:\n",
    "        sessions[\"glitch_flag\"] = False\n",
    "        sessions[\"glitch_reason\"] = \"\"\n",
    "\n",
    "        for idx, row in sessions.iterrows():\n",
    "            mode = row[\"activity\"]\n",
    "            if not isinstance(mode, str):\n",
    "                continue\n",
    "\n",
    "            # only apply to charging modes\n",
    "            if not mode.startswith(\"CHARGING\"):\n",
    "                continue\n",
    "\n",
    "            soc_drop = row.get(\"soc_drop\", 0) or 0\n",
    "\n",
    "            # CHARGING_ACTIVE â€“ absolutely no SOC drop expected\n",
    "            if mode == \"CHARGING_ACTIVE\" and soc_drop > THRESH_ACTIVE:\n",
    "                sessions.at[idx, \"glitch_flag\"] = True\n",
    "                sessions.at[idx, \"glitch_reason\"] = (\n",
    "                    f\"SOC dropped {soc_drop:.2f}% during CHARGING_ACTIVE\"\n",
    "                )\n",
    "                sessions.at[idx, \"activity\"] = \"GLITCH\"\n",
    "                continue\n",
    "\n",
    "            # CHARGING_MAINTAIN â€“ small drop allowed\n",
    "            if mode == \"CHARGING_MAINTAIN\" and soc_drop > THRESH_MAINT:\n",
    "                sessions.at[idx, \"glitch_flag\"] = True\n",
    "                sessions.at[idx, \"glitch_reason\"] = (\n",
    "                    f\"SOC dropped {soc_drop:.2f}% during CHARGING_MAINTAIN\"\n",
    "                )\n",
    "                sessions.at[idx, \"activity\"] = \"GLITCH\"\n",
    "                continue\n",
    "\n",
    "            # CHARGING_IDLE â€“ small jitter allowed\n",
    "            if mode == \"CHARGING_IDLE\" and soc_drop > THRESH_IDLE:\n",
    "                sessions.at[idx, \"glitch_flag\"] = True\n",
    "                sessions.at[idx, \"glitch_reason\"] = (\n",
    "                    f\"SOC dropped {soc_drop:.2f}% during CHARGING_IDLE\"\n",
    "                )\n",
    "                sessions.at[idx, \"activity\"] = \"GLITCH\"\n",
    "                continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 3 â€” Push 'session' back into raw df_day rows\n",
    "    # -----------------------------------------------------------\n",
    "    df_day = df_day.copy()\n",
    "    df_day[\"session\"] = None\n",
    "\n",
    "    for ses in sessions.itertuples(index=False):\n",
    "        mask = (\n",
    "            (df_day[\"id\"] == ses.id) &\n",
    "            (df_day[\"timestamp\"] >= ses.start_time) &\n",
    "            (df_day[\"timestamp\"] <= ses.end_time)\n",
    "        )\n",
    "        df_day.loc[mask, \"session\"] = ses.session\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 4 â€” Bucket distributions (TEMP, VOLT, SOC)\n",
    "    # -----------------------------------------------------------\n",
    "    BUCKET_MAP = {\n",
    "        'maxtemp_bucket': [\"<28\", \"28â€“32\", \"32â€“35\", \"35â€“40\", \">40\"],\n",
    "        'temp_delta_bucket': [\"<2\", \"2â€“5\", \"5â€“8\", \">8\"],\n",
    "        'volt_delta_bucket': [\"0â€“10\", \"10â€“20\", \"20â€“30\", \">30\"],\n",
    "        'soc_band_bucket': [\n",
    "            \"0â€“10\",\"10â€“20\",\"20â€“30\",\"30â€“40\",\"40â€“50\",\n",
    "            \"50â€“60\",\"60â€“70\",\"70â€“80\",\"80â€“90\",\"90â€“100\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for col, categories in BUCKET_MAP.items():\n",
    "        if col not in df_day.columns:\n",
    "            continue\n",
    "\n",
    "        # normalize category labels\n",
    "        df_day[col] = df_day[col].astype(str).str.replace(\"-\", \"â€“\")\n",
    "        df_day[col] = pd.Categorical(df_day[col], categories=categories, ordered=True)\n",
    "\n",
    "        # per-vehicle, per-session distribution\n",
    "        pct = (\n",
    "            df_day\n",
    "            .dropna(subset=[\"session\"])\n",
    "            .groupby([\"id\", \"session\"])[col]\n",
    "            .value_counts(normalize=True)\n",
    "            .mul(100)\n",
    "            .round(2)\n",
    "        )\n",
    "\n",
    "        pct_pivot = pct.unstack(fill_value=0)\n",
    "\n",
    "        pct_pivot.columns = [\n",
    "            f\"{col}_{str(c).replace('â€“','_').replace('<','lt').replace('>','gt')}_pct\"\n",
    "            for c in pct_pivot.columns\n",
    "        ]\n",
    "\n",
    "        pct_pivot = pct_pivot.reset_index()\n",
    "        sessions = sessions.merge(pct_pivot, on=[\"id\", \"session\"], how=\"left\")\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 5 â€” Final column ordering\n",
    "    # -----------------------------------------------------------\n",
    "    sessions[\"charge_rate\"] = sessions[\"charge_rate\"].fillna(0);\n",
    "    sessions[\"discharge_rate\"] = sessions[\"discharge_rate\"].fillna(0);\n",
    "    ordered_cols = [\n",
    "        \"id\", \"reg_num\", \"customer\", \"model\",\n",
    "        \"activity\", \"session\", \"date\",\n",
    "        \"start_time\", \"end_time\", \"duration_mins\",\n",
    "        \"charging_pct\", \"discharging_pct\", \"motion_pct\",\n",
    "        \"lv_pct\", \"off_pct\",\n",
    "        \"kwh_charging\", \"kwh_discharging\", \"charge_rate\", \"discharge_rate\",\n",
    "        \"soc_start\", \"soc_end\", \"soc_gain\", \"soc_drop\",\n",
    "    ]\n",
    "\n",
    "    ordered_cols += [c for c in sessions.columns if c not in ordered_cols]\n",
    "\n",
    "    return sessions[ordered_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac61a82-c761-4ae1-ae18-69ef010ca323",
   "metadata": {},
   "source": [
    "## ðŸ“ Distance Column Semantics\n",
    "\n",
    "This table defines the meaning, computation, and role of the primary distance columns used in session data.\n",
    "\n",
    "| Column | Meaning | Computation & Derivation | Role & Semantics |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **`net_odo_km`** | **Pure Odometer Span** within the session. | Computed as: $$\\text{net\\_odo\\_km} = \\text{odo\\_end} - \\text{odo\\_start}$$ Uses the **cleaned/imputed** odometer values ($\\text{odo\\_}^{*}$ columns). | Represents the distance reported by the vehicle's odometer **before** any physics-based sanity checks or corrections. |\n",
    "| **`dist_km_raw`** | **Initial Distance Candidate** (Pre-Guardrail). | Usually starts as: $\\text{dist\\_km\\_raw} \\approx \\text{net\\_odo\\_km}$. May incorporate: speed-based integration ($\\sum v \\cdot dt$), wraparound handling, or other corrections. | **Still \"pre-guardrail\"**â€”it is a raw measurement/initial estimate, not yet clipped against physical plausibility. |\n",
    "| **`max_physical_km`** | **Physics-Based Upper Bound** (Sanity Ceiling). | Approximate formula: $$\\text{max\\_physical\\_km} \\approx \\text{vmax\\_kmph} \\times \\left(\\frac{\\text{duration\\_mins}}{60}\\right) \\times \\text{margin}$$ **Not a measurement**, but a computed **sanity ceiling**. | If $\\text{net\\_odo\\_km}$ or $\\text{dist\\_km\\_raw}$ exceed this value, the data is **physically implausible** and should be clipped and/or flagged as a glitch. |\n",
    "| **`dist_km`** | **Final, Trusted Session Distance** (Canonical). | Derived from $\\text{dist\\_km\\_raw}$ with rules like: **enforce non-negativity**, and **clip to $\\text{max\\_physical\\_km}$** when necessary. | This is the **canonical distance column** to use for: * Reporting * $\\text{kWh/km}$ calculations * Fleet-level analytics and KPIs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e0905e9-ee6d-429f-8650-8b565939ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enrich_discharging_metrics(\n",
    "    session_df: pd.DataFrame,\n",
    "    raw_df: pd.DataFrame,\n",
    "    max_kmph_for_physics: float = 120.0,\n",
    "    physics_tolerance: float = 1.3,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Option 3: physics-aware, GLITCH-capable enrichment.\n",
    "\n",
    "    Adds per-session:\n",
    "      - dist_km (final, physics-sanitised)\n",
    "      - avg_speed, med_speed, max_speed\n",
    "      - avg/med/max/p95 volt_delta_mv\n",
    "      - avg/med/max/p95 batt_temp_delta\n",
    "      - energy_active_kwh, kwh_per_km\n",
    "      - odo_start, odo_end, net_odo_km\n",
    "      - dist_km_raw (pre-physics cumulative diffs)\n",
    "      - max_physical_km\n",
    "      - glitch_flag (bool) + glitch_reason (text)\n",
    "\n",
    "    If session_df has an 'activity' column, GLITCH sessions get\n",
    "    activity=\"GLITCH\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure time alignment\n",
    "    raw_df = raw_df.sort_values([\"id\", \"timestamp\"]).copy()\n",
    "\n",
    "    # --- Holder columns (base metrics) ---\n",
    "    session_df[\"dist_km\"] = 0.0\n",
    "    session_df[\"avg_speed\"] = 0.0\n",
    "    session_df[\"med_speed\"] = 0.0\n",
    "    session_df[\"max_speed\"] = 0.0\n",
    "\n",
    "    session_df[\"avg_volt_delta_mv\"] = 0.0\n",
    "    session_df[\"med_volt_delta_mv\"] = 0.0\n",
    "    session_df[\"max_volt_delta_mv\"] = 0.0\n",
    "    session_df[\"p95_volt_delta_mv\"] = 0.0\n",
    "\n",
    "    session_df[\"avg_batt_temp_delta\"] = 0.0\n",
    "    session_df[\"med_batt_temp_delta\"] = 0.0\n",
    "    session_df[\"max_batt_temp_delta\"] = 0.0\n",
    "    session_df[\"p95_batt_temp_delta\"] = 0.0\n",
    "\n",
    "    session_df[\"energy_active_kwh\"] = 0.0\n",
    "    session_df[\"kwh_per_km\"] = 0.0\n",
    "    session_df[\"net_kwh_per_km\"] = 0.0    \n",
    "\n",
    "    # --- NEW diagnostic / physics fields ---\n",
    "    session_df[\"odo_start\"] = np.nan\n",
    "    session_df[\"odo_end\"]   = np.nan\n",
    "    session_df[\"net_odo_km\"] = 0.0      # odo_end - odo_start (clamped â‰¥ 0)\n",
    "    session_df[\"dist_km_raw\"] = 0.0     # sum of positive diffs before physics clamp\n",
    "    session_df[\"max_physical_km\"] = 0.0\n",
    "\n",
    "    session_df[\"glitch_flag\"] = False\n",
    "    session_df[\"glitch_reason\"] = \"\"\n",
    "\n",
    "    has_activity_col = \"activity\" in session_df.columns\n",
    "\n",
    "    # --- Loop through each session ---\n",
    "    for idx, row in session_df.iterrows():\n",
    "        vid = row[\"id\"]\n",
    "        t1  = row[\"start_time\"]\n",
    "        t2  = row[\"end_time\"]\n",
    "\n",
    "        # Keep your original intent: only DISCHARGING_ACTIVE are \"drive\" sessions\n",
    "        # if has_activity_col and row[\"activity\"] != \"DISCHARGING_ACTIVE\":\n",
    "        #     continue\n",
    "\n",
    "        mask = (\n",
    "            (raw_df[\"id\"] == vid) &\n",
    "            (raw_df[\"timestamp\"] >= t1) &\n",
    "            (raw_df[\"timestamp\"] <= t2)\n",
    "        )\n",
    "        chunk = raw_df[mask].copy()\n",
    "\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        # -------------------------------\n",
    "        # 0. Choose odometer source\n",
    "        # -------------------------------\n",
    "        if \"odometer_final\" in chunk.columns:\n",
    "            odo_series = chunk[\"odometer_final\"].astype(\"float64\")\n",
    "        # else:\n",
    "        #     odo_series = chunk[\"odometerreading\"].astype(\"float64\")\n",
    "\n",
    "        odo_series = odo_series.dropna()\n",
    "        if odo_series.empty:\n",
    "            # No odo â†’ skip distance & energy, but still do volt/temp/speed\n",
    "            odo_start = np.nan\n",
    "            odo_end = np.nan\n",
    "            net_odo = 0.0\n",
    "        else:\n",
    "            odo_start = float(odo_series.iloc[0])\n",
    "            odo_end   = float(odo_series.iloc[-1])\n",
    "            net_odo   = max(odo_end - odo_start, 0.0)\n",
    "\n",
    "        session_df.at[idx, \"odo_start\"] = odo_start\n",
    "        session_df.at[idx, \"odo_end\"]   = odo_end\n",
    "        session_df.at[idx, \"net_odo_km\"] = net_odo\n",
    "\n",
    "        # -------------------------------\n",
    "        # 1. Distance via forward-only diffs (raw)\n",
    "        # -------------------------------\n",
    "        if \"odometer_final\" in chunk.columns:\n",
    "            odo_full = chunk[\"odometer_final\"].astype(\"float64\")\n",
    "        # else:\n",
    "        #     odo_full = chunk[\"odometerreading\"].astype(\"float64\")\n",
    "\n",
    "        odo_diff = odo_full.diff()\n",
    "        # Keep only strictly positive increments\n",
    "        dist_km_raw = odo_diff[odo_diff > 0].sum(skipna=True)\n",
    "        if pd.isna(dist_km_raw):\n",
    "            dist_km_raw = 0.0\n",
    "        session_df.at[idx, \"dist_km_raw\"] = float(dist_km_raw)\n",
    "        \n",
    "        # --------------------------------------------------------------\n",
    "        # 2) Speed stats â€“ for ALL sessions\n",
    "        # --------------------------------------------------------------\n",
    "        v_all = chunk[\"vehicle_speed_vcu\"].astype(\"float64\")\n",
    "        v_all = v_all.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "        if not v_all.empty:\n",
    "            # 1) Average over all samples (including stops)\n",
    "            avg_speed = float(v_all.mean())\n",
    "    \n",
    "            # 2) Median of *moving* speeds (>0.1 km/h); fall back to global median\n",
    "            v_mov = v_all[v_all > 0.1]\n",
    "            if not v_mov.empty:\n",
    "                med_speed = float(v_mov.median())\n",
    "            else:\n",
    "                med_speed = float(v_all.median())\n",
    "    \n",
    "            # 3) Max speed\n",
    "            max_speed = float(v_all.max())\n",
    "        else:\n",
    "            avg_speed = med_speed = max_speed = 0.0\n",
    "    \n",
    "        session_df.at[idx, \"avg_speed\"] = round(avg_speed, 2)\n",
    "        session_df.at[idx, \"med_speed\"] = round(med_speed, 2)\n",
    "        session_df.at[idx, \"max_speed\"] = round(max_speed, 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3. Voltage delta stats\n",
    "        # -------------------------------\n",
    "        vd = chunk[\"volt_delta_mv\"].dropna()\n",
    "        if not vd.empty:\n",
    "            session_df.at[idx, \"avg_volt_delta_mv\"] = round(float(vd.mean()), 2)\n",
    "            session_df.at[idx, \"med_volt_delta_mv\"] = round(float(vd.median()), 2)\n",
    "            session_df.at[idx, \"max_volt_delta_mv\"] = round(float(vd.max()),2)\n",
    "            session_df.at[idx, \"p95_volt_delta_mv\"] = round(float(vd.quantile(0.95)), 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 4. Temperature delta stats\n",
    "        # -------------------------------\n",
    "        td = chunk[\"batt_temp_delta\"].dropna()\n",
    "        if not td.empty:\n",
    "            session_df.at[idx, \"avg_batt_temp_delta\"] = round(float(td.mean()),2)\n",
    "            session_df.at[idx, \"med_batt_temp_delta\"] = round(float(td.median()),2)\n",
    "            session_df.at[idx, \"max_batt_temp_delta\"] = round(float(td.max()),2)\n",
    "            session_df.at[idx, \"p95_batt_temp_delta\"] = round(float(td.quantile(0.95)), 2)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 5. Energy integration (kWh)\n",
    "        # -------------------------------\n",
    "        # Power (kW) = V * I / 1000\n",
    "        # Energy (kWh) = Î£ power * (dt_sec / 3600)\n",
    "        chunk[\"power_kw\"] = round((\n",
    "            chunk[\"bat_voltage\"].astype(\"float64\") *\n",
    "            chunk[\"total_battery_current\"].astype(\"float64\")\n",
    "        ) / 1000.0, 2)\n",
    "\n",
    "        chunk[\"energy_kwh\"] = round(chunk[\"power_kw\"] * (\n",
    "            chunk[\"dt_sec\"].astype(\"float64\") / 3600.0\n",
    "        ), 2)\n",
    "\n",
    "        energy_active_kwh = chunk.loc[chunk[\"energy_kwh\"] > 0, \"energy_kwh\"].sum()\n",
    "        if pd.isna(energy_active_kwh):\n",
    "            energy_active_kwh = 0.0\n",
    "\n",
    "        session_df.at[idx, \"energy_active_kwh\"] = round(float(energy_active_kwh), 2)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 6. Physics model: max possible km\n",
    "        # -------------------------------\n",
    "        ts_min = chunk[\"timestamp\"].min()\n",
    "        ts_max = chunk[\"timestamp\"].max()\n",
    "        if pd.isna(ts_min) or pd.isna(ts_max):\n",
    "            duration_hr = 0.0\n",
    "        else:\n",
    "            duration_sec = (ts_max - ts_min).total_seconds()\n",
    "            duration_hr = max(duration_sec / 3600.0, 0.0)\n",
    "\n",
    "        # cap median speed by max_kmph_for_physics\n",
    "        eff_avg_speed = min((max_speed+med_speed)/2, max_kmph_for_physics)\n",
    "        max_physical_km = eff_avg_speed * duration_hr * physics_tolerance\n",
    "\n",
    "        session_df.at[idx, \"max_physical_km\"] = round(float(max_physical_km), 3)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 7. GLITCH detection (Option 3)\n",
    "        # -------------------------------\n",
    "        glitch_flag = False\n",
    "        reasons = []\n",
    "\n",
    "        eps = 1e-6\n",
    "\n",
    "        # A: net odometer itself exceeds physics limit\n",
    "        if net_odo > max_physical_km + eps:\n",
    "            glitch_flag = True\n",
    "            reasons.append(\n",
    "                f\"net_odo {net_odo:.3f}km > max_phys {max_physical_km:.3f}km\"\n",
    "            )\n",
    "\n",
    "        # B: raw cumulative distance exceeds physics limit dramatically\n",
    "        if dist_km_raw > max_physical_km + eps:\n",
    "            glitch_flag = True\n",
    "            reasons.append(\n",
    "                f\"dist_km_raw {dist_km_raw:.3f}km > max_phys {max_physical_km:.3f}km\"\n",
    "            )\n",
    "\n",
    "        # C: backward odometer (shouldn't happen after your finaliser, but guard anyway)\n",
    "        if odo_end is not np.nan and odo_start is not np.nan and odo_end + eps < odo_start:\n",
    "            glitch_flag = True\n",
    "            reasons.append(\n",
    "                f\"odo_end {odo_end:.3f} < odo_start {odo_start:.3f}\"\n",
    "            )\n",
    "\n",
    "        # -------------------------------\n",
    "        # 8. Final distance selection\n",
    "        # -------------------------------\n",
    "        # Start with raw cumulative\n",
    "        dist_final = float(dist_km_raw)\n",
    "\n",
    "        # If raw cumulative is significantly higher than net change, it's jitter\n",
    "        if net_odo > 0 and dist_final > net_odo * physics_tolerance:\n",
    "            reasons.append(\n",
    "                f\"dist_km_raw {dist_final:.3f}km >> net_odo {net_odo:.3f}km, using net_odo\"\n",
    "            )\n",
    "            dist_final = float(net_odo)\n",
    "\n",
    "        # If GLITCH due to physics but net_odo is still sane, keep net_odo as best guess\n",
    "        if glitch_flag:\n",
    "            if net_odo <= max_physical_km + eps:\n",
    "                dist_final = float(net_odo)\n",
    "            else:\n",
    "                # Completely impossible â†’ distance is untrustworthy\n",
    "                dist_final = 0.0\n",
    "\n",
    "        session_df.at[idx, \"dist_km\"] = dist_final\n",
    "\n",
    "\n",
    "        # 5b. Net energy over the session: (discharging - charging)\n",
    "        kwh_dis = session_df.at[idx, \"kwh_discharging\"] if \"kwh_discharging\" in session_df.columns else 0.0\n",
    "        kwh_chg = session_df.at[idx, \"kwh_charging\"] if \"kwh_charging\" in session_df.columns else 0.0\n",
    "    \n",
    "        # robust against NaN\n",
    "        if pd.isna(kwh_dis):\n",
    "            kwh_dis = 0.0\n",
    "        if pd.isna(kwh_chg):\n",
    "            kwh_chg = 0.0\n",
    "    \n",
    "        net_energy_kwh = kwh_dis - kwh_chg\n",
    "    \n",
    "        # net kWh/km using the *final* distance choice\n",
    "        if dist_final > 0:\n",
    "            session_df.at[idx, \"net_kwh_per_km\"] = round(float(net_energy_kwh / dist_final), 2)\n",
    "        else:\n",
    "            session_df.at[idx, \"net_kwh_per_km\"] = 0.0\n",
    "\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 9. kWh/km using final distance\n",
    "        # -------------------------------\n",
    "        if dist_final > 0:\n",
    "            session_df.at[idx, \"kwh_per_km\"] = round(float(energy_active_kwh / dist_final),2)\n",
    "        else:\n",
    "            session_df.at[idx, \"kwh_per_km\"] = 0\n",
    "\n",
    "        # -------------------------------\n",
    "        # 10. Persist GLITCH info\n",
    "        # -------------------------------\n",
    "        if glitch_flag:\n",
    "            session_df.at[idx, \"glitch_flag\"] = True\n",
    "            session_df.at[idx, \"glitch_reason\"] = \"; \".join(reasons)\n",
    "            if has_activity_col:\n",
    "                session_df.at[idx, \"activity\"] = \"GLITCH\"\n",
    "\n",
    "    return session_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39cb9788-0e71-414f-84c3-184480acfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 3. Fleet-level temp summary (used for heatmaps, etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_fleet_temp_table(\n",
    "    sessions_df: pd.DataFrame,\n",
    "    threshold_pct: float = 40.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a per-vehicle summary table from the session-level DataFrame\n",
    "    (output of build_charging_and_discharging_sessions).\n",
    "\n",
    "    For CHARGING_ACTIVE sessions:\n",
    "      - Computes % of sessions where\n",
    "            (maxtemp_bucket_35_40_pct + maxtemp_bucket_gt40_pct) > threshold_pct\n",
    "      - Computes mean % time >35Â°C across all charging-active sessions.\n",
    "\n",
    "    Returns a DataFrame with:\n",
    "        id, reg_num, customer, model,\n",
    "        num_sessions, num_hot_sessions,\n",
    "        hot_session_pct,\n",
    "        mean_pct_above35\n",
    "    \"\"\"\n",
    "    if sessions_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = sessions_df.copy()\n",
    "    df[\"id\"] = df[\"id\"].astype(\"int32\", errors=\"ignore\")\n",
    "\n",
    "    d = df[df[\"activity\"] == \"CHARGING_ACTIVE\"].copy()\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"id\", \"reg_num\", \"customer\", \"model\",\n",
    "            \"num_sessions\", \"num_hot_sessions\",\n",
    "            \"hot_session_pct\", \"mean_pct_above35\",\n",
    "        ])\n",
    "\n",
    "    d[\"maxtemp_bucket_35_40_pct\"] = d.get(\"maxtemp_bucket_35_40_pct\", 0.0)\n",
    "    d[\"maxtemp_bucket_gt40_pct\"] = d.get(\"maxtemp_bucket_gt40_pct\", 0.0)\n",
    "\n",
    "    d[\"pct_above_35\"] = (\n",
    "        d[\"maxtemp_bucket_35_40_pct\"].fillna(0.0)\n",
    "        + d[\"maxtemp_bucket_gt40_pct\"].fillna(0.0)\n",
    "    )\n",
    "\n",
    "    d[\"is_hot\"] = d[\"pct_above_35\"] > threshold_pct\n",
    "\n",
    "    agg = (\n",
    "        d.groupby([\"id\", \"reg_num\", \"customer\", \"model\"])\n",
    "        .agg(\n",
    "            num_sessions=(\"session\", \"nunique\"),\n",
    "            num_hot_sessions=(\"is_hot\", \"sum\"),\n",
    "            mean_pct_above35=(\"pct_above_35\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    agg[\"hot_session_pct\"] = (\n",
    "        agg[\"num_hot_sessions\"] * 100.0 / agg[\"num_sessions\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35edde9-dbdb-4aca-9b43-62bd723eb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_daily_tms_analysis(\n",
    "    df_raw: pd.DataFrame,\n",
    "    df_mapping: Optional[pd.DataFrame] = None,\n",
    "    max_gap_sec: float = 600.0,\n",
    "    active_threshold: float = 5.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full daily pipeline\n",
    "    1. rename columns\n",
    "    2. impute missing values\n",
    "    3. odometer repair\n",
    "    4. finalize odometer\n",
    "    5. state preparation\n",
    "    6. tms session extraction\n",
    "    7. fleet summaries\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 1) RENAME RAW COLUMNS â†’ canonical battery column names\n",
    "    # --------------------------------------------------------------\n",
    "    df = rename_battery_temp_columns(df_raw)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 2) IMPUTE missing values (temperature, voltage, etc.)\n",
    "    # --------------------------------------------------------------\n",
    "    df = impute_missing_values(df)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 3) IMPUTE ODOMETER intelligently using speed + interpolation\n",
    "    # --------------------------------------------------------------\n",
    "    df = impute_odometer(df)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 4) FINALIZE ODOMETER (clip negatives, fix small reversals, etc.)\n",
    "    # --------------------------------------------------------------\n",
    "    df = finalize_odometer(df)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 5) PREPARE STATE (temp buckets, volt buckets, dt_sec, activity, etc.)\n",
    "    # --------------------------------------------------------------\n",
    "    df_state = prepare_df_with_state(df, df_mapping)\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 6) EXTRACT TMS SESSIONS\n",
    "    # --------------------------------------------------------------\n",
    "    sessions = build_charging_and_discharging_sessions(df_state)\n",
    "\n",
    "    # ðŸ”¹ Enrich discharging sessions with distance / kWh/km\n",
    "    if not sessions.empty:\n",
    "        sessions = enrich_discharging_metrics(sessions, df_state)\n",
    "        # ðŸ”¹ Enforce canonical column ordering here\n",
    "        cols_present = [c for c in SESSION_COL_ORDER if c in sessions.columns]\n",
    "        other_cols = [c for c in sessions.columns if c not in cols_present]\n",
    "        sessions = sessions[cols_present + other_cols]        \n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "    # 7) FLEET-LEVEL SUMMARY TABLE\n",
    "    # --------------------------------------------------------------\n",
    "    fleet_temp = build_fleet_temp_table(sessions)\n",
    "    \n",
    "    return {\n",
    "        \"df_state\": df_state,\n",
    "        \"sessions\": sessions,\n",
    "        \"fleet_temp\": fleet_temp,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f487d836-03f2-4a01-b63e-e347151fdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_distinct_device_ids(conn, schema=\"facts_prod\", table=\"can_parsed_output_all\"):\n",
    "#     sql = f\"\"\"\n",
    "#         SELECT DISTINCT id\n",
    "#         FROM {schema}.{table}\n",
    "#         WHERE id IS NOT NULL\n",
    "#         ORDER BY id\n",
    "#     \"\"\"\n",
    "#     cur = conn.cursor()\n",
    "#     cur.execute(sql)\n",
    "#     ids = [str(r[0]).strip() for r in cur.fetchall()]\n",
    "#     return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4096364b-dd00-48bd-bb84-1e34425c56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import timedelta\n",
    "\n",
    "# def get_utc_window_for_ist_day(day_str: str, tz_name: str = \"Asia/Kolkata\"):\n",
    "#     \"\"\"\n",
    "#     Given an IST calendar day (e.g. '2025-09-02'), return the UTC start\n",
    "#     and end timestamps that fully cover that local day.\n",
    "\n",
    "#     Example:\n",
    "#       IST day 2025-09-02 00:00 â†’ 2025-09-03 00:00\n",
    "#       becomes UTC window:\n",
    "#          2025-09-01 18:30:00  to  2025-09-02 18:30:00\n",
    "#     \"\"\"\n",
    "#     ist_midnight = pd.Timestamp(day_str).tz_localize(tz_name)\n",
    "#     ist_next_midnight = ist_midnight + timedelta(days=1)\n",
    "\n",
    "#     utc_start = ist_midnight.tz_convert(\"UTC\")\n",
    "#     utc_end = ist_next_midnight.tz_convert(\"UTC\")\n",
    "\n",
    "#     # return naive UTC timestamps for SQL\n",
    "#     return (\n",
    "#         utc_start.tz_convert(\"UTC\").replace(tzinfo=None),\n",
    "#         utc_end.tz_convert(\"UTC\").replace(tzinfo=None),\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37ec7fc5-1674-4bed-8e27-136cf47b702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_day_tms_analysis_memory_only(\n",
    "    conn,\n",
    "    start_date: str,\n",
    "    num_days: int,\n",
    "    df_mapping: Optional[pd.DataFrame],\n",
    "    core_cols: list,\n",
    "    schema: str = \"facts_prod\",\n",
    "    table: str = \"can_parsed_output_all\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-day wrapper around the daily TMS pipeline.\n",
    "\n",
    "    - start_date is an IST calendar date string, e.g. '2025-09-01'\n",
    "    - num_days is how many IST days to process\n",
    "    - Data is fetched in UTC using the correct ISTâ†’UTC window.\n",
    "    \"\"\"\n",
    "    ids = fetch_distinct_device_ids(conn, schema=schema, table=table)\n",
    "    print(f\"Found {len(ids)} device IDs\")\n",
    "\n",
    "    start_dt = pd.to_datetime(start_date).date()\n",
    "    results_all_days = []\n",
    "\n",
    "    for offset in range(num_days):\n",
    "        day = start_dt + timedelta(days=offset)\n",
    "        day_str = day.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        print(f\"\\n=== Processing IST day {day_str} ===\")\n",
    "\n",
    "        # ðŸ”¹ Use the IST-aware fetch\n",
    "        df_raw = fetch_data_for_day_trino(\n",
    "            conn=conn,\n",
    "            day_str=day_str,           # <-- matches fetch_data_for_day signature\n",
    "            ids=ids,\n",
    "            core_cols=core_cols,\n",
    "            table=table,\n",
    "            schema=schema,\n",
    "        )\n",
    "\n",
    "        if df_raw.empty:\n",
    "            print(f\"No data for IST day {day_str}\")\n",
    "            results_all_days.append({\n",
    "                \"date\": day_str,\n",
    "                \"state_df\": pd.DataFrame(),\n",
    "                \"sessions_df\": pd.DataFrame(),\n",
    "                \"fleet_df\": pd.DataFrame(),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- NEW: per-day NO DATA log ----------------------------------\n",
    "        present_ids = set(df_raw[\"id\"].astype(str).unique())\n",
    "        missing_ids = [vid for vid in ids if vid not in present_ids]\n",
    "\n",
    "        if missing_ids:\n",
    "            print(\n",
    "                f\"  â†’ {len(missing_ids)} IDs had NO DATA on {day_str} \"\n",
    "                f\"(showing first 10): {missing_ids[:10]}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  â†’ All {len(ids)} IDs present on {day_str}\")\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "        # Build IST timestamp & day column for downstream logic\n",
    "        # If timestamp is naive UTC, localize first\n",
    "        if not pd.api.types.is_datetime64tz_dtype(df_raw[\"timestamp\"].dtype):\n",
    "            df_raw[\"timestamp\"] = df_raw[\"timestamp\"].dt.tz_localize(\"UTC\")\n",
    "\n",
    "        df_raw[\"timestamp_ist\"] = df_raw[\"timestamp\"].dt.tz_convert(\"Asia/Kolkata\")\n",
    "        df_raw[\"date_val\"] = df_raw[\"timestamp_ist\"].dt.date\n",
    "\n",
    "        # Your existing daily pipeline, which ultimately calls prepare_df_with_state,\n",
    "        # session builder, fleet table, etc.\n",
    "        # may be None, prepare_df_with_state handles it\n",
    "        daily = run_daily_tms_analysis(df_raw=df_raw,df_mapping=df_mapping,  )\n",
    "\n",
    "        results_all_days.append({\"date\": day_str,\"state_df\": daily[\"df_state\"],\"sessions_df\": daily[\"sessions\"],\"fleet_df\": daily[\"fleet_temp\"],})\n",
    "\n",
    "        del df_raw, daily\n",
    "        gc.collect()\n",
    "\n",
    "    return {\"daily\": results_all_days, \"device_ids\": ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1461256-dad9-4bc9-9ae2-11b041cecf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import logging\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# from db_operations import (\n",
    "#     fetch_data_for_day_trino,\n",
    "#     fetch_distinct_ids_for_day_trino,\n",
    "#     write_df_to_iceberg,\n",
    "# )\n",
    "# from bcs_tms import run_daily_tms_analysis\n",
    "\n",
    "def run_tms_sessions_range_to_iceberg(\n",
    "    conn,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    core_cols,\n",
    "    df_mapping=None,\n",
    "    source_schema: str = \"facts_prod\",\n",
    "    source_table: str = \"can_parsed_output_all\",\n",
    "    target_schema: str = \"facts_dev\",\n",
    "    target_table: str = \"bcs_tms_sessions_v1\",\n",
    "    partition_by=(\"date\",),\n",
    "    push_to_db: bool = True,\n",
    "    collect_sessions: bool = False,\n",
    "    ids=None,   # optional explicit list of device IDs (global filter)\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-safe range runner for TMS sessions.\n",
    "\n",
    "    - Loops from start_date to end_date (inclusive), interpreted in IST.\n",
    "    - For each day:\n",
    "        * If `ids` is None:\n",
    "            - discover the IDs that actually have data on this IST day\n",
    "          else:\n",
    "            - use the provided `ids` list as a filter\n",
    "        * For each id in that day's list:\n",
    "            - fetch_data_for_day_trino(conn, day, [id], ...)\n",
    "            - run_daily_tms_analysis(...)\n",
    "        * After all ids for that day:\n",
    "            - concat all sessions for that day\n",
    "            - optionally write that day's sessions to Iceberg\n",
    "    \"\"\"\n",
    "\n",
    "    # --- normalize start_date / end_date to date objects ---\n",
    "    if isinstance(start_date, str):\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    else:\n",
    "        start = start_date\n",
    "\n",
    "    if isinstance(end_date, str):\n",
    "        end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    else:\n",
    "        end = end_date\n",
    "\n",
    "    if end < start:\n",
    "        raise ValueError(\"end_date must be >= start_date\")\n",
    "\n",
    "    n_days = (end - start).days + 1\n",
    "\n",
    "    # Optional global filter: if user passes ids, weâ€™ll intersect with per-day ids\n",
    "    global_ids_filter = None\n",
    "    if ids is not None:\n",
    "        global_ids_filter = set(str(x) for x in ids)\n",
    "\n",
    "    logging.info(\n",
    "        \"ðŸš€ TMS sessions range job: %s â†’ %s (%d days)\",\n",
    "        start,\n",
    "        end,\n",
    "        n_days,\n",
    "    )\n",
    "\n",
    "    all_days_sessions = [] if collect_sessions else None\n",
    "\n",
    "    day = start\n",
    "    while day <= end:\n",
    "        day_str = day.strftime(\"%Y-%m-%d\")\n",
    "        logging.info(\"ðŸ“… Processing IST day %s\", day_str)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Per-day ID discovery\n",
    "        # ------------------------------------------------------------------\n",
    "        if global_ids_filter is None:\n",
    "            ids_for_day = fetch_distinct_ids_for_day_trino(\n",
    "                conn,\n",
    "                day,\n",
    "                schema=source_schema,\n",
    "                table=source_table,\n",
    "            )\n",
    "        else:\n",
    "            # discover ids for day, then intersect with global filter\n",
    "            day_ids_raw = fetch_distinct_ids_for_day_trino(\n",
    "                conn,\n",
    "                day,\n",
    "                schema=source_schema,\n",
    "                table=source_table,\n",
    "            )\n",
    "            ids_for_day = [vid for vid in day_ids_raw if vid in global_ids_filter]\n",
    "\n",
    "        if not ids_for_day:\n",
    "            logging.info(\"  â„¹ï¸ No IDs with data on %s. Skipping entire day.\", day_str)\n",
    "            day += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        logging.info(\n",
    "            \"  ðŸ”Ž Found %d IDs with data on %s: %s\",\n",
    "            len(ids_for_day),\n",
    "            day_str,\n",
    "            \", \".join(ids_for_day),\n",
    "        )\n",
    "\n",
    "        # Collect all sessions for this day (across these IDs)\n",
    "        day_sessions_list = []\n",
    "\n",
    "        for vid in ids_for_day:\n",
    "            logging.info(\"  ðŸš Processing id=%s on %s\", vid, day_str)\n",
    "\n",
    "            # Fetch raw data for THIS (id, day)\n",
    "            df_raw = fetch_data_for_day_trino(\n",
    "                conn,          # connection\n",
    "                day,           # date (IST calendar day)\n",
    "                [vid],         # list of ids (single id)\n",
    "                core_cols=core_cols,\n",
    "                schema=source_schema,\n",
    "                table=source_table,\n",
    "            )\n",
    "\n",
    "            if df_raw.empty:\n",
    "                # This should be rare now, but keep it safe\n",
    "                del df_raw\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            # TMS analysis for this id/day\n",
    "            daily = run_daily_tms_analysis(\n",
    "                df_raw=df_raw,\n",
    "                df_mapping=df_mapping,\n",
    "            )\n",
    "            sessions_id = daily[\"sessions\"]\n",
    "\n",
    "            if sessions_id.empty:\n",
    "                del df_raw, daily, sessions_id\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            # ensure 'date' is this IST day\n",
    "            if \"date\" not in sessions_id.columns:\n",
    "                sessions_id[\"date\"] = day\n",
    "            else:\n",
    "                sessions_id[\"date\"] = pd.to_datetime(sessions_id[\"date\"]).dt.date\n",
    "                sessions_id[\"date\"] = day\n",
    "\n",
    "            day_sessions_list.append(sessions_id)\n",
    "\n",
    "            del df_raw, daily, sessions_id\n",
    "            gc.collect()\n",
    "\n",
    "        # --- after looping over all ids for this day ---\n",
    "        if not day_sessions_list:\n",
    "            logging.info(\"  â„¹ï¸ No sessions built for %s across all IDs. Skipping DB write.\", day_str)\n",
    "            day += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        day_sessions_df = pd.concat(day_sessions_list, ignore_index=True)\n",
    "        logging.info(\n",
    "            \"  âœ… Built %d sessions total for %s (across %d IDs).\",\n",
    "            len(day_sessions_df),\n",
    "            day_str,\n",
    "            day_sessions_df[\"id\"].nunique() if \"id\" in day_sessions_df.columns else -1,\n",
    "        )\n",
    "\n",
    "        # Single write per day\n",
    "        if push_to_db:\n",
    "            write_df_to_iceberg(\n",
    "                conn=conn,\n",
    "                df=day_sessions_df,\n",
    "                schema=target_schema,\n",
    "                table=target_table,\n",
    "                partition_by=list(partition_by) if partition_by else None,\n",
    "            )\n",
    "            logging.info(\n",
    "                \"  ðŸ’¾ Inserted %d rows into %s.%s for %s\",\n",
    "                len(day_sessions_df),\n",
    "                target_schema,\n",
    "                target_table,\n",
    "                day_str,\n",
    "            )\n",
    "\n",
    "        if collect_sessions:\n",
    "            all_days_sessions.append(day_sessions_df.copy())\n",
    "\n",
    "        del day_sessions_list, day_sessions_df\n",
    "        gc.collect()\n",
    "\n",
    "        day += timedelta(days=1)\n",
    "\n",
    "    logging.info(\"âœ… Completed TMS sessions range job: %s â†’ %s\", start, end)\n",
    "\n",
    "    if collect_sessions:\n",
    "        if all_days_sessions:\n",
    "            return pd.concat(all_days_sessions, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef3c8b20-fc8a-4f85-8f81-0235b1a3da61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 21:25:12 - INFO - ðŸ”Œ STEP 1: Connecting to Trino...\n",
      "2025-12-11 21:25:12 - INFO - âœ… STEP 1: Connected to Trino\n",
      "2025-12-11 21:25:12 - INFO - âš™ï¸ Executing query...\n",
      "2025-12-11 21:25:20 - INFO - âœ… Query executed successfully (no results expected)!\n"
     ]
    }
   ],
   "source": [
    "# conn = connect_to_trino()\n",
    "# sql = f\"DROP TABLE IF EXISTS adhoc.facts_dev.bcs_tms_sessions_v1\"\n",
    "# execute_query(conn, sql, return_results=False)\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b19a8088-360c-423c-9a3e-c00e74a50bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc463454-1f06-4e8b-b8c4-b16556e84b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 21:13:43 - INFO - ðŸ”Œ STEP 1: Connecting to Trino...\n",
      "2025-12-11 21:13:43 - INFO - âœ… STEP 1: Connected to Trino\n",
      "2025-12-11 21:13:43 - INFO - ðŸš€ TMS sessions range job: 2025-08-01 â†’ 2025-08-03 (3 days)\n",
      "2025-12-11 21:13:43 - INFO - ðŸ“… Processing IST day 2025-08-01\n",
      "2025-12-11 21:13:43 - INFO - ðŸ” Fetching distinct ids for 2025-08-01 (UTC window 2025-07-31 18:30:00 â†’ 2025-08-01 18:30:00)\n",
      "2025-12-11 21:13:45 - INFO - ðŸ” Found 11 ids with data on 2025-08-01\n",
      "2025-12-11 21:13:45 - INFO -   ðŸ”Ž Found 11 IDs with data on 2025-08-01: 11, 12, 13, 14, 15, 16, 18, 3, 6, 7, 9\n",
      "2025-12-11 21:13:45 - INFO -   ðŸš Processing id=11 on 2025-08-01\n",
      "2025-12-11 21:13:49 - INFO -   ðŸš Processing id=12 on 2025-08-01\n",
      "2025-12-11 21:13:51 - INFO -   ðŸš Processing id=13 on 2025-08-01\n",
      "2025-12-11 21:13:56 - INFO -   ðŸš Processing id=14 on 2025-08-01\n",
      "2025-12-11 21:14:00 - INFO -   ðŸš Processing id=15 on 2025-08-01\n",
      "2025-12-11 21:14:04 - INFO -   ðŸš Processing id=16 on 2025-08-01\n",
      "2025-12-11 21:14:06 - INFO -   ðŸš Processing id=18 on 2025-08-01\n",
      "2025-12-11 21:14:09 - INFO -   ðŸš Processing id=3 on 2025-08-01\n",
      "2025-12-11 21:14:11 - INFO -   ðŸš Processing id=6 on 2025-08-01\n",
      "2025-12-11 21:14:16 - INFO -   ðŸš Processing id=7 on 2025-08-01\n",
      "2025-12-11 21:14:20 - INFO -   ðŸš Processing id=9 on 2025-08-01\n",
      "2025-12-11 21:14:24 - INFO -   âœ… Built 68 sessions total for 2025-08-01 (across 11 IDs).\n",
      "2025-12-11 21:14:24 - INFO - ðŸ’¾ [4/5] STEP 4a: Preparing to write results to Iceberg table adhoc.facts_dev.bcs_tms_sessions_v1...\n",
      "2025-12-11 21:14:24 - INFO - ðŸ“‚ Table will be partitioned by: ['date']\n",
      "2025-12-11 21:14:24 - INFO - ðŸ” [4/5] STEP 4b: Creating table if not exists...\n",
      "2025-12-11 21:14:24 - INFO - ðŸ› ï¸ [4/5] STEP 4c: Ensured Iceberg table bcs_tms_sessions_v1 exists\n",
      "2025-12-11 21:14:24 - INFO - ðŸ” [4/5] STEP 4d: Prepared INSERT statement\n",
      "2025-12-11 21:14:24 - INFO - ðŸ” [4/5] STEP 4e: First row preview: ('11', None, None, None, datetime.date(2025, 8, 1), 'MISSING_INFO', 1, datetime.datetime(2025, 8, 1, 0, 0), datetime.datetime(2025, 8, 1, 5, 30, 0, 90000), 330.0, 9044.5, 9044.5, 0.0, 0.0, 0.0, 0.0, None, None, 0.0, 0.0, 0.0, 0.0, 0.0, None, None, None, None, None, None, None, None, None, 0.0, 0.0, 0.0, 6.0, 6.0, 6.0, 6.0, 4.0, 4.0, 4.0, 4.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, False, '')\n",
      "2025-12-11 21:15:03 - INFO - âœ… [4/5] STEP 4f: Inserted 68 rows (total 68) into bcs_tms_sessions_v1\n",
      "2025-12-11 21:15:03 - INFO - ðŸŽ‰ [4/5] STEP 4g: Finished inserting 68 rows into bcs_tms_sessions_v1\n",
      "2025-12-11 21:15:03 - INFO -   ðŸ’¾ Inserted 68 rows into facts_dev.bcs_tms_sessions_v1 for 2025-08-01\n",
      "2025-12-11 21:15:03 - INFO - ðŸ“… Processing IST day 2025-08-02\n",
      "2025-12-11 21:15:03 - INFO - ðŸ” Fetching distinct ids for 2025-08-02 (UTC window 2025-08-01 18:30:00 â†’ 2025-08-02 18:30:00)\n",
      "2025-12-11 21:15:05 - INFO - ðŸ” Found 8 ids with data on 2025-08-02\n",
      "2025-12-11 21:15:05 - INFO -   ðŸ”Ž Found 8 IDs with data on 2025-08-02: 11, 13, 14, 15, 3, 6, 7, 9\n",
      "2025-12-11 21:15:05 - INFO -   ðŸš Processing id=11 on 2025-08-02\n",
      "2025-12-11 21:15:10 - INFO -   ðŸš Processing id=13 on 2025-08-02\n",
      "2025-12-11 21:15:14 - INFO -   ðŸš Processing id=14 on 2025-08-02\n",
      "2025-12-11 21:15:18 - INFO -   ðŸš Processing id=15 on 2025-08-02\n",
      "2025-12-11 21:15:21 - INFO -   ðŸš Processing id=3 on 2025-08-02\n",
      "2025-12-11 21:15:24 - INFO -   ðŸš Processing id=6 on 2025-08-02\n",
      "2025-12-11 21:15:27 - INFO -   ðŸš Processing id=7 on 2025-08-02\n",
      "2025-12-11 21:15:31 - INFO -   ðŸš Processing id=9 on 2025-08-02\n",
      "2025-12-11 21:15:34 - INFO -   âœ… Built 51 sessions total for 2025-08-02 (across 8 IDs).\n",
      "2025-12-11 21:15:34 - INFO - ðŸ’¾ [4/5] STEP 4a: Preparing to write results to Iceberg table adhoc.facts_dev.bcs_tms_sessions_v1...\n",
      "2025-12-11 21:15:34 - INFO - ðŸ“‚ Table will be partitioned by: ['date']\n",
      "2025-12-11 21:15:34 - INFO - ðŸ” [4/5] STEP 4b: Creating table if not exists...\n",
      "2025-12-11 21:15:34 - INFO - ðŸ› ï¸ [4/5] STEP 4c: Ensured Iceberg table bcs_tms_sessions_v1 exists\n",
      "2025-12-11 21:15:34 - INFO - ðŸ” [4/5] STEP 4d: Prepared INSERT statement\n",
      "2025-12-11 21:15:34 - INFO - ðŸ” [4/5] STEP 4e: First row preview: ('11', None, None, None, datetime.date(2025, 8, 2), 'MISSING_INFO', 1, datetime.datetime(2025, 8, 2, 0, 0), datetime.datetime(2025, 8, 2, 5, 30, 0, 574000), 330.01, 9552.25, 9552.25, 0.0, 0.0, 0.0, 0.0, None, None, 0.0, 0.0, 0.0, 0.0, 0.0, None, None, None, None, None, None, None, None, None, 0.0, 0.0, 0.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, False, '')\n",
      "2025-12-11 21:16:07 - INFO - âœ… [4/5] STEP 4f: Inserted 51 rows (total 51) into bcs_tms_sessions_v1\n",
      "2025-12-11 21:16:07 - INFO - ðŸŽ‰ [4/5] STEP 4g: Finished inserting 51 rows into bcs_tms_sessions_v1\n",
      "2025-12-11 21:16:07 - INFO -   ðŸ’¾ Inserted 51 rows into facts_dev.bcs_tms_sessions_v1 for 2025-08-02\n",
      "2025-12-11 21:16:07 - INFO - ðŸ“… Processing IST day 2025-08-03\n",
      "2025-12-11 21:16:07 - INFO - ðŸ” Fetching distinct ids for 2025-08-03 (UTC window 2025-08-02 18:30:00 â†’ 2025-08-03 18:30:00)\n",
      "2025-12-11 21:16:09 - INFO - ðŸ” Found 8 ids with data on 2025-08-03\n",
      "2025-12-11 21:16:09 - INFO -   ðŸ”Ž Found 8 IDs with data on 2025-08-03: 11, 13, 14, 15, 3, 6, 7, 9\n",
      "2025-12-11 21:16:09 - INFO -   ðŸš Processing id=11 on 2025-08-03\n",
      "2025-12-11 21:16:13 - INFO -   ðŸš Processing id=13 on 2025-08-03\n",
      "2025-12-11 21:16:17 - INFO -   ðŸš Processing id=14 on 2025-08-03\n",
      "2025-12-11 21:16:22 - INFO -   ðŸš Processing id=15 on 2025-08-03\n",
      "2025-12-11 21:16:24 - INFO -   ðŸš Processing id=3 on 2025-08-03\n",
      "2025-12-11 21:16:27 - INFO -   ðŸš Processing id=6 on 2025-08-03\n",
      "2025-12-11 21:16:29 - INFO -   ðŸš Processing id=7 on 2025-08-03\n",
      "2025-12-11 21:16:34 - INFO -   ðŸš Processing id=9 on 2025-08-03\n",
      "2025-12-11 21:16:37 - INFO -   âœ… Built 47 sessions total for 2025-08-03 (across 8 IDs).\n",
      "2025-12-11 21:16:37 - INFO - ðŸ’¾ [4/5] STEP 4a: Preparing to write results to Iceberg table adhoc.facts_dev.bcs_tms_sessions_v1...\n",
      "2025-12-11 21:16:37 - INFO - ðŸ“‚ Table will be partitioned by: ['date']\n",
      "2025-12-11 21:16:37 - INFO - ðŸ” [4/5] STEP 4b: Creating table if not exists...\n",
      "2025-12-11 21:16:37 - INFO - ðŸ› ï¸ [4/5] STEP 4c: Ensured Iceberg table bcs_tms_sessions_v1 exists\n",
      "2025-12-11 21:16:37 - INFO - ðŸ” [4/5] STEP 4d: Prepared INSERT statement\n",
      "2025-12-11 21:16:37 - INFO - ðŸ” [4/5] STEP 4e: First row preview: ('11', None, None, None, datetime.date(2025, 8, 3), 'MISSING_INFO', 1, datetime.datetime(2025, 8, 3, 0, 0), datetime.datetime(2025, 8, 3, 5, 30, 0, 91000), 330.0, 10400.75, 10400.75, 0.0, 0.0, 0.0, 0.0, None, None, 0.0, 0.0, 0.0, 0.0, 0.0, None, None, None, None, None, None, None, None, None, 78.61, 78.61, 78.61, 13.0, 13.0, 13.0, 13.0, 4.0, 4.0, 4.0, 4.0, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, False, '')\n",
      "2025-12-11 21:17:08 - INFO - âœ… [4/5] STEP 4f: Inserted 47 rows (total 47) into bcs_tms_sessions_v1\n",
      "2025-12-11 21:17:08 - INFO - ðŸŽ‰ [4/5] STEP 4g: Finished inserting 47 rows into bcs_tms_sessions_v1\n",
      "2025-12-11 21:17:08 - INFO -   ðŸ’¾ Inserted 47 rows into facts_dev.bcs_tms_sessions_v1 for 2025-08-03\n",
      "2025-12-11 21:17:09 - INFO - âœ… Completed TMS sessions range job: 2025-08-01 â†’ 2025-08-03\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_trino()\n",
    "\n",
    "run_tms_sessions_range_to_iceberg(\n",
    "    conn=conn,\n",
    "    start_date=\"2025-08-01\",\n",
    "    end_date=\"2025-08-3\",\n",
    "    core_cols=CORE_COLS,\n",
    "    df_mapping=None,\n",
    "    source_schema=\"facts_prod\",\n",
    "    source_table=\"can_parsed_output_all\",\n",
    "    target_schema=\"facts_dev\",\n",
    "    target_table=\"bcs_tms_sessions_v1\",\n",
    "    push_to_db=True,\n",
    "    partition_by=(\"date\",),\n",
    "    collect_sessions=False,   # pure pipeline mode\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "# sessions_all.query(\"date == '2025-09-02' and id == '16'\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a5c56-5139-4bc4-9a23-b4fa502269d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9d794-6d37-4067-9d87-ded90c6db32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    logger.info(\"ðŸ”Œ STEP 1: Connecting to Trino...\")\n",
    "    conn = connect_to_trino()\n",
    "    logger.info(\"âœ… STEP 1: Connected to Trino\")\n",
    "\n",
    "    start_date, end_date = resolve_dates(args)\n",
    "\n",
    "    logger.info(\"ðŸ“… STEP 2: Determining date range...\")\n",
    "    logger.info(f\"   â†’ Running TMS sessions from {start_date} to {end_date} (IST basis)\")\n",
    "\n",
    "    logger.info(\"âš™ï¸ STEP 3: Running TMS sessions pipeline...\")\n",
    "    sessions_all = run_tms_sessions_range_to_iceberg(\n",
    "        conn=conn,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        core_cols=CORE_COLS,\n",
    "        df_mapping=None,  # or your mapping df\n",
    "        source_schema=\"facts_prod\",\n",
    "        source_table=\"can_parsed_output_all\",\n",
    "        target_schema=\"facts_dev\",\n",
    "        target_table=\"bcs_tms_sessions_v1\",\n",
    "        push_to_db=True,\n",
    "        partition_by=[\"date\"],\n",
    "    )\n",
    "\n",
    "    logger.info(f\"ðŸ“Š STEP 3: Pipeline produced {len(sessions_all)} session rows\")\n",
    "\n",
    "    logger.info(\"ðŸ”’ STEP 4: Closing Trino connection...\")\n",
    "    conn.close()\n",
    "    logger.info(\"âœ… STEP 4: Connection closed.\")\n",
    "    logger.info(\"ðŸŽ‰ STEP 5: TMS sessions job completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcf4b4-3770-4858-83cf-17b58c56f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect_to_trino()   # your DB function\n",
    "\n",
    "# mapping_df = fetch_vehicle_mapping(conn)\n",
    "res = run_multi_day_tms_analysis_memory_only(\n",
    "    conn=conn,\n",
    "    start_date=\"2025-09-01\",\n",
    "    num_days=2,\n",
    "    df_mapping = None,\n",
    "    core_cols=CORE_COLS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf6336-ae42-42a5-8cbd-ce1461445dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df_day1 = res[\"daily\"][1][\"sessions_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed5b9a-d0d3-4030-b699-95977265f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df_day1.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd3929-a3a9-4f3d-afd9-6e73e31a715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SESSION_COL_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad597fd3-e7ec-4a2b-8d05-da589c212bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sessions_df_day1[sessions_df_day1.id=='16'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5e344-0ab2-4ac8-bb2c-1bdd4e1a7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df_day1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0bb2a-bc29-488c-a65e-5250057e20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day_result in res[\"daily\"]:\n",
    "    print(day_result[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b757060-0503-4edb-91c9-b518641c6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state = res[\"daily\"][1][\"state_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76595a-f181-4c79-b973-2975e97d1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick that big DISCHARGING_ACTIVE session for id 16\n",
    "row = sessions_df_day1.query(\"id == '16' & activity == 'DISCHARGING_ACTIVE'\").iloc[0]\n",
    "\n",
    "vid = row[\"id\"]\n",
    "t1 = row[\"start_time\"]\n",
    "t2 = row[\"end_time\"]\n",
    "\n",
    "chunk = df_state[(df_state[\"id\"] == vid) &\n",
    "                 (df_state[\"timestamp\"] >= t1) &\n",
    "                 (df_state[\"timestamp\"] <= t2)].copy()\n",
    "\n",
    "v = chunk[\"vehicle_speed_vcu\"].dropna().astype(float)\n",
    "\n",
    "print(\"mean:\", v.mean())\n",
    "print(\"median (all samples):\", v.median())\n",
    "print(\"median (moving >0.1):\", v[v > 0.1].median())\n",
    "print(\"share of zero speeds:\", (v == 0).mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naarni_venv",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
