{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02c4669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vishal\tcommon\tnaarni_venv\n"
     ]
    }
   ],
   "source": [
    "!ls /home/notebook/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2d6f90-57be-487b-9a5c-f9143402162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import logging\n",
    "import argparse\n",
    "from datetime import datetime, date, timedelta\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "repo_path = '/home/notebook'\n",
    "sys.path.append(os.path.join(repo_path, 'test'))\n",
    "\n",
    "\n",
    "# from common.cpoall_aggregator import run_daily_tms_analysis\n",
    "# Import necessary files and its respective functions\n",
    "from common.db_operations import connect_to_trino, fetch_distinct_ids_for_day_trino,fetch_data_for_day_trino, write_df_to_iceberg, parse_arguments, get_target_date\n",
    "from common.cpoall_aggregator import run_daily_tms_analysis,CORE_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb2013e-ed5f-4c92-916a-292db49c7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = connect_to_trino()\n",
    "# sql = f\"DROP TABLE IF EXISTS adhoc.facts_dev.bcs_tms_sessions_v1\"\n",
    "# execute_query(conn, sql, return_results=False)\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a146a72-2664-4de7-b292-ab1af050b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tms_sessions_range_to_iceberg(\n",
    "    conn,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    core_cols,\n",
    "    df_mapping=None,\n",
    "    source_schema: str = \"facts_prod\",\n",
    "    source_table: str = \"can_parsed_output_all\",\n",
    "    target_schema: str = \"facts_dev\",\n",
    "    target_table: str = \"bcs_tms_sessions_v1\",\n",
    "    partition_by=(\"date\",),\n",
    "    push_to_db: bool = True,\n",
    "    collect_sessions: bool = False,\n",
    "    ids=None,   # optional explicit list of device IDs (global filter)\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-safe range runner for TMS sessions.\n",
    "\n",
    "    - Loops from start_date to end_date (inclusive), interpreted in IST.\n",
    "    - For each day:\n",
    "        * If `ids` is None:\n",
    "            - discover the IDs that actually have data on this IST day\n",
    "          else:\n",
    "            - use the provided `ids` list as a filter\n",
    "        * For each id in that day's list:\n",
    "            - fetch_data_for_day_trino(conn, day, [id], ...)\n",
    "            - run_daily_tms_analysis(...)\n",
    "        * After all ids for that day:\n",
    "            - concat all sessions for that day\n",
    "            - optionally write that day's sessions to Iceberg\n",
    "    \"\"\"\n",
    "\n",
    "    # --- normalize start_date / end_date to date objects ---\n",
    "    if isinstance(start_date, str):\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    else:\n",
    "        start = start_date\n",
    "\n",
    "    if isinstance(end_date, str):\n",
    "        end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    else:\n",
    "        end = end_date\n",
    "\n",
    "    if end < start:\n",
    "        raise ValueError(\"end_date must be >= start_date\")\n",
    "\n",
    "    n_days = (end - start).days + 1\n",
    "\n",
    "    # Optional global filter: if user passes ids, weâ€™ll intersect with per-day ids\n",
    "    global_ids_filter = None\n",
    "    if ids is not None:\n",
    "        global_ids_filter = set(str(x) for x in ids)\n",
    "\n",
    "    logging.info(\n",
    "        \"ðŸš€ TMS sessions range job: %s â†’ %s (%d days)\",\n",
    "        start,\n",
    "        end,\n",
    "        n_days,\n",
    "    )\n",
    "\n",
    "    all_days_sessions = [] if collect_sessions else None\n",
    "\n",
    "    day = start\n",
    "    while day <= end:\n",
    "        day_str = day.strftime(\"%Y-%m-%d\")\n",
    "        logging.info(\"ðŸ“… Processing IST day %s\", day_str)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Per-day ID discovery\n",
    "        # ------------------------------------------------------------------\n",
    "        if global_ids_filter is None:\n",
    "            ids_for_day = fetch_distinct_ids_for_day_trino(\n",
    "                conn,\n",
    "                day,\n",
    "                schema=source_schema,\n",
    "                table=source_table,\n",
    "            )\n",
    "        else:\n",
    "            # discover ids for day, then intersect with global filter\n",
    "            day_ids_raw = fetch_distinct_ids_for_day_trino(\n",
    "                conn,\n",
    "                day,\n",
    "                schema=source_schema,\n",
    "                table=source_table,\n",
    "            )\n",
    "            ids_for_day = [vid for vid in day_ids_raw if vid in global_ids_filter]\n",
    "\n",
    "        if not ids_for_day:\n",
    "            logging.info(\"  â„¹ï¸ No IDs with data on %s. Skipping entire day.\", day_str)\n",
    "            day += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        logging.info(\n",
    "            \"  ðŸ”Ž Found %d IDs with data on %s: %s\",\n",
    "            len(ids_for_day),\n",
    "            day_str,\n",
    "            \", \".join(ids_for_day),\n",
    "        )\n",
    "\n",
    "        # Collect all sessions for this day (across these IDs)\n",
    "        day_sessions_list = []\n",
    "\n",
    "        for vid in ids_for_day:\n",
    "            logging.info(\"  ðŸš Processing id=%s on %s\", vid, day_str)\n",
    "\n",
    "            # Fetch raw data for THIS (id, day)\n",
    "            df_raw = fetch_data_for_day_trino(\n",
    "                conn,          # connection\n",
    "                day,           # date (IST calendar day)\n",
    "                [vid],         # list of ids (single id)\n",
    "                core_cols=core_cols,\n",
    "                schema=source_schema,\n",
    "                table=source_table,\n",
    "            )\n",
    "\n",
    "            if df_raw.empty:\n",
    "                # This should be rare now, but keep it safe\n",
    "                del df_raw\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            # TMS analysis for this id/day\n",
    "            daily = run_daily_tms_analysis(\n",
    "                df_raw=df_raw,\n",
    "                df_mapping=df_mapping,\n",
    "            )\n",
    "            sessions_id = daily[\"sessions\"]\n",
    "\n",
    "            if sessions_id.empty:\n",
    "                del df_raw, daily, sessions_id\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            # ensure 'date' is this IST day\n",
    "            if \"date\" not in sessions_id.columns:\n",
    "                sessions_id[\"date\"] = day\n",
    "            else:\n",
    "                sessions_id[\"date\"] = pd.to_datetime(sessions_id[\"date\"]).dt.date\n",
    "                sessions_id[\"date\"] = day\n",
    "\n",
    "            day_sessions_list.append(sessions_id)\n",
    "\n",
    "            del df_raw, daily, sessions_id\n",
    "            gc.collect()\n",
    "\n",
    "        # --- after looping over all ids for this day ---\n",
    "        if not day_sessions_list:\n",
    "            logging.info(\"  â„¹ï¸ No sessions built for %s across all IDs. Skipping DB write.\", day_str)\n",
    "            day += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        day_sessions_df = pd.concat(day_sessions_list, ignore_index=True)\n",
    "        logging.info(\n",
    "            \"  âœ… Built %d sessions total for %s (across %d IDs).\",\n",
    "            len(day_sessions_df),\n",
    "            day_str,\n",
    "            day_sessions_df[\"id\"].nunique() if \"id\" in day_sessions_df.columns else -1,\n",
    "        )\n",
    "\n",
    "        # Single write per day\n",
    "        if push_to_db:\n",
    "            write_df_to_iceberg(\n",
    "                conn=conn,\n",
    "                df=day_sessions_df,\n",
    "                schema=target_schema,\n",
    "                table=target_table,\n",
    "                partition_by=list(partition_by) if partition_by else None,\n",
    "            )\n",
    "            logging.info(\n",
    "                \"  ðŸ’¾ Inserted %d rows into %s.%s for %s\",\n",
    "                len(day_sessions_df),\n",
    "                target_schema,\n",
    "                target_table,\n",
    "                day_str,\n",
    "            )\n",
    "\n",
    "        if collect_sessions:\n",
    "            all_days_sessions.append(day_sessions_df.copy())\n",
    "\n",
    "        del day_sessions_list, day_sessions_df\n",
    "        gc.collect()\n",
    "\n",
    "        day += timedelta(days=1)\n",
    "\n",
    "    logging.info(\"âœ… Completed TMS sessions range job: %s â†’ %s\", start, end)\n",
    "\n",
    "    if collect_sessions:\n",
    "        if all_days_sessions:\n",
    "            return pd.concat(all_days_sessions, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcc4e7-cdc1-471b-8b83-31251cc201e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 22:25:45 - INFO - ðŸ”Œ STEP 1: Connecting to Trino...\n",
      "2025-12-11 22:25:45 - INFO - âœ… STEP 1: Connected to Trino\n",
      "2025-12-11 22:25:45 - INFO - ðŸš€ TMS sessions range job: 2025-08-01 â†’ 2025-08-03 (3 days)\n",
      "2025-12-11 22:25:45 - INFO - ðŸ“… Processing IST day 2025-08-01\n",
      "2025-12-11 22:25:45 - INFO - ðŸ” Fetching distinct ids for 2025-08-01 (UTC window 2025-07-31 18:30:00 â†’ 2025-08-01 18:30:00)\n",
      "2025-12-11 22:25:47 - INFO - ðŸ” Found 11 ids with data on 2025-08-01\n",
      "2025-12-11 22:25:47 - INFO -   ðŸ”Ž Found 11 IDs with data on 2025-08-01: 11, 12, 13, 14, 15, 16, 18, 3, 6, 7, 9\n",
      "2025-12-11 22:25:47 - INFO -   ðŸš Processing id=11 on 2025-08-01\n",
      "2025-12-11 22:25:51 - INFO -   ðŸš Processing id=12 on 2025-08-01\n",
      "2025-12-11 22:25:54 - INFO -   ðŸš Processing id=13 on 2025-08-01\n",
      "2025-12-11 22:25:58 - INFO -   ðŸš Processing id=14 on 2025-08-01\n",
      "2025-12-11 22:26:03 - INFO -   ðŸš Processing id=15 on 2025-08-01\n",
      "2025-12-11 22:26:07 - INFO -   ðŸš Processing id=16 on 2025-08-01\n",
      "2025-12-11 22:26:09 - INFO -   ðŸš Processing id=18 on 2025-08-01\n",
      "2025-12-11 22:26:11 - INFO -   ðŸš Processing id=3 on 2025-08-01\n",
      "2025-12-11 22:26:14 - INFO -   ðŸš Processing id=6 on 2025-08-01\n",
      "2025-12-11 22:26:18 - INFO -   ðŸš Processing id=7 on 2025-08-01\n",
      "2025-12-11 22:26:22 - INFO -   ðŸš Processing id=9 on 2025-08-01\n",
      "2025-12-11 22:26:26 - INFO -   âœ… Built 68 sessions total for 2025-08-01 (across 11 IDs).\n",
      "2025-12-11 22:26:26 - INFO - ðŸ’¾ [4/5] STEP 4a: Preparing to write results to Iceberg table adhoc.facts_dev.bcs_tms_sessions_v1...\n",
      "2025-12-11 22:26:26 - INFO - ðŸ“‚ Table will be partitioned by: ['date']\n",
      "2025-12-11 22:26:26 - INFO - ðŸ” [4/5] STEP 4b: Creating table if not exists...\n",
      "2025-12-11 22:26:26 - INFO - ðŸ› ï¸ [4/5] STEP 4c: Ensured Iceberg table bcs_tms_sessions_v1 exists\n",
      "2025-12-11 22:26:26 - INFO - ðŸ” [4/5] STEP 4d: Prepared INSERT statement\n",
      "2025-12-11 22:26:26 - INFO - ðŸ” [4/5] STEP 4e: First row preview: ('11', datetime.date(2025, 8, 1), 'MISSING_INFO', 1, datetime.datetime(2025, 8, 1, 0, 0), datetime.datetime(2025, 8, 1, 5, 30, 0, 90000), 330.0, 9044.5, 9044.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 6.0, 6.0, 6.0, 4.0, 4.0, 4.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, False, '')\n",
      "2025-12-11 22:27:04 - INFO - âœ… [4/5] STEP 4f: Inserted 68 rows (total 68) into bcs_tms_sessions_v1\n",
      "2025-12-11 22:27:04 - INFO - ðŸŽ‰ [4/5] STEP 4g: Finished inserting 68 rows into bcs_tms_sessions_v1\n",
      "2025-12-11 22:27:04 - INFO -   ðŸ’¾ Inserted 68 rows into facts_dev.bcs_tms_sessions_v1 for 2025-08-01\n",
      "2025-12-11 22:27:05 - INFO - ðŸ“… Processing IST day 2025-08-02\n",
      "2025-12-11 22:27:05 - INFO - ðŸ” Fetching distinct ids for 2025-08-02 (UTC window 2025-08-01 18:30:00 â†’ 2025-08-02 18:30:00)\n",
      "2025-12-11 22:27:07 - INFO - ðŸ” Found 8 ids with data on 2025-08-02\n",
      "2025-12-11 22:27:07 - INFO -   ðŸ”Ž Found 8 IDs with data on 2025-08-02: 11, 13, 14, 15, 3, 6, 7, 9\n",
      "2025-12-11 22:27:07 - INFO -   ðŸš Processing id=11 on 2025-08-02\n",
      "2025-12-11 22:27:11 - INFO -   ðŸš Processing id=13 on 2025-08-02\n",
      "2025-12-11 22:27:16 - INFO -   ðŸš Processing id=14 on 2025-08-02\n",
      "2025-12-11 22:27:20 - INFO -   ðŸš Processing id=15 on 2025-08-02\n",
      "2025-12-11 22:27:22 - INFO -   ðŸš Processing id=3 on 2025-08-02\n",
      "2025-12-11 22:27:25 - INFO -   ðŸš Processing id=6 on 2025-08-02\n",
      "2025-12-11 22:27:28 - INFO -   ðŸš Processing id=7 on 2025-08-02\n",
      "2025-12-11 22:27:32 - INFO -   ðŸš Processing id=9 on 2025-08-02\n",
      "2025-12-11 22:27:36 - INFO -   âœ… Built 51 sessions total for 2025-08-02 (across 8 IDs).\n",
      "2025-12-11 22:27:36 - INFO - ðŸ’¾ [4/5] STEP 4a: Preparing to write results to Iceberg table adhoc.facts_dev.bcs_tms_sessions_v1...\n",
      "2025-12-11 22:27:36 - INFO - ðŸ“‚ Table will be partitioned by: ['date']\n",
      "2025-12-11 22:27:36 - INFO - ðŸ” [4/5] STEP 4b: Creating table if not exists...\n",
      "2025-12-11 22:27:36 - INFO - ðŸ› ï¸ [4/5] STEP 4c: Ensured Iceberg table bcs_tms_sessions_v1 exists\n",
      "2025-12-11 22:27:36 - INFO - ðŸ” [4/5] STEP 4d: Prepared INSERT statement\n",
      "2025-12-11 22:27:36 - INFO - ðŸ” [4/5] STEP 4e: First row preview: ('11', datetime.date(2025, 8, 2), 'MISSING_INFO', 1, datetime.datetime(2025, 8, 2, 0, 0), datetime.datetime(2025, 8, 2, 5, 30, 0, 574000), 330.01, 9552.25, 9552.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, False, '')\n",
      "2025-12-11 22:28:11 - INFO - âœ… [4/5] STEP 4f: Inserted 51 rows (total 51) into bcs_tms_sessions_v1\n",
      "2025-12-11 22:28:11 - INFO - ðŸŽ‰ [4/5] STEP 4g: Finished inserting 51 rows into bcs_tms_sessions_v1\n",
      "2025-12-11 22:28:11 - INFO -   ðŸ’¾ Inserted 51 rows into facts_dev.bcs_tms_sessions_v1 for 2025-08-02\n",
      "2025-12-11 22:28:11 - INFO - ðŸ“… Processing IST day 2025-08-03\n",
      "2025-12-11 22:28:11 - INFO - ðŸ” Fetching distinct ids for 2025-08-03 (UTC window 2025-08-02 18:30:00 â†’ 2025-08-03 18:30:00)\n",
      "2025-12-11 22:28:13 - INFO - ðŸ” Found 8 ids with data on 2025-08-03\n",
      "2025-12-11 22:28:13 - INFO -   ðŸ”Ž Found 8 IDs with data on 2025-08-03: 11, 13, 14, 15, 3, 6, 7, 9\n",
      "2025-12-11 22:28:13 - INFO -   ðŸš Processing id=11 on 2025-08-03\n",
      "2025-12-11 22:28:16 - INFO -   ðŸš Processing id=13 on 2025-08-03\n",
      "2025-12-11 22:28:21 - INFO -   ðŸš Processing id=14 on 2025-08-03\n",
      "2025-12-11 22:28:25 - INFO -   ðŸš Processing id=15 on 2025-08-03\n",
      "2025-12-11 22:28:28 - INFO -   ðŸš Processing id=3 on 2025-08-03\n",
      "2025-12-11 22:28:30 - INFO -   ðŸš Processing id=6 on 2025-08-03\n",
      "2025-12-11 22:28:33 - INFO -   ðŸš Processing id=7 on 2025-08-03\n",
      "2025-12-11 22:28:37 - INFO -   ðŸš Processing id=9 on 2025-08-03\n",
      "2025-12-11 22:28:40 - INFO -   âœ… Built 47 sessions total for 2025-08-03 (across 8 IDs).\n",
      "2025-12-11 22:28:40 - INFO - ðŸ’¾ [4/5] STEP 4a: Preparing to write results to Iceberg table adhoc.facts_dev.bcs_tms_sessions_v1...\n",
      "2025-12-11 22:28:40 - INFO - ðŸ“‚ Table will be partitioned by: ['date']\n",
      "2025-12-11 22:28:40 - INFO - ðŸ” [4/5] STEP 4b: Creating table if not exists...\n",
      "2025-12-11 22:28:40 - INFO - ðŸ› ï¸ [4/5] STEP 4c: Ensured Iceberg table bcs_tms_sessions_v1 exists\n",
      "2025-12-11 22:28:40 - INFO - ðŸ” [4/5] STEP 4d: Prepared INSERT statement\n",
      "2025-12-11 22:28:40 - INFO - ðŸ” [4/5] STEP 4e: First row preview: ('11', datetime.date(2025, 8, 3), 'MISSING_INFO', 1, datetime.datetime(2025, 8, 3, 0, 0), datetime.datetime(2025, 8, 3, 5, 30, 0, 91000), 330.0, 10400.75, 10400.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 78.61, 78.61, 78.61, 13.0, 13.0, 13.0, 13.0, 4.0, 4.0, 4.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, False, '')\n",
      "2025-12-11 22:29:13 - INFO - âœ… [4/5] STEP 4f: Inserted 47 rows (total 47) into bcs_tms_sessions_v1\n",
      "2025-12-11 22:29:13 - INFO - ðŸŽ‰ [4/5] STEP 4g: Finished inserting 47 rows into bcs_tms_sessions_v1\n",
      "2025-12-11 22:29:13 - INFO -   ðŸ’¾ Inserted 47 rows into facts_dev.bcs_tms_sessions_v1 for 2025-08-03\n",
      "2025-12-11 22:29:13 - INFO - âœ… Completed TMS sessions range job: 2025-08-01 â†’ 2025-08-03\n"
     ]
    }
   ],
   "source": [
    "# conn = connect_to_trino()\n",
    "\n",
    "# run_tms_sessions_range_to_iceberg(\n",
    "#     conn=conn,\n",
    "#     start_date=\"2025-09-01\",\n",
    "#     end_date=\"2025-09-03\",\n",
    "#     core_cols=CORE_COLS,\n",
    "#     df_mapping=None,\n",
    "#     source_schema=\"facts_prod\",\n",
    "#     source_table=\"can_parsed_output_all\",\n",
    "#     target_schema=\"facts_dev\",\n",
    "#     target_table=\"bcs_tms_sessions_v2\",\n",
    "#     push_to_db=True,\n",
    "#     partition_by=(\"date\",),\n",
    "#     collect_sessions=False,   # pure pipeline mode\n",
    "# )\n",
    "\n",
    "# # quick sanity check\n",
    "# # sessions_all.query(\"date == '2025-09-02' and id == '16'\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- TMS main, mirroring energy_mileage main() ----\n",
    "def main(\n",
    "    start_date_str: str | None = None,\n",
    "    end_date_str: str | None = None,\n",
    "    source_schema: str = \"facts_prod\",\n",
    "    source_table: str = \"can_parsed_output_all\",\n",
    "    target_schema: str = \"facts_dev\",\n",
    "    target_table: str = \"bcs_tms_sessions_v1\",\n",
    "    push_to_db: bool = True,\n",
    "    partition_by: tuple | None = (\"date\",),\n",
    "    ids: list[str] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entry point for the BCS/TMS sessions pipeline.\n",
    "\n",
    "    - If start_date_str and end_date_str are provided:\n",
    "        process that inclusive date range (IST).\n",
    "    - Otherwise:\n",
    "        use CLI args via parse_arguments() + get_target_date()\n",
    "        and process a single IST day (usually 'yesterday').\n",
    "    \"\"\"\n",
    "\n",
    "    conn = connect_to_trino()\n",
    "    if not conn:\n",
    "        logging.critical(\"âŒ Failed to establish a database connection. Exiting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Decide date_range\n",
    "        if start_date_str and end_date_str:\n",
    "            logging.info(\n",
    "                f\"ðŸ“… Processing explicit date range: {start_date_str} to {end_date_str}\"\n",
    "            )\n",
    "            start_date = date.fromisoformat(start_date_str)\n",
    "            end_date = date.fromisoformat(end_date_str)\n",
    "        else:\n",
    "            # Use CLI semantics (like energy_mileage_daily)\n",
    "            args = parse_arguments()\n",
    "            # override push_to_db if --no-push used\n",
    "            if args.no_push:\n",
    "                push_to_db = False\n",
    "\n",
    "            # optional ids from CLI\n",
    "            if args.ids:\n",
    "                ids = args.ids\n",
    "\n",
    "            target_date = get_target_date(args)\n",
    "            start_date = end_date = target_date\n",
    "\n",
    "        # Single call to our range runner (it already loops day-by-day & id-by-id)\n",
    "        logging.info(\n",
    "            f\"â–¶ï¸ Starting TMS sessions job for {start_date} â†’ {end_date} \"\n",
    "            f\"(push_to_db={push_to_db})\"\n",
    "        )\n",
    "\n",
    "        run_tms_sessions_range_to_iceberg(\n",
    "            conn=conn,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            core_cols=CORE_COLS,\n",
    "            df_mapping=None,  # or your mapping df when you have it\n",
    "            source_schema=source_schema,\n",
    "            source_table=source_table,\n",
    "            target_schema=target_schema,\n",
    "            target_table=target_table,\n",
    "            partition_by=partition_by,\n",
    "            push_to_db=push_to_db,\n",
    "            collect_sessions=False,\n",
    "            ids=ids,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"âŒ A critical error occurred in TMS main: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        logging.info(\"ðŸ”’ STEP 5: Closing Trino connection...\")\n",
    "        conn.close()\n",
    "        logging.info(\"âœ… STEP 5: Connection closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Airflow-style call (explicit range, DB write on):\n",
    "    main(start_date_str=\"2025-09-01\", end_date_str=\"2025-09-03\", push_to_db=True)\n",
    "\n",
    "    # CLI-style default: uses parse_arguments() + get_target_date()\n",
    "    # main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naarni_venv",
   "language": "python",
   "name": "naarni_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
